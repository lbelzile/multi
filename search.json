[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Plan de cours",
    "section": "",
    "text": "Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\n\n\n   automne 2023\n   jeudi (J01) et mercredi (S01)\n   12h à 15h (J01) et 18h45 à 21h45 (S01)\n   Decelles, Victoriaville (S01) et Forestville (J01)"
  },
  {
    "objectID": "syllabus.html#thèmes-couverts",
    "href": "syllabus.html#thèmes-couverts",
    "title": "Plan de cours",
    "section": "Thèmes couverts",
    "text": "Thèmes couverts\n\nAnalyse exploratoire\nAnalyse factorielle exploratoire (analyse en composantes principales)\nAnalyse de regroupements\nSélection de variables pour les modèles de régression\nRégression logistique (incluant les données multinomiales, modèle à cotes proportionnelles)\nAnalyse de survie\nDonnées manquantes"
  },
  {
    "objectID": "syllabus.html#clientèle-cible",
    "href": "syllabus.html#clientèle-cible",
    "title": "Plan de cours",
    "section": "Clientèle cible",
    "text": "Clientèle cible\nCours obligatoire de spécialisation dans le programme de M.Sc. (spécialisation intelligence d’affaires) et dans le microprogramme en exploitation de données en intelligence d’affaires."
  },
  {
    "objectID": "syllabus.html#cours-incompatible",
    "href": "syllabus.html#cours-incompatible",
    "title": "Plan de cours",
    "section": "Cours incompatible",
    "text": "Cours incompatible\nVous ne pouvez pas vous inscrire à ce cours si vous avez postulé ou réussi le cours MATH 60603(A), ou si MATH 60603(A) fait partie de votre structure."
  },
  {
    "objectID": "syllabus.html#logiciels-et-programmation",
    "href": "syllabus.html#logiciels-et-programmation",
    "title": "Plan de cours",
    "section": "Logiciels et programmation",
    "text": "Logiciels et programmation\nNous utiliserons le langage de programmation R. Les notes de cours et les exercices contiennent des scripts pour vous aider. La courbe d’apprentissage est abrupte, mais nous consacrerons la deuxième séance de cours à couvrir les bases du langage, la manipulation de bases de données et la visualisation. Vous trouverez des instructions pour installer, R, Rstudio et d’autres utilitaires ici.\nChercher sur un moteur de recherche des sujets liés à R est parfois fastidieux; essayez rstats comme mot-clé si r ne retourne rien. Le forum de question StackOverflow, un site de questions et réponses pour la programmation, ainsi que la communauté RStudio, un forum dédié à RStudio et au tidyverse, sont d’autres ressources utiles."
  },
  {
    "objectID": "syllabus.html#manuel",
    "href": "syllabus.html#manuel",
    "title": "Plan de cours",
    "section": "Manuel",
    "text": "Manuel\nLes notes de cours en ligne font office d’ouvrage de référence."
  },
  {
    "objectID": "horaire.html",
    "href": "horaire.html",
    "title": "Horaire",
    "section": "",
    "text": "Contenu (): Cette page contient la liste des lectures, activités et enregistrements pour cette semaine.\nExercices (): Cette page contient des exercices pratiques en R avec solutionnaire. Ces derniers vous serviront de pratique en vue de la complétion des devoirs.\nDevoirs (): Cette page contient des instructions sur les devoirs.\n\n\nContenu\n\n\n\n\n\n\n\nTitre\n\n\nContenu\n\n\nExercice\n\n\nEvaluation\n\n\n\n\n\n\nSéance 1\n\n\n\n\n30 août–31 août\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 2\n\n\n\n\n 6 septembre– 7 septembre\n\n\nProgrammation\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 3\n\n\n\n\n13 septembre–14 septembre\n\n\nSélection de variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 4\n\n\n\n\n20 septembre–21 septembre\n\n\nSélection de variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 5\n\n\n\n\n27 septembre–28 septembre\n\n\nRégression logistique : bases\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 6\n\n\n\n\n 4 octobre– 5 octobre\n\n\nRégression logistique : prédiction et classification\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 7\n\n\n\n\n11 octobre–12 octobre\n\n\nDonnées multinomiales et valeurs manquantes\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelâche\n\n\n\n\n20 octobre\n\n\nDevoir 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n18 octobre–24 octobre\n\n\nRelâche\n\n\n\n\n\n\n\n\n\n\n\n\n\n28 octobre\n\n\nExamen intratrimestriel\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 8\n\n\n\n\n 1 novembre– 2 novembre\n\n\nAnalyse de survie\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 9\n\n\n\n\n 8 novembre– 9 novembre\n\n\nAnalyse de survie\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 10\n\n\n\n\n15 novembre–16 novembre\n\n\nRéduction de la dimension\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 11\n\n\n\n\n22 novembre–23 novembre\n\n\nAnalyse de regroupements\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéance 12\n\n\n\n\n29 novembre–30 novembre\n\n\nAnalyse de regroupements\n\n\n\n\n\n\n\n\n\n\n\n\n\n 1 décembre\n\n\nDevoir 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamen\n\n\n\n\n14 décembre\n\n\nExamen final"
  },
  {
    "objectID": "exercices/index.html",
    "href": "exercices/index.html",
    "title": "Exercices",
    "section": "",
    "text": "Cette section contient des exercices de pratique avec les solutions."
  },
  {
    "objectID": "exercices/06-exercice.html",
    "href": "exercices/06-exercice.html",
    "title": "Analyse de regroupements",
    "section": "",
    "text": "Les données fictives regroupement1 sont inspirées de Hsu & Lee (2002). Ces données contiennent des échelles pour certains éléments d’un questionnaire. Ce dernier a été élaboré afin d’évaluer l’importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l’aide d’une échelle de Likert à cinq points, allant de extrêmement important (5) à pas du tout important (1).\nLes variables représentent les activités sociales, les politiques de l’opérateur et références, la flexibilité des horaires, la santé et sécurité, le matériel publicitaire et la réputation.\n\nDoit-on standardiser les données avant d’effectuer l’analyse?\nFaites une analyse en composantes principales et projetez les observations sur un nuage de points avec les deux premières composantes principales.\nUtilisez l’algorithme des \\(K\\)-moyennes en faisant varier le nombre de groupes de 1 à 10. Utilisez une dizaine d’initialisations aléatoires.\n\nSélectionnez un nombre de regroupement adéquat\nRetournez le nombre d’observation par groupe pour la valeur de \\(K\\) choisie.\nRapportez les statistiques descriptives (moyennes, etc.) de chaque segment\nInterprétez les profils obtenus.\n\nFaites une segmentation avec d’autres méthodes. Est-ce que la segmentation est plus satisfaisante? Justifiez votre raisonnement."
  },
  {
    "objectID": "exercices/06-exercice.html#exercice-6.1",
    "href": "exercices/06-exercice.html#exercice-6.1",
    "title": "Analyse de regroupements",
    "section": "",
    "text": "Les données fictives regroupement1 sont inspirées de Hsu & Lee (2002). Ces données contiennent des échelles pour certains éléments d’un questionnaire. Ce dernier a été élaboré afin d’évaluer l’importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l’aide d’une échelle de Likert à cinq points, allant de extrêmement important (5) à pas du tout important (1).\nLes variables représentent les activités sociales, les politiques de l’opérateur et références, la flexibilité des horaires, la santé et sécurité, le matériel publicitaire et la réputation.\n\nDoit-on standardiser les données avant d’effectuer l’analyse?\nFaites une analyse en composantes principales et projetez les observations sur un nuage de points avec les deux premières composantes principales.\nUtilisez l’algorithme des \\(K\\)-moyennes en faisant varier le nombre de groupes de 1 à 10. Utilisez une dizaine d’initialisations aléatoires.\n\nSélectionnez un nombre de regroupement adéquat\nRetournez le nombre d’observation par groupe pour la valeur de \\(K\\) choisie.\nRapportez les statistiques descriptives (moyennes, etc.) de chaque segment\nInterprétez les profils obtenus.\n\nFaites une segmentation avec d’autres méthodes. Est-ce que la segmentation est plus satisfaisante? Justifiez votre raisonnement."
  },
  {
    "objectID": "exercices/05-exercice.html",
    "href": "exercices/05-exercice.html",
    "title": "Analyse factorielle",
    "section": "",
    "text": "Les observations de la base de données bjffacto sont tirées de l’article Bastian et al. (2014) et sont rattachées à une expérience en psychologie visant à corroborer l’hypothèse qu’il y a une coopération accrue entre individus sujets à une expérience traumatisante. La moitié des participant(e)s a dû plonger sa main dans un bain d’eau glacé, tandis que l’autre moitié a dû faire la même chose dans un bain d’eau tiède; les deux groupes devaient ensuite faire un jeu visant à identifier leur niveau de coopération.\nLa variable condition indique le groupe expérimental (zéro pour groupe contrôle, un pour douleur).\nIndication: utilisez la matrice de corrélation pour vos analyses factorielles\n\nPourquoi n’est-il pas nécessaire de standardiser les variables avant de procéder à l’analyse exploratoire? Justifiez votre réponse\nEffectuez une analyse factorielle exploratoire à l’aide de la méthode des composantes principales.\n\nEn utilisant le critère de Kaiser (valeurs propres) ou le diagramme d’éboulis, déterminez un nombre adéquat de facteurs à employer.\nProduisez un diagramme d’éboulis et rapportez ce dernier.\nQuel pourcentage de la variance totale est expliquée par votre combinaison de facteurs?\n\nRépétez la procédure, cette fois avec la méthode d’estimation par maximum de vraisemblance.\n\nRapportez les valeurs des critères d’information (AIC et BIC) pour \\(m=2, \\ldots, 6\\) facteurs dans un tableau.\nQuel nombre optimal de facteurs ces différents critères retournent-ils?\nY a-t-il un problème avec la solution de l’un d’entre eux?\n\nComparez les regroupements obtenus avec les deux méthodes: est-ce que les regroupements sont semblables (c’est-à-dire, est-ce que les variables retournées dans les regroupements sont associées aux mêmes facteurs)?\nÉtiquetez les facteurs obtenus avec la méthode des composantes principales et \\(m=2\\) facteurs. Expliquez brièvement leur signification. Utilisez le seuil de coupure \\(r=0.5\\) pour les chargements avec rotation varimax pour déterminer si une variable fait partie d’un facteur.\nCréez des échelles à partir des facteurs et calculez leur cohérence interne: rapportez le \\(\\alpha\\) de Cronbach pour chacun des facteurs.\nRetournez un tableau de statistiques descriptives (moyenne et écart-type uniquement) pour chaque échelle, par condition expérimental (condition). Arrondissez à deux chiffres après la virgule et commentez sur les différences entre groupes, le cas échéant."
  },
  {
    "objectID": "exercices/05-exercice.html#exercice-5.1",
    "href": "exercices/05-exercice.html#exercice-5.1",
    "title": "Analyse factorielle",
    "section": "",
    "text": "Les observations de la base de données bjffacto sont tirées de l’article Bastian et al. (2014) et sont rattachées à une expérience en psychologie visant à corroborer l’hypothèse qu’il y a une coopération accrue entre individus sujets à une expérience traumatisante. La moitié des participant(e)s a dû plonger sa main dans un bain d’eau glacé, tandis que l’autre moitié a dû faire la même chose dans un bain d’eau tiède; les deux groupes devaient ensuite faire un jeu visant à identifier leur niveau de coopération.\nLa variable condition indique le groupe expérimental (zéro pour groupe contrôle, un pour douleur).\nIndication: utilisez la matrice de corrélation pour vos analyses factorielles\n\nPourquoi n’est-il pas nécessaire de standardiser les variables avant de procéder à l’analyse exploratoire? Justifiez votre réponse\nEffectuez une analyse factorielle exploratoire à l’aide de la méthode des composantes principales.\n\nEn utilisant le critère de Kaiser (valeurs propres) ou le diagramme d’éboulis, déterminez un nombre adéquat de facteurs à employer.\nProduisez un diagramme d’éboulis et rapportez ce dernier.\nQuel pourcentage de la variance totale est expliquée par votre combinaison de facteurs?\n\nRépétez la procédure, cette fois avec la méthode d’estimation par maximum de vraisemblance.\n\nRapportez les valeurs des critères d’information (AIC et BIC) pour \\(m=2, \\ldots, 6\\) facteurs dans un tableau.\nQuel nombre optimal de facteurs ces différents critères retournent-ils?\nY a-t-il un problème avec la solution de l’un d’entre eux?\n\nComparez les regroupements obtenus avec les deux méthodes: est-ce que les regroupements sont semblables (c’est-à-dire, est-ce que les variables retournées dans les regroupements sont associées aux mêmes facteurs)?\nÉtiquetez les facteurs obtenus avec la méthode des composantes principales et \\(m=2\\) facteurs. Expliquez brièvement leur signification. Utilisez le seuil de coupure \\(r=0.5\\) pour les chargements avec rotation varimax pour déterminer si une variable fait partie d’un facteur.\nCréez des échelles à partir des facteurs et calculez leur cohérence interne: rapportez le \\(\\alpha\\) de Cronbach pour chacun des facteurs.\nRetournez un tableau de statistiques descriptives (moyenne et écart-type uniquement) pour chaque échelle, par condition expérimental (condition). Arrondissez à deux chiffres après la virgule et commentez sur les différences entre groupes, le cas échéant."
  },
  {
    "objectID": "exercices/05-exercice.html#exercice-5.2",
    "href": "exercices/05-exercice.html#exercice-5.2",
    "title": "Analyse factorielle",
    "section": "Exercice 5.2",
    "text": "Exercice 5.2\nLes données sondage_entreprise contiennent les résultats d’un sondage effectué par une compagnie auprès de ses employés. Le but de l’exercice est d’ajuster un modèle d’analyse factorielle exploratoire (avec rotation varimax) aux données.\n\nProduisez des statistiques descriptives pour les variables q8 à q17\nCombien y a-t-il de répondants? Déterminez si ce nombre est suffisant pour effectuer une analyse factorielle.\nUtilisez la méthode d’estimation par composantes principales et le critère de Kaiser pour ajuster le modèle d’analyse factorielle. Combien de facteurs serait-il raisonnable de retenir?\nRépétez l’exercice, cette fois avec la méthode du maximum de vraisemblance\n\nEst-ce qu’un de ces modèles ajustés est un cas de quasi-Heywood?\nCombien de facteurs les critères d’information recommendent-ils?\nSi vous ajoutez des facteurs, est-ce que votre interprétation change?\n\nCréez des échelles et vérifiez leur cohérence interne."
  },
  {
    "objectID": "exercices/04-exercice.html",
    "href": "exercices/04-exercice.html",
    "title": "Analyse de survie",
    "section": "",
    "text": "Fotopoulos & Louri (2000) considèrent les facteurs de risque pour les nouvelles compagnies manufacturières en Grèce établies entre 1982 et 1984. Utilisez le Tableau 1 de l’article pour pour répondre aux questions suivantes:\n\nQuel type de mécanisme de censure est présent dans ces données?\nDonnez une estimation de la probabilité qu’une entreprise survive trois ans ou plus.\nDonnez une estimation de la probabilité qu’une entreprise survive entre 4 et 5 ans, soit l’intervalle [4,5) ans\nQuel pourcentage des observations sont censurées?\nÊtes-vous en mesure de nous fournir une estimation du troisième quartile de la fonction de survie? Justifiez votre réponse.\n\nLe Tableau 3 de l’article fournit les estimations d’un modèle à risques proportionnels de Cox.\n\nComparez les modèles (1) et (2). Est-ce que l’effet de cohorte impacte la survie?\nPour le modèle (3), décrivez le plus précisément possible l’effet des variables capital (FIXED_ASSET) et le montant de la dette (DEBT) sur la durée de vie des entreprises."
  },
  {
    "objectID": "exercices/04-exercice.html#exercice-4.1",
    "href": "exercices/04-exercice.html#exercice-4.1",
    "title": "Analyse de survie",
    "section": "",
    "text": "Fotopoulos & Louri (2000) considèrent les facteurs de risque pour les nouvelles compagnies manufacturières en Grèce établies entre 1982 et 1984. Utilisez le Tableau 1 de l’article pour pour répondre aux questions suivantes:\n\nQuel type de mécanisme de censure est présent dans ces données?\nDonnez une estimation de la probabilité qu’une entreprise survive trois ans ou plus.\nDonnez une estimation de la probabilité qu’une entreprise survive entre 4 et 5 ans, soit l’intervalle [4,5) ans\nQuel pourcentage des observations sont censurées?\nÊtes-vous en mesure de nous fournir une estimation du troisième quartile de la fonction de survie? Justifiez votre réponse.\n\nLe Tableau 3 de l’article fournit les estimations d’un modèle à risques proportionnels de Cox.\n\nComparez les modèles (1) et (2). Est-ce que l’effet de cohorte impacte la survie?\nPour le modèle (3), décrivez le plus précisément possible l’effet des variables capital (FIXED_ASSET) et le montant de la dette (DEBT) sur la durée de vie des entreprises."
  },
  {
    "objectID": "exercices/04-exercice.html#exercice-4.2",
    "href": "exercices/04-exercice.html#exercice-4.2",
    "title": "Analyse de survie",
    "section": "Exercice 4.2",
    "text": "Exercice 4.2\nUn commerce de chaussures de Montréal veut optimiser son inventaire afin de maximiser ses profits et fait appel à votre société de conseil. La base de données chaussures contient des observations fictives et les variables suivantes:\n\nstatut: variable catégorielle, 0 s’il est vendu, 1 si l’article est toujours en stock, 2 s’il est déstocké (les modèles invendus après 40 mois en magasins sont passés aux pertes et profits).\ntemps: temps de stockage de l’article (en mois).\nprix: prix de vente réelle de l’article (avec rabais si applicable), arrondi à l’unité près.\nsexe: variable catégorielle, 0 pour modèle pour homme, 1 pour femme.\n\nNotre objectif premier est d’estimer le temps qu’un article passe en stock avant d’être vendu.\n\nQue représente la censure dans cet exemple?\nEstimez le temps de stockage à l’aide d’un modèle de Kaplan–Meier et rapportez les estimés des quartiles.\nAjustez un modèle à risque proportionnel de Cox pour la durée de stockage en fonction du sexe et du prix de vente.\n\nRapportez et interprétez les coefficients des variables.\nEst-ce que les effets estimés sont significatifs?\n\nTracez les estimés des courbes de survie d’une chaussure de l’année dont le prix de vente est 120$ pour les modèles pour homme et pour femme.\n\nOn vous informe que, pour éliminer les invendus lors de l’arrivée de nouveaux modèles, l’entreprise offre une réduction de 20% après 15 mois.\n\nAjustez un modèle de Cox qui prendra en compte cette nouvelle information. Rapportez les estimés des paramètres de votre modèle; est-ce que vos interprétations changent?\nOn pourrait considérer un modèle à risque non-proportionnels contenant une interaction entre le prix et le temps de manière à ajuster le même modèle. Expliquez comment cela pourrait être fait de manière à obtenir les mêmes estimés des paramètres."
  },
  {
    "objectID": "exercices/03-exercice.html",
    "href": "exercices/03-exercice.html",
    "title": "Régression logistique",
    "section": "",
    "text": "Les données logistclient contiennent des données simulées pour un cas fictif de promotion pour des clients.\n\nEstimez le modèle logistique pour la probabilité que promo=1 avec les variables explicatives nachats, sexe et tclient.\nInterprétez les coefficients du modèle à l’échelle de la cote en terme de pourcentage d’augmentation ou de diminution.\nTestez si l’effet de nachats est statistiquement significatif à niveau \\(\\alpha = 0.05\\).\nChoisissez un point de coupure pour la classification pour maximiser le taux de bonne classification.\n\nPour le point de coupure choisi, construisez une matrice de confusion.\nCalculez la sensibilité, la spécificité et le taux de bonne classification manuellement. Vérifiez vos réponses avec la sortie du tableau.\n\nProduisez un graphique de la fonction d’efficacité du récepteur (courbe ROC) et rapportez l’aire sous la courbe estimée à l’aide de la validation croisée.\nCalculez la statistique de Spiegelhalter (1986) pour la calibration du modèle. Y a-t-il des preuves de surajustement?"
  },
  {
    "objectID": "exercices/03-exercice.html#exercice-3.1",
    "href": "exercices/03-exercice.html#exercice-3.1",
    "title": "Régression logistique",
    "section": "",
    "text": "Les données logistclient contiennent des données simulées pour un cas fictif de promotion pour des clients.\n\nEstimez le modèle logistique pour la probabilité que promo=1 avec les variables explicatives nachats, sexe et tclient.\nInterprétez les coefficients du modèle à l’échelle de la cote en terme de pourcentage d’augmentation ou de diminution.\nTestez si l’effet de nachats est statistiquement significatif à niveau \\(\\alpha = 0.05\\).\nChoisissez un point de coupure pour la classification pour maximiser le taux de bonne classification.\n\nPour le point de coupure choisi, construisez une matrice de confusion.\nCalculez la sensibilité, la spécificité et le taux de bonne classification manuellement. Vérifiez vos réponses avec la sortie du tableau.\n\nProduisez un graphique de la fonction d’efficacité du récepteur (courbe ROC) et rapportez l’aire sous la courbe estimée à l’aide de la validation croisée.\nCalculez la statistique de Spiegelhalter (1986) pour la calibration du modèle. Y a-t-il des preuves de surajustement?"
  },
  {
    "objectID": "exercices/03-exercice.html#exercice-3.2",
    "href": "exercices/03-exercice.html#exercice-3.2",
    "title": "Régression logistique",
    "section": "Exercice 3.2",
    "text": "Exercice 3.2\nLe modèle de Bradley & Terry (1952) décrit la probabilité que le résultat de l’ «équipe \\(i\\) » soit supérieur à celui de l’«équipe» \\(j\\), \\[\\begin{align*}\n\\Pr(Y_i &gt; Y_j)= \\frac{\\exp(\\beta_i)}{\\exp(\\beta_i) + \\exp(\\beta_j)}, \\quad i, j \\in \\{1, \\ldots, K\\},\n\\end{align*}\\] en assumant que les doublons (égalité) ne surviennent pas.\nCe modèle simple peut servir pour prédire le classement d’équipes sportives: si on écrit le modèle en terme de cote, on obtient pour l’équipe \\(i\\) à domicile et l’équipe \\(j\\) en visite \\[\\begin{align*}\n\\ln\\left\\{\\frac{\\Pr(\\text{victoire équipe $i$ (domicile)})}{\\Pr(\\text{victoire équipe $j$ (visiteur)})}\\right\\}= \\beta_i - \\beta_j.\n\\end{align*}\\]\nLe modèle décrit ci-dessus peut être ajusté à l’aide d’une régression logistique avec un ensemble de \\(K-1\\) variable explicatives1 où pour le match \\(i\\) et l’équipe \\(k=2, \\ldots, K\\), on a \\[\\begin{align*}\nX_{ik} = \\begin{cases}\n\\hphantom{-}1, & k = i,\\\\\n-1, & k = j,\\\\\n\\hphantom{-}0, & \\text{sinon}.\n\\end{cases}\n\\end{align*}\\] Le modèle Bradley–Terry de base n’a pas d’ordonnée à l’origine. Si on l’ajoute, l’équation du modèle pour une partie devient \\[\\begin{align*}\n\\ln\\left\\{\\frac{\\Pr(\\text{victoire équipe $i$ (domicile)})}{\\Pr(\\text{victoire équipe $j$ (visiteur)})}\\right\\}= \\alpha + \\beta_i - \\beta_j,\n\\end{align*}\\] où \\(\\beta_i\\) représente la force de l’équipe à domicile, \\(\\beta_j\\) la force de l’équipe en visite et l’ordonnée à l’origine \\(\\alpha\\) capture l’effet du jeu à domicile.\nLa base de données lnh du paquet hecmulti contient les résultats de chaque partie par équipe, tandis que lnh_BT fournit les mêmes données, mais dans un format propice pour l’ajustement du modèle de Bradley–Terry.\nAjustez le modèle de Bradley–Terry aux données lnh_BT (utilisez la formule vainqueur ~ . pour ajuster le modèle avec toutes les équipes). La catégorie de référence est Anaheim_Ducks, qui n’apparaît pas dans les sorties.\n\nInterprétez le coefficient pour l’ordonnée à l’origine \\(\\alpha\\) en terme de pourcentage d’augmentation ou de diminution de la cote par rapport à la référence jouer à l’extérieur.\nCalculez un intervalle de confiance de niveau 95% pour l’ordonnée à l’origine et déterminez si jouer à domicile impacte significativement le score.\nFournissez un tableau avec le classement des cinq premières équipes qui ont la plus grande chance de succès selon le modèle.2\nPour chaque match, utilisez le modèle logistique pour prédire l’équipe gagnante.\n\nConstruisez une matrice de confusion (1 pour une victoire de l’équipe à domicile, 0 sinon) avec un point de coupure de 0.5 (assignation à l’événement ou à la classe la plus probable) et rapportez cette dernière.\nCalculez le taux de bonne classification, la sensibilité et la spécificité à partir de votre matrice de confusion.\n\nProduisez un graphique de la fonction d’efficacité du récepteur et rapportez l’aire sous la courbe. Commentez sur la qualité prédictive globale du modèle."
  },
  {
    "objectID": "exercices/03-exercice.html#exercice-3.3",
    "href": "exercices/03-exercice.html#exercice-3.3",
    "title": "Régression logistique",
    "section": "Exercice 3.3",
    "text": "Exercice 3.3\nOn s’intéresse à la satisfaction de clients par rapport à un produit. Cette dernière est mesurée à l’aide d’une échelle de Likert, allant de très insatisfait (1) à très satisfait (5). Les 1000 observations se trouvent dans la base de données multinom du paquet hecmulti.\nModélisez la satisfaction des clients en fonction de l’âge, du niveau d’éducation, du sexe et du niveau de revenu.\n\nEst-ce que le modèle de régression multinomiale ordinale à cote proportionnelles est une simplification adéquate du modèle de régression multinomiale logistique? Si oui, utilisez ce modèle pour la suite. Si non, ajustez le modèle de régression multinomiale logistique avec 1 comme catégorie de référence pour la satisfaction, 1 pour revenu et sec pour éducation3 et utilisez ce dernier pour répondre aux autres questions.\nInterprétez l’effet des variables éducation et sexe pour la catégorie 2 par rapport à 1.\nEst-ce que le modèle avec une probabilité constante pour chaque item est adéquat lorsque comparé au modèle qui inclut toutes les covariables?\nEst-ce que l’effet de la variable âge est globalement significatif?\nFournissez un intervalle de confiance à niveau 95% pour l’effet multiplicatif d’une augmentation d’une unité de la variable âge pour chacune des cote par rapport à très insatisfait (1). Que concluez-vous sur l’effet de âge pour les réponses 2 à 5 par rapport à 1?\nÉcrivez l’équation de la cote ajustée pour satisfait (4) par rapport à très insatisfait (1).\nPrédisez la probabilité qu’un homme de 30 ans qui a un diplôme collégial et qui fait partie de la classe moyenne sélectionne une catégorie donnée. Quelle modalité est la plus susceptible?"
  },
  {
    "objectID": "exercices/03-exercice.html#footnotes",
    "href": "exercices/03-exercice.html#footnotes",
    "title": "Régression logistique",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nIl n’y a pas de variable explicative \\(X_{\\text{ref}}\\) pour la catégorie de référence, autrement les données seraient colinéaires. Une des catégories \\(\\text{ref} \\in \\{1, \\ldots, K\\}\\) sert de référence et le coefficient correspondant est nul, soit \\(\\beta_{\\text{ref}}=0\\).↩︎\nAttention à la catégorie de référence!↩︎\nUtilisez la fonction relevel pour changer la catégorie de référence, avec relevel(educ, ref = 'sec').↩︎"
  },
  {
    "objectID": "exercices/02-exercice.html",
    "href": "exercices/02-exercice.html",
    "title": "Sélection de variables",
    "section": "",
    "text": "Le but de l’exercice est de bâtir un modèle prédictif pour le nombre annuel de demandes d’admission à partir de la base de données college.\n\nFaites une analyse exploratoire des variables explicatives:\n\nQuelles variables devraient êtres exclues de la modélisation? Justifiez votre réponse.\nComparez la variable réponse avec les autres variables: y a-t-il des transformations qui améliorerait l’ensemble de variables candidates: interactions, création de variables dychotomiques, transformations (racines carrée, transformation logarithmique, etc.)?\nVérifiez s’il y a des variables catégorielles encodées comme des variables numériques.\n\nScindez la base de données en échantillon avec données d’entraînement (environ 2/3 des données) et échantillon de validation; utilisez le germe aléatoire 60602 via set.seed(60602).\n\nSélectionnez un modèle à l’aide d’une des méthodes couvertes, mais en basant votre choix sur l’erreur moyenne quadratique évaluée sur l’échantillon de validation.\n\nRépétez la sélection, cette fois en prenant comme critère pour l’erreur moyenne quadratique évaluée par validation croisée (aléatoire) à cinq plis.\nCréez un tableau avec le nombre de coefficients de votre modèle final et un estimé de l’erreur moyenne quadratique obtenu par validation externe ou croisée."
  },
  {
    "objectID": "exercices/02-exercice.html#exercice-2.1",
    "href": "exercices/02-exercice.html#exercice-2.1",
    "title": "Sélection de variables",
    "section": "",
    "text": "Le but de l’exercice est de bâtir un modèle prédictif pour le nombre annuel de demandes d’admission à partir de la base de données college.\n\nFaites une analyse exploratoire des variables explicatives:\n\nQuelles variables devraient êtres exclues de la modélisation? Justifiez votre réponse.\nComparez la variable réponse avec les autres variables: y a-t-il des transformations qui améliorerait l’ensemble de variables candidates: interactions, création de variables dychotomiques, transformations (racines carrée, transformation logarithmique, etc.)?\nVérifiez s’il y a des variables catégorielles encodées comme des variables numériques.\n\nScindez la base de données en échantillon avec données d’entraînement (environ 2/3 des données) et échantillon de validation; utilisez le germe aléatoire 60602 via set.seed(60602).\n\nSélectionnez un modèle à l’aide d’une des méthodes couvertes, mais en basant votre choix sur l’erreur moyenne quadratique évaluée sur l’échantillon de validation.\n\nRépétez la sélection, cette fois en prenant comme critère pour l’erreur moyenne quadratique évaluée par validation croisée (aléatoire) à cinq plis.\nCréez un tableau avec le nombre de coefficients de votre modèle final et un estimé de l’erreur moyenne quadratique obtenu par validation externe ou croisée."
  },
  {
    "objectID": "exercices/01-exercice.html",
    "href": "exercices/01-exercice.html",
    "title": "Analyse exploratoire",
    "section": "",
    "text": "La base de données aerien du paquet hecmulti contient les résultats de sondages pour une compagnie aérienne non identifiée.\nEffectuez une analyse exploratoire des données aerien:\n\nFormulez des questions en lien avec la description de la base de données préliminaires à l’analyse exploratoire.\nExaminez la base de données; identifiez le type de variable et leur nature.\nY a-t-il des valeurs manquantes? Si oui, pour quelles variables? Serait-il logique de les imputer par leur moyenne, dans le cas présent?\nCalculez les statistiques descriptives pour les variables continues et produisez des tableaux de fréquence ou de contingence pour les variables catégorielles. Que remarquez-vous?\nReprésentez graphiquement la distribution de quelques variables de la base de données selon les différents niveaux de satisfaction.\nRésumez les faits saillants de votre analyse exploratoire en cinq à 10 points."
  },
  {
    "objectID": "exercices/01-exercice.html#exercice-1.1",
    "href": "exercices/01-exercice.html#exercice-1.1",
    "title": "Analyse exploratoire",
    "section": "",
    "text": "La base de données aerien du paquet hecmulti contient les résultats de sondages pour une compagnie aérienne non identifiée.\nEffectuez une analyse exploratoire des données aerien:\n\nFormulez des questions en lien avec la description de la base de données préliminaires à l’analyse exploratoire.\nExaminez la base de données; identifiez le type de variable et leur nature.\nY a-t-il des valeurs manquantes? Si oui, pour quelles variables? Serait-il logique de les imputer par leur moyenne, dans le cas présent?\nCalculez les statistiques descriptives pour les variables continues et produisez des tableaux de fréquence ou de contingence pour les variables catégorielles. Que remarquez-vous?\nReprésentez graphiquement la distribution de quelques variables de la base de données selon les différents niveaux de satisfaction.\nRésumez les faits saillants de votre analyse exploratoire en cinq à 10 points."
  },
  {
    "objectID": "evaluations/index.html",
    "href": "evaluations/index.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "En plus de l’examen intratrimestriel et de l’examen final, vous aurez deux devoirs à compléter en équipe. Chaque devoir est noté sur 15 points.\nLes devoirs sont à remettre au plus tard à 23h55 le jour de la remise et seront notés selon le barem suivant:\n\n\n\n\n\n\n\n\n\nClarté\nContenu\nRigueur\nPrésentation\n\n\n\n\nÉnoncés sans jargon, explications claires et enchaînement logique des idées.\nLe contenu présenté est succinct et pertinent pour répondre aux questions.\nRéponses correctes, réflexion poussée\nPas d’erreur d’orthographe, de grammaire ou de syntaxe. Références bibliographiques adéquates (le cas échéant). Règles de présentation respectées.\n\n\nÉnoncés généralement clairs. Explications majoritairement claires. Justification adéquate des arguments énoncés.\nLe contenu est présenté de manière adéquate, mais parfois superflu ou présentant quelques lacunes.\nLa plupart des réponses sont correctes, réflexion soutenue\nQuelques fautes de français au plus. Références bibliographes présentes, mais inadéquatement citées. Organisation de la présentation globalement satisfaisante.\n\n\nFormulation alambiquée ne répondant que partiellement aux questions.\nLe contenu présenté est parfois tangentiel, trop de détails inutiles sont présentés ou plusieurs éléments de réponse sont manquants.\nPlusieurs erreurs logiques, réflexion ténue et superficielle\nPlusieurs erreurs de français, absence de bibliographie ou de sources. Présentation déficiente non conformes au vade mecum.\n\n\n\nJ’assignerai 0% pour un devoir non remis ou visiblement bâclé.\n\nTout travail remis hors délai sera pénalisé de 5% par jour de retard."
  },
  {
    "objectID": "evaluations/06-devoir-2022.html",
    "href": "evaluations/06-devoir-2022.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nCe travail est à réaliser individuellement.\nVous devez remettre les documents suivants:\n\nvotre rapport au format PDF\nvotre code R ou un fichier Rmarkdown\nun fichier CSV avec vos regroupements (voir instructions en bas de page)\n\nUtilisez la convention de nomenclature d6_matricule.extension, où matricule est votre matricule et extension est un de pdf, R ou Rmd.\nLes données electionsqc du paquet hecmulti contiennent les résultats électoraux (en pourcentage) pour les cinq principaux partis (CAQ, QS, PQ, PLQ, PCQ) ainsi que le taux de participation pour chacune des 125 circonscriptions aux élections générales de 2018 et 2022. La base de données contient aussi plusieurs variables sociodémographiques.\nVous devez réaliser une analyse de regroupement avec les résultats électoraux pour faire ressortir des informations pertinentes.\nVoici quelques balises:\n\nEffectuez si besoin des transformations préalables des variables explicatives (manipulations, créations de nouvelles variables, traitement des valeurs manquantes.\nDéterminez si vous devez standardiser les données et comment.\nChoisissez au moins trois méthodes (différentes dissemblances, algorithmes, etc.) et faites l’analyse de regroupement. Choisissez adéquatement vos hyperparamètres en vous basant sur les critères couverts en classe.\nChoisissez la solution qui vous semble la plus satisfaisante parmi celles obtenus en justifiant adéquatement votre choix.\nProcédez à l’interprétation des prototypes à une échelle interprétable et calculez les statistiques descriptives des variables sociodémographiques. Commentez sur le résultat de votre segmentation au vu de votre connaissance du système politique.\n\nVous devez retourner en plus de votre rapport et de votre code un fichier CSV avec deux colonnes:\n\nla première colonne pour les noms de circonscriptions (circonscription),\nla deuxième colonne pour les étiquettes de groupe (regroupements)."
  },
  {
    "objectID": "evaluations/04-devoir-2022.html",
    "href": "evaluations/04-devoir-2022.html",
    "title": "Question 1",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nCe travail est à réaliser en équipe (minimum deux, maximum quatre personnes).\nVous devez remettre les documents suivants:\nUtilisez la convention de nomenclature d4_matricule.extension, où matricule est le matricule de l’étudiant(e) qui soumet le rapport et extension est un de pdf, R ou Rmd."
  },
  {
    "objectID": "evaluations/04-devoir-2022.html#footnotes",
    "href": "evaluations/04-devoir-2022.html#footnotes",
    "title": "Question 1",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nIl n’y a pas de variable explicative \\(X_{\\text{ref}}\\) pour la catégorie de référence, autrement les données seraient colinéaires.↩︎\nAttention à la catégorie de référence.↩︎\nNotez que les auteurs de l’article ont centré préalablement les variables explicatives (c’est-à-dire, en soustrayant la moyenne de chacune des colonnes).↩︎\nAstuce: vous pouvez calculer le rapport de cote en remplaçant les coefficients dans l’équation, ou prédire les probabilités pour les deux profils de personnes présentés et calculer le rapport de cote.↩︎\nAstuce: créez une base de données (data.frame) avec une séquence adéquate de valeurs de aleatoire pour les valeurs données de risque et de epistemique.↩︎"
  },
  {
    "objectID": "evaluations/02-devoir.html",
    "href": "evaluations/02-devoir.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Ce travail est à réaliser en équipe (minimum deux, maximum quatre personnes).\nVous devez avant de commencer le devoir retélécharger le paquet hecmulti, viz.\nremotes::install_github(\"lbelzile/hecmulti\")\nVous devez remettre votre rapport au format PDF ainsi que votre code R (ou un fichier Quarto). Utilisez la convention de nomenclature d2_matricule.extension, où matricule est le matricule de l’étudiant(e) qui soumet le rapport et extension est un de pdf, qmd, rmd ou R.\nTéléchargez la version PDF de l’énoncé"
  },
  {
    "objectID": "evaluations/02-devoir.html#question-1",
    "href": "evaluations/02-devoir.html#question-1",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Question 1",
    "text": "Question 1\nPlusieurs systèmes de technologies de l’information servent à centraliser les requêtes pour des services. On considère le temps d’attente (en heures, variable duree) avant la résolution d’un billet, jusqu’à concurrence de 120 heures dans la base de données tisupport du paquet hecmulti.\n\nUne des opérations préliminaires à l’analyse de survie est le calcul du temps d’attente sur la base des registres informatiques; les données sont typiquement au format YYYY:MM:DD HH.MM. Si l’entreprise opère uniquement de 9h à 17h du lundi au vendredi, décrivez brièvement comment vous pourriez prendre en compte cette particularité lors du calcul du temps entre l’ouverture et la fermeture du billet de support si vous aviez à calculer duree vous-même.\nEstimez la fonction de survie séparément pour chaque type de billets à l’aide de l’estimateur de Kaplan–Meier.\n\nRapportez les quartiles de la surive pour les requêtes.\nRapportez la probabilité estimée qu’un billet pour un problème soit encore ouverte après 8h (soit un jour ouvrable de traitement).\nExpliquez dans vos mots pourquoi certains quartiles pour les “problèmes” ne sont pas disponibles (valeurs manquantes).\nProduisez un graphique des deux courbes de survie estimées avec des intervalles de confiance à niveau 95%. Est-ce que les courbes sont significativement différentes l’une de l’autre?\n\nAjustez un modèle de Cox pour prendre en compte l’effet du type de demande, du niveau de difficulté (traité comme variable catégorielle initialement) et des commentaires.\n\nOn suppose que les commentaires reflètent des allers-retours entre la personne qui fait une demande de service et le personnel TI et entraîne des temps de traitement plus longs en moyenne. Est-ce que les données corroborent cette hypothèse?\nToute chose étant égale par ailleurs, pour quel niveau de difficulté est-ce que le temps de survie est le plus long? et le plus court? Justifiez votre réponse.\nInterprétez le coefficient du modèle de Cox associé au nombre de commentaires en terme de pourcentage d’augmentation ou de diminution du risque.\nVérifiez le postulat de risques proportionnels pour le modèle qui inclut niveau comme variable catégorielle.\nAjustez le modèle en stratifiant par type de demande et rapportez les coefficients estimés dans un tableau.\nOn pourrait traiter le niveau comme variable continue en supposant une gradation constante entre les niveaux de difficulté. Est-ce que les données supportent cette hypothèse? Rapportez la statistique de test, la valeur-\\(p\\) et la conclusion de votre test."
  },
  {
    "objectID": "evaluations/02-devoir.html#question-2",
    "href": "evaluations/02-devoir.html#question-2",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Question 2",
    "text": "Question 2\nLes données de cette question, dans la base de données avarice du paquet hecmulti, sont extraites de ResearchBox 159 et mises à disposition sous licence CC BY 4.0, ont servi à construire une échelle psychométrique pour mesurer l’avarice. Les données de \\(n=300\\) participant(e)s ont été collectées à partir de Amazon M’Turk.\n\nZeelenberg M, Seuntjens TG, van de Ven N, Breugelmans SM (2021). Dispositional Greed Scales. European Journal of Psychological Assessment. 1–10. doi: 10.1027/1015-5759/a000647\n\nLes participant(e)s devaient répondre à 30 questions`. Les instructions étaient: «Indiquez pour chaque énoncé à quel point vous êtes en accord» à l’aide d’une échelle de Likert avec comme choix «fortement en désaccord» (1), «en désaccord» (2), «ni en accord, ni en désaccord» (3), «d’accord» (4) et « fortement d’accord». Les questions sont reproduites ci-dessous.\n\nEst-il nécessaire de faire des modifications aux données brutes avant l’analyse (standardisation, encodage inverse des échelles)? Justifiez votre réponse.\nEffectuez une analyse factorielle exploratoire à l’aide de la méthode des composantes principales avec rotation varimax (option par défaut).\n\nÀ l’aide des critères couverts en classe (valeurs propres de Kaiser, coude), déterminez un nombre adéquat de facteurs à employer. Justifiez adéquatement votre choix.\nRapportez le pourcentage de la variance expliquée par votre méthode.\nÉtiquetez les facteurs obtenus et expliquez brièvement leur signification.\nEn utilisant les facteurs choisis, créez des échelles et vérifiez leur cohérence interne via le \\(\\alpha\\) de Cronbach.\n\n\n\nListe des questions\n\nI like to collect expensive things\nAt work/school, I keep good ideas to myself so that only I can get credit for them in the long run\nNo matter how much I have of something, I always want more\nMy actions are strongly focused on material things\nI get the most fun out of buying myself all sorts of things\nA simple basic life is sufficient for me\nWhen I think about all the things I have, my first thought is about what I would like to have next\nOne can never have enough\nWhen something is being shared, I try to get as big a share as possible\nOne can never have too much money\nI enjoy being a part of exclusive clubs or groups that are not open to everyone\nI believe that money is essential; friends are replaceable\nI consider myself successful if I have a job that pays a lot of money\nThe pursuit of more and better is an important goal in life for me\nNo matter how much I have, I always want more\nI am easily satisfied with what I’ve got\n“I want it all” would be a good motto for me\nSometimes I feel a real urge to possess something\nI can’t imagine having too many things\nI do not enjoy sharing positions of power\nI always want more\nActually, I’m kind of greedy\nIt doesn’t matter how much I have, I’m never completely satisfied\nWhen I play on my own, I sometimes cheat a little\nFinancially supporting the less fortunate is a priority for me\nAs soon as I have acquired something, I start to think about the next thing I want\nEven when I am fulfilled, I often seek more\nMy life motto is ‘more is better’\nBeing financially wealthy is my number one goal.\nIn order to get what I want, I can accept the fact that other people may suffer damage"
  },
  {
    "objectID": "evaluations/01-devoir.html",
    "href": "evaluations/01-devoir.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nCe travail est à réaliser en équipe (minimum deux, maximum quatre personnes).\nOn cherche à prédire si une personne s’est prévalue d’une offre promotionnelle pour une nouvelle carte de crédit (promo) à l’aide d’autres variables explicatives présentes dans la base de données visapromo; 50 données sont intentionnellement manquantes pour permettre d’évaluer vos modèles finaux et faire un classement des équipes.\nLes étapes suivantes sont obligatoires:\nRésumez votre analyse en deux pages maximum (texte): ne soulevez que les points importants et n’incluez que les graphiques nécessaires.\nParmi les méthodes couvertes figurent notamment.\nRapportez l’erreur de classification, la sensibilité et la spécificité pour les différents modèles estimés dans un tableau et justifiez adéquatement votre choix final de modèle.\nVous devez remettre les documents suivants:\nUtilisez la convention de nomenclature d1_matricule.extension, où matricule est le matricule de l’étudiant(e) qui soumet le rapport et extension est un de pdf, R, csv.\nLa base de données devrait contenir uniquement deux colonnes et les 50 lignes correspondant aux données manquantes pour promo:\nIndication : vérifiez votre base de données pour vous assurer de respecter les consignes (pénalités pour toute personne qui déroge aux consignes).\nAssurez-vous également que les prédictions sont sensées et cohérentes avec ce qui est présent dans la base de données (avez-vous des prédictions binaires)? Vous ne devriez pas avoir de valeurs manquantes.\nVous serez évalués sur votre méthodologie, et non pas la performance relative de votre modèle par rapport à celles des autres étudiant(e)s : en revanche, les deux sont typiquement corrélées. Vous devez expliquer clairement votre démarche (méthodologie) dans votre rapport et décrire le modèle que vous avez retenu (méthode et nombre de variables final). Prenez garde au surajustement!"
  },
  {
    "objectID": "evaluations/01-devoir.html#footnotes",
    "href": "evaluations/01-devoir.html#footnotes",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nDisponible à l’aide de la commande write.csv(..., file = \"d1_matricule.csv\", row.names = FALSE)↩︎"
  },
  {
    "objectID": "contenu/ressourcesR.html",
    "href": "contenu/ressourcesR.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Il existe plusieurs ressources en ligne pour apprendre R et le tidyverse, notamment\n\nLes ressources didactiques listées sur le CRAN\nIntroduction to R and RStudio par Open Intro Stat\nTeacups, giraffes & statistics: révise les concepts statistiques et les bases de la programmation\nle livret RYouWithMe de R-Ladies Sydney\nle livre R for Data Science, qui adhère aux principes du tidyverse.\nle paquet R DoSStoolkit, développé à l’université de Toronto\nla documentation d’introverse\nles feuilles d’aide mémoire de RStudio, aussi disponibles depuis le menu Aide dans RStudio\n\nVous pouvez installer le paquet hecmulti, de même que les paquets employés dans les notes de cours,à l’aide des commandes suivantes:\n\nlib &lt;- c(\"remotes\", \"psych\", \"glmnet\", \"caret\", \n         \"leaps\", \"car\", \"glmbb\", \"AER\",\n         \"tidyverse\", \"nnet\", \"mice\", \"survminer\", \n     \"survival\", \"patchwork\", \"Hmisc\", \n         \"factoextra\", \"energy\", \"dbscan\", \"kcca\",\n         \"fastClust\", \"genieclust\", \"mclust\")\nfor(pack in lib){\n  if(!require(pack, quietly = TRUE)){\n    install.packages(pack)\n  }\n}\n\nremotes::install_github(\"lbelzile/hecmulti\")"
  },
  {
    "objectID": "contenu/index.html",
    "href": "contenu/index.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Cliquez sur les onglets de droite pour naviguer vers le contenu et le matériel pédagogique de chaque séance de cours.\nUn guide d’installation pour R et RStudio et des ressources sont également mises à disposition."
  },
  {
    "objectID": "contenu/11-contenu.html",
    "href": "contenu/11-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Objectifs et base de l’analyse de regroupements\nMesures de dissemblance\nAlgorithme des \\(K\\)-moyennes et choix des hyperparamètres"
  },
  {
    "objectID": "contenu/11-contenu.html#contenu",
    "href": "contenu/11-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Objectifs et base de l’analyse de regroupements\nMesures de dissemblance\nAlgorithme des \\(K\\)-moyennes et choix des hyperparamètres"
  },
  {
    "objectID": "contenu/11-contenu.html#lecture",
    "href": "contenu/11-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur l’analyse de regroupements, Sections 1 à 5.1"
  },
  {
    "objectID": "contenu/11-contenu.html#diapositives",
    "href": "contenu/11-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Analyse de regroupements\n Code"
  },
  {
    "objectID": "contenu/11-contenu.html#objectifs-dapprentissage",
    "href": "contenu/11-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nPouvoir effectuer les étapes d’une analyse de regroupement.\nÊtre en mesure de juger de la qualité d’une analyse de regroupements en fonction de l’interprétabilité des segments et de considération pratique (taille des groupes, homogénéité, etc.)\nComprendre les avantages et inconvénients de différents algorithmes de regroupement"
  },
  {
    "objectID": "contenu/09-contenu.html",
    "href": "contenu/09-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Extensions du modèle de Cox\nRisques non proportionnels\nExercice 4.2"
  },
  {
    "objectID": "contenu/09-contenu.html#contenu",
    "href": "contenu/09-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Extensions du modèle de Cox\nRisques non proportionnels\nExercice 4.2"
  },
  {
    "objectID": "contenu/09-contenu.html#lecture",
    "href": "contenu/09-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur l’analyse de survie, Sections 6 et 7"
  },
  {
    "objectID": "contenu/09-contenu.html#diapositives",
    "href": "contenu/09-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Analyse de survie\n Code"
  },
  {
    "objectID": "contenu/09-contenu.html#objectifs-dapprentissage",
    "href": "contenu/09-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nPouvoir manipuler une base de données pour des variables explicatives qui varient dans le temps\nComprendre les avantages et inconvénients des extensions du modèle de Cox (stratification, risques non proportionnels)\nPouvoir correctement spécifier et interpréter un modèle à risques non proportionnels."
  },
  {
    "objectID": "contenu/07-contenu.html",
    "href": "contenu/07-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Méthodes d’imputation pour données manquantes\nModèles de régression pour données multinomiales"
  },
  {
    "objectID": "contenu/07-contenu.html#contenu",
    "href": "contenu/07-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Méthodes d’imputation pour données manquantes\nModèles de régression pour données multinomiales"
  },
  {
    "objectID": "contenu/07-contenu.html#lecture",
    "href": "contenu/07-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur les données manquantes\n Chapitre sur la régression logistique, Section 4"
  },
  {
    "objectID": "contenu/07-contenu.html#diapositives",
    "href": "contenu/07-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Régression logistique\n Code"
  },
  {
    "objectID": "contenu/07-contenu.html#objectifs-dapprentissage",
    "href": "contenu/07-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nPouvoir calculer le taux de cas manquants\nClassifier les données manquantes (complètement aléatoire, aléatoire, non aléatoire) selon le contexte\nPouvoir utiliser l’imputation multiple avec les modèles de régression\nÀ l’aide de logiciel, pouvoir proposer des modèles de régression logistique pour la prédiction et la classification\nÊtre en mesure d’expliquer le ciblage et d’offrir des stratégies pour la prédiction.\nÊtre capable d’interpréter les coefficients d’un modèle multinomial logistique ou d’un modèle cumulatif à cotes proportionnelles\nPouvoir effectuer un test du postulat de cotes proportionnelles\nÊtre en mesure de calculer les prédictions d’un modèle multinomial logistique simple"
  },
  {
    "objectID": "contenu/05-contenu.html",
    "href": "contenu/05-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Modèle de régression logistique\nCote\nInterprétation des paramètres\nInférence statistique\n\ntests d’hypothèse\nintervalles de confiance"
  },
  {
    "objectID": "contenu/05-contenu.html#contenu",
    "href": "contenu/05-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Modèle de régression logistique\nCote\nInterprétation des paramètres\nInférence statistique\n\ntests d’hypothèse\nintervalles de confiance"
  },
  {
    "objectID": "contenu/05-contenu.html#prérequis",
    "href": "contenu/05-contenu.html#prérequis",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Prérequis",
    "text": "Prérequis\nCe chapitre suppose que vous êtes familiers avec la notion de variable aléatoire et la méthode du maximum de vraisemblance (couvert notamment dans le cours de MATH 60619 Analyse et inférence statistique. La première capsule reprend cette matière (jusqu’à diapo 9).\nLes vidéos pour cette section sont disponibles sur cette chaîne YouTube.\n\nLoi Bernoulli\nLoi normale\nLoi Poisson\n\nVous pouvez aussi visionner les différents vidéos de la chaîne (en passant d’une vidéo à l’autre) ici:"
  },
  {
    "objectID": "contenu/05-contenu.html#lecture",
    "href": "contenu/05-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur la régression logistique, Sections 1 et 2"
  },
  {
    "objectID": "contenu/05-contenu.html#diapositives",
    "href": "contenu/05-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Régression logistique\n Code"
  },
  {
    "objectID": "contenu/05-contenu.html#objectifs-dapprentissage",
    "href": "contenu/05-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nÊtre capable d’ajuster un modèle logistique avec R.\nPouvoir interpréter les coefficients d’un modèle logistique en terme d’augmentation ou de diminution de la cote.\nPouvoir tester la significativité globale et individuelle de variables explicatives à l’aide de tests de rapport de vraisemblance."
  },
  {
    "objectID": "contenu/03-contenu.html",
    "href": "contenu/03-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Rappels sur la régression\nSurajustement\nMéthodes d’estimation de l’erreur:\n\npénalisation,\nvalidation croisée et\nvalidation externe"
  },
  {
    "objectID": "contenu/03-contenu.html#contenu",
    "href": "contenu/03-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Rappels sur la régression\nSurajustement\nMéthodes d’estimation de l’erreur:\n\npénalisation,\nvalidation croisée et\nvalidation externe"
  },
  {
    "objectID": "contenu/03-contenu.html#lecture",
    "href": "contenu/03-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur la sélection de variables et de modèles, Sections 1 à 3"
  },
  {
    "objectID": "contenu/03-contenu.html#diapositives",
    "href": "contenu/03-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Sélection de variables\n Code"
  },
  {
    "objectID": "contenu/03-contenu.html#objectifs-dapprentissage",
    "href": "contenu/03-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nÊtre capable d’ajuster un modèle linéaire avec R\nÊtre en mesure de décrire le suroptimisme et son impact sur l’évaluation de modèles\nPouvoir expliquer les avantages et inconvénients des méthodes d’estimation de l’erreur"
  },
  {
    "objectID": "contenu/01-contenu.html",
    "href": "contenu/01-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Plan de cours et objectifs d’apprentissage\nSurvol du contenu du cours\nConsidérations éthiques: discussion de l’article How Companies Learn Your Secrets, via ZoneCours\nAnalyse exploratoire des données\nIntroduction aux données manquantes"
  },
  {
    "objectID": "contenu/01-contenu.html#contenu",
    "href": "contenu/01-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Plan de cours et objectifs d’apprentissage\nSurvol du contenu du cours\nConsidérations éthiques: discussion de l’article How Companies Learn Your Secrets, via ZoneCours\nAnalyse exploratoire des données\nIntroduction aux données manquantes"
  },
  {
    "objectID": "contenu/01-contenu.html#avant-le-cours",
    "href": "contenu/01-contenu.html#avant-le-cours",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Avant le cours",
    "text": "Avant le cours\n\nSe familiariser avec le plan de cours, le contenu des séances, le forum en ligne, les exercices et les devoirs.\nS’inscrire sur Piazza."
  },
  {
    "objectID": "contenu/01-contenu.html#lectures",
    "href": "contenu/01-contenu.html#lectures",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lectures",
    "text": "Lectures\n\n\n\n\n\n\nAvertissement\n\n\n\nIdéalement, je vous encourage à lire les notes de cours avant de venir en classe. Cela vous permettra de venir avec des questions en tête et de demander des éclaircissements.\n\n\n\n Chapitre sur l’analyse exploratoire des données"
  },
  {
    "objectID": "contenu/01-contenu.html#diapositives",
    "href": "contenu/01-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Survol des sujets\n Graphiques et analyse exploratoire"
  },
  {
    "objectID": "contenu/01-contenu.html#activités",
    "href": "contenu/01-contenu.html#activités",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Activités",
    "text": "Activités\nAtelier de visualisation: vous devrez analyser des graphiques en équipes et donner une appréciation d’ensemble de l’efficacité du graphique pour transmettre une histoire en plus d’identifier les informations suivantes:\n\nles variables\nla dimension à laquelle elles sont affectées (axes des abscisse, axe des ordonnées, couleur, forme, taille, etc.)\nle type de représentation (diagramme en bâton, nuage de point, en violon, etc.)\nles forces et faiblesses des représentations graphiques (absence de légende, problème de lisibilité, etc.)"
  },
  {
    "objectID": "contenu/01-contenu.html#objectifs-dapprentissage",
    "href": "contenu/01-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nRéaliser une analyse exploratoire et formuler des questions en lien avec une base de données.\nDécomposer un graphique en éléments et déterminer la représentation adéquate pour une question donnée."
  },
  {
    "objectID": "contenu/01-contenu.html#ressources-complémentaires",
    "href": "contenu/01-contenu.html#ressources-complémentaires",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Ressources complémentaires",
    "text": "Ressources complémentaires\n\nChapitre 7 de R for Data Science par Garrett Grolemund et Hadley Wickham\nChapitre 1 de Data Visualization: A practical introduction par Kieran Healy\nFeuille aide-mémoire de DataViz"
  },
  {
    "objectID": "contenu/02-contenu.html",
    "href": "contenu/02-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Bases de la programmation R\nVisualisation graphique\nManipulation de bases de données"
  },
  {
    "objectID": "contenu/02-contenu.html#contenu",
    "href": "contenu/02-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Bases de la programmation R\nVisualisation graphique\nManipulation de bases de données"
  },
  {
    "objectID": "contenu/02-contenu.html#avant-le-cours",
    "href": "contenu/02-contenu.html#avant-le-cours",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Avant le cours",
    "text": "Avant le cours\n\nTéléchargez et installez R et RStudio en suivant le guide."
  },
  {
    "objectID": "contenu/02-contenu.html#diapositives",
    "href": "contenu/02-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\nChaque heure de la séance sera consacré à un sujet différent. Vous pouvez télécharger l’archive contenant les documents et les données pour les exercices réalisés en classe.\n\n\n\n\n\n\nAstuce\n\n\n\nSi vous tapez? (ou shift + /) en naviguant à travers la série de diapositives, vous obtiendrez une liste d’options.\n\n\n\n\n\n\n\n\n\n\n\n Ouvrir dans une nouvelle fenêtre  Télécharger au format PDF\n\n\n\n\n\n\n\n\n\n\n Ouvrir dans une nouvelle fenêtre  Télécharger au format PDF\n\n\n\n\n\n\n\n\n\n\n Ouvrir dans une nouvelle fenêtre  Télécharger au format PDF"
  },
  {
    "objectID": "contenu/02-contenu.html#vidéos",
    "href": "contenu/02-contenu.html#vidéos",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Vidéos",
    "text": "Vidéos\nLes vidéos pour cette section sont disponibles sur cette chaîne YouTube.\n\nIntroduction (partie 1)\nIntroduction (partie 2)\nManipulations\nGraphiques\n\nVous pouvez aussi visionner les différents vidéos de la chaîne (en passant d’une vidéo à l’autre) ici:"
  },
  {
    "objectID": "contenu/02-contenu.html#exercices",
    "href": "contenu/02-contenu.html#exercices",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Exercices",
    "text": "Exercices\n\n Exercices 1"
  },
  {
    "objectID": "contenu/02-contenu.html#objectifs-dapprentissage",
    "href": "contenu/02-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nPouvoir utiliser R à partir d’un environnement de développement intégré (EDI)\nÊtre en mesure de réaliser un graphique (histogramme, boîte à moustaches, nuage de points, diagramme à bande)\nIdentifier les éléments géométriques d’un graphique\nÊtre capable d’extraire des statistiques descriptives, manipuler, transformer, extraire des variables d’une base de données"
  },
  {
    "objectID": "contenu/02-contenu.html#ressources-complémentaires",
    "href": "contenu/02-contenu.html#ressources-complémentaires",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Ressources complémentaires",
    "text": "Ressources complémentaires\n\nMatériel de cours d’Aurélie Labbe\nChapitres 2 et 4 de R for Data Science"
  },
  {
    "objectID": "contenu/04-contenu.html",
    "href": "contenu/04-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Méthodes de sélection de variables: recherches exhaustives et procédures séquentielles\nModèle LASSO\nMoyenne de modèles"
  },
  {
    "objectID": "contenu/04-contenu.html#contenu",
    "href": "contenu/04-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Méthodes de sélection de variables: recherches exhaustives et procédures séquentielles\nModèle LASSO\nMoyenne de modèles"
  },
  {
    "objectID": "contenu/04-contenu.html#lecture",
    "href": "contenu/04-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur la sélection de variables et de modèles, Sections 4 à 6"
  },
  {
    "objectID": "contenu/04-contenu.html#diapositives",
    "href": "contenu/04-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Sélection de variables\n Code"
  },
  {
    "objectID": "contenu/04-contenu.html#exercices",
    "href": "contenu/04-contenu.html#exercices",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Exercices",
    "text": "Exercices\n\n Exercices 2"
  },
  {
    "objectID": "contenu/04-contenu.html#objectifs-dapprentissage",
    "href": "contenu/04-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nÊtre capable d’ajuster les procédures de sélections couvertes dans R\nPouvoir calculer la performance manuellement d’un modèle\nÊtre en mesure de diagnostiquer le surajustement et d’appliquer une procédure de sélection de variables"
  },
  {
    "objectID": "contenu/06-contenu.html",
    "href": "contenu/06-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Classification\nMatrice de gain et point de coupure\nMesures de la qualité de la classification: gain, fonction d’efficacité du récepteur, courbe lift\nSélection de modèle avec données binaires"
  },
  {
    "objectID": "contenu/06-contenu.html#contenu",
    "href": "contenu/06-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Classification\nMatrice de gain et point de coupure\nMesures de la qualité de la classification: gain, fonction d’efficacité du récepteur, courbe lift\nSélection de modèle avec données binaires"
  },
  {
    "objectID": "contenu/06-contenu.html#lecture",
    "href": "contenu/06-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur la régression logistique, Section 3"
  },
  {
    "objectID": "contenu/06-contenu.html#diapositives",
    "href": "contenu/06-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Régression logistique\n Code"
  },
  {
    "objectID": "contenu/06-contenu.html#objectifs-dapprentissage",
    "href": "contenu/06-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nÊtre capable de calculer la spécificité, la sensibilité, etc. d’une classification.\nDéterminer la qualité d’un modèle de classification à l’aide de diagnostics graphiques.\nPouvoir effectuer une sélection de variable avec des données binaires"
  },
  {
    "objectID": "contenu/08-contenu.html",
    "href": "contenu/08-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Introduction à l’analyse de survie\nEstimateur de Kaplan–Meier\nModèle à risques proportionnels de Cox"
  },
  {
    "objectID": "contenu/08-contenu.html#contenu",
    "href": "contenu/08-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Introduction à l’analyse de survie\nEstimateur de Kaplan–Meier\nModèle à risques proportionnels de Cox"
  },
  {
    "objectID": "contenu/08-contenu.html#lecture",
    "href": "contenu/08-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur l’analyse de survie, Sections 1 à 5"
  },
  {
    "objectID": "contenu/08-contenu.html#diapositives",
    "href": "contenu/08-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Analyse de survie\n Code"
  },
  {
    "objectID": "contenu/08-contenu.html#objectifs-dapprentissage",
    "href": "contenu/08-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nDécrire la censure à droite dans un problème de survie.\nPouvoir interpréter un tableau de survie et les estimations associées.\nInterpréter les coefficients d’un modèle de Cox."
  },
  {
    "objectID": "contenu/10-contenu.html",
    "href": "contenu/10-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Analyse en composantes principales\nAnalyse factorielle exploratoire"
  },
  {
    "objectID": "contenu/10-contenu.html#contenu",
    "href": "contenu/10-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Analyse en composantes principales\nAnalyse factorielle exploratoire"
  },
  {
    "objectID": "contenu/10-contenu.html#lecture",
    "href": "contenu/10-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur la réduction de la dimension"
  },
  {
    "objectID": "contenu/10-contenu.html#diapositives",
    "href": "contenu/10-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Réduction de la dimension\n Code"
  },
  {
    "objectID": "contenu/10-contenu.html#exercices",
    "href": "contenu/10-contenu.html#exercices",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Exercices",
    "text": "Exercices\n\n Exercices 5"
  },
  {
    "objectID": "contenu/10-contenu.html#objectifs-dapprentissage",
    "href": "contenu/10-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nReconnaître les situations où la réduction de la dimension est utile\nPouvoir déterminer le nombre adéquats de composantes principales ou de facteurs\nPouvoir lister les avantages et inconvénients des méthodes d’estimation pour l’analyse factorielle\nPouvoir calculer des échelles et vérifier leur cohérence interne."
  },
  {
    "objectID": "contenu/12-contenu.html",
    "href": "contenu/12-contenu.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Algorithmes de regroupements"
  },
  {
    "objectID": "contenu/12-contenu.html#contenu",
    "href": "contenu/12-contenu.html#contenu",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Algorithmes de regroupements"
  },
  {
    "objectID": "contenu/12-contenu.html#lecture",
    "href": "contenu/12-contenu.html#lecture",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Lecture",
    "text": "Lecture\n\n Chapitre sur l’analyse de regroupements, Section 7.5"
  },
  {
    "objectID": "contenu/12-contenu.html#diapositives",
    "href": "contenu/12-contenu.html#diapositives",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Diapositives",
    "text": "Diapositives\n\n Analyse de regroupements\n Code"
  },
  {
    "objectID": "contenu/12-contenu.html#objectifs-dapprentissage",
    "href": "contenu/12-contenu.html#objectifs-dapprentissage",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Objectifs d’apprentissage",
    "text": "Objectifs d’apprentissage\n\nComprendre les avantages et inconvénients de différents algorithmes de regroupement\nChoisir adéquatement les hyperparamètres associés aux algorithmes"
  },
  {
    "objectID": "contenu/installation.html",
    "href": "contenu/installation.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Nous utiliserons le langage de programmation libre-accès R et l’environnement de développement intégré RStudio comme porte d’accès à R.\n\nInstaller R\nPour commencer, on installe la dernière version de R (la machine qui fait les calculs), actuellement 4.3.2 (Eye Holes).\n\nAllez sur le site du Comprehensive R Archive Network (CRAN): https://cran.r-project.org/\nCliquez sur “Download R for XXX”, où XXX est Mac ou Windows:\n\n\n\n\n\n\nSi vous utilisez macOS, faites défiler le menu jusqu’au premier fichier .pkg de la liste et téléchargez-le.\n\n\n\n\n\nSi vous utilisez Windows, choisissez “base” (ou cliquez sur l’hyperlien en gras “install R for the first time”) et téléchargez le programme.\n\n\n\n\n\n\nDouble cliquez sur le fichier de téléchargement. Approuvez toutes les requêtes, comme pour n’importe quel autre programme.\n\n4a. Si vous utilisez macOS, téléchargez et installez XQuartz.\n4b. Si vous utilisez Windows, téléchargez et installez Rtools\n\n\nInstaller RStudio\nUne fois R installée, nous installerons une interface utilisateur graphique, RStudio, pour interagir avec R.\n\nNaviguez jusqu’à la section de téléchargements gratuits du site de RStudio: https://posit.co/download/rstudio-desktop/\n\nNaviguez vers le tableau et choisissez la version de RStudio Desktop adéquate.\n\n\n\n\n\n\nDouble-cliquez sur le fichier de téléchargement (potentiellement caché dans votre dossier de Téléchargements). Installez comme n’importe lequel autre logiciel.\n\nDouble-cliquez sur l’icône RStudio pour lancer l’application.\n\n\nInstaller tidyverse\nLes paquets R sont faciles à installer avec l’interface graphique. Sélectionnez le panneau “packages”, cliquez sur “Install,” tapez le nom du paquet que vous voulez installer et appuyer sur la touche Retour.\n\n\n\n\n\nLe paquet tidyverse est une collection d’une douzaine de paquets (incluant ggplot2 et dplyr) qui fonctionnent ensemble selon une syntaxe commune. L’installer (ou charger le paquet) inclura automatiquement toutes les dépendances associées\n\n\n\n\n\nNotez que RStudio génère le code R pour l’installation: install.packages(\"tidyverse\"). Vous pourriez utiliser directement cette fonction et faire copier-coller dans la console pour installer les paquets du tidyverse.\n\n\nInstaller Quarto\nQuarto est un système de publication de documents sous formats (Word, PDF, html) qui combine un bloc d’instructions définissant les paramètres du document (YAML), de même que des blocs codes (R, Python, Julia, …) et des blocs textes (Markdown, avec support pour \\(\\LaTeX\\)).\nVous pouvez installer Quarto en visitant le site du projet, en téléchargeant les fichiers d’installation\n\n\n\n\n\nUne fois l’utilitaire installé, installez le paquet R quarto via\n\ninstall.packages(\"quarto\")\n\n\n\nInstaller LaTeX via tinytex\nQuand vous créez un fichier Rmarkdown (.Rmd) ou Quarto (.qmd) pour créer un document reproductible qui inclut calculs et texte et que vous tricotez ce dernier en PDF, R utilise le programme de typographie scientifique LaTeX. L’installation la plus aisée pour ce dernier, si votre machine n’a pas déjà de suite LaTeX, est d’installer le paquet tinytex pour obtenir une version minimale qui prend moins d’espace.\nPour installer tinytex:\n\nUtilisez l’onglet Packages dans le panneau RStudio pour installer tinytex, comme n’importe quel autre paquet. Sinon, tapez install.packages(\"tinytex\") dans la console.\nExécutez tinytex::install_tinytex() dans la console.\nAttendez que le téléchargement soit complété. Vous devriez ensuite être en mesure de tricoter des documents PDF."
  },
  {
    "objectID": "evaluations/01-devoir-2022.html",
    "href": "evaluations/01-devoir-2022.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nLa base de données visaprem du paquet hecmulti contiennent les profils de 1294 clients d’une institution bancaire française avant l’entrée de la France dans la zone euro. Les données ont été collectées lors d’une enquête mensuelle.\n\nTransformez la variable catégorielle sexe en variable binaire entière avec homme=0, femme=1, et NA pour les valeurs manquantes.\nRé-étiquetez les situations familiales (famiq) selon que la personne est seule (seu) ou en couple (cou). Transformez les valeurs manquantes (inc pour inconnu) en NA (voir dplyr::na_if).\nÉliminez les observations pour lesquelles la variable age est manquante.\nCréez une colonne, nbjd, qui représente le nombre total de jours à débit des trois derniers mois. Éliminez les variables utilisées lors de la création de nbjd.\nConsidérez le nombre total de cartes ntcas. Y a-t-il des incohérences en lien avec les autres variables?\nQue représentent les variables manquantes résiduelles de zocnb? Indice: voir la question précédente.\n\nExpliquez pourquoi il serait logique de remplacer ces valeurs manquantes par des valeurs numériques (laquelle).\nEffectuez la modification.\n\nProduisez un histogramme de la variable ancienneté du compte (relat) avec ggplot. Que remarquez-vous?\nProduisez un nuage de point de relat et age et commentez (quel est le lien entre relat et age)?\n\nLa plupart des manipulations sont à effectuer directement; votre code et la base de données feront foi de votre travail.\nIndication: Si vous modifiez une variable, assurez-vous d’écraser la colonne existante (par exemple, la base de données devrait contenir une colonne sexe encodée 0L/1L/NA).\nVous devez remettre trois fichiers sur ZoneCours,\n\nun rapport au format PDF\nvotre code R ou un fichier Rmarkdown\nla base de données créée à la suite des manipulations, au format CSV (valeurs séparées par des virgules), à l’aide de la commande write.csv(db, file = \"d1_matricule.csv\") où db est le nom de votre base de données.\n\nTous vos fichiers seront nommés selon la convention d1_matricule.extension en remplaçant matricule par votre numéro d’étudiant(e) et .extension par .pdf, .R, .Rmd et .csv, selon le type du fichier.\nAssurez-vous que vous n’avez pas supprimé la variable matric.\nAstuces: vous pouvez utiliser la fonction knitr::purl() pour extraire le code R d’un fichier Rmarkdown (extension .Rmd)."
  },
  {
    "objectID": "evaluations/02-devoir-2022.html",
    "href": "evaluations/02-devoir-2022.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nLes observations de la base de données bjffacto sont tirées de l’article Bastian et al. (2014) et sont rattachées à une expérience en psychologie visant à corroborer l’hypothèse qu’il y a une coopération accrue entre individus sujets à une expérience traumatisante. La moitié des participant(e)s a dû plonger sa main dans un bain d’eau glacé, tandis que l’autre moitié a dû faire la même chose dans un bain d’eau tiède; les deux groupes devaient ensuite faire un jeu visant à identifier leur niveau de coopération.\nLa variable condition indique le groupe expérimental (zéro pour groupe contrôle, un pour douleur).\nIndication: utilisez la matrice de corrélation pour effectuer l’analyse factorielle.\nVous devez remettre\nNommez ces derniers selon la convention d2_matricule.pdf, d2_matricule.R, etc."
  },
  {
    "objectID": "evaluations/02-devoir-2022.html#footnotes",
    "href": "evaluations/02-devoir-2022.html#footnotes",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nVous pouvez extraire les variances des composantes principales à partir de eigen(...)$values.↩︎\nIl est possible que les signes soient différents, ou que l’ordre des facteurs soit différent. Cela ne devrait pas affecter votre conclusion.↩︎"
  },
  {
    "objectID": "evaluations/03-devoir-2022.html",
    "href": "evaluations/03-devoir-2022.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nCe travail est à réaliser en équipe (minimum deux, maximum quatre personnes).\nOn cherche à estimer le total du solde de la carte de crédit Visa Première (credm, en francs) à l’aide d’autres variables explicatives présentes dans la base de données visacredm; 25 données sont intentionnellement manquantes pour permettre d’évaluer vos modèles finaux et faire un classement des équipes.\nRésumez votre analyse en une page maximum (texte et graphiques): ne soulevez que les points importants.\nRapportez l’erreur moyenne quadratique pour les différents modèles estimés dans un tableau et justifiez adéquatement votre choix final.\nVous devez remettre les documents suivants:\nUtilisez la convention de nomenclature d3_matricule.extension, où matricule est le matricule de l’étudiant(e) qui soumet le rapport et extension est un de pdf, R, csv.\nLa base de données devrait contenir uniquement deux colonnes et les 25 lignes correspondant aux données manquantes pour credm:\nIndication : vérifiez votre base de données pour vous assurer de respecter les consignes (pénalités salées pour toute personne qui déroge aux consignes).\nAssurez-vous également que les prédictions sont sensées et cohérentes avec ce qui est présent dans la base de données (avez-vous des prédictions en dehors de l’étendue des données d’entraînement)? Vous ne devriez pas avoir de valeurs manquantes.\nVous serez évalués sur votre méthodologie, et non pas la performance relative de votre modèle par rapport à celles des autres étudiant(e)s : en revanche, les deux sont typiquement corrélées. Vous devez expliquer clairement votre démarche (méthodologie) dans votre rapport et décrire le modèle que vous avez retenu (méthode et nombre de variables final). Prenez garde au surajustement!"
  },
  {
    "objectID": "evaluations/03-devoir-2022.html#footnotes",
    "href": "evaluations/03-devoir-2022.html#footnotes",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nNotez que le modèle LASSO n’est pas ajusté par maximum de vraisemblance!↩︎\nDisponible à l’aide de la commande write.csv(..., file = \"d3_matricule.csv\", row.names = FALSE)↩︎"
  },
  {
    "objectID": "evaluations/05-devoir-2022.html",
    "href": "evaluations/05-devoir-2022.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Téléchargez la version PDF de l’énoncé\nCe travail est à réaliser en équipe (minimum deux, maximum quatre personnes).\nVous devez remettre les documents suivants:\n\nvotre rapport au format PDF\nvotre code R ou un fichier Rmarkdown\n\nUtilisez la convention de nomenclature d5_matricule.extension, où matricule est le matricule de l’étudiant(e) qui soumet le rapport et extension est un de pdf, R ou Rmd.\nDans le cadre d’un programme gouvernemental, on s’intéresse au nombre de semaines de paiements écoulé entre une perte d’emploi et un nouvel emploi pour des chômeurs et chômeuses âgé(e)s entre 18 et 65 ans, dans le but d’élaborer un programme de formation destiné au chômage de longue durée. La base de données chomage contient un échantillon de \\(n=2\\ 500\\) personnes inscrites à l’Assurance emploi entre mars 2020 et décembre 2021.\n\nduree: nombre de semaines de prestations payées par le programme d’assurance emploi.\ndureemin: nombre de semaines de prestations déjà écoulées au début de la collecte de données.\nage: variable catégorielle pour la tranche d’âge, soit [18,25), [25,50) et [50,65) ans.\nformation: variable catégorielle, soit aucune, secondaire, professionnel, collegial ou universitaire.\nsexe: variable catégorielle binaire; soit femme ou homme.\nretrait: 1 si la personne bénéficiaire se désinscrit du programme avant la fin de la période maximale de prestation parce qu’elle cesse de chercher un emploi, 2 si elle trouve un emploi, 0 sinon.\nmontant: montant hebdomadaire des prestations, allant jusqu’à 55% de la rémunération hebdomadaire avant perte d’emploi jusqu’à concurrence de 547$ par semaine.\nnheures: nombre d’heures cumulées à l’emploi avant d’être au chômage, soit un minimum de 14 semaines (420 heures) ou 22 semaines (700 heures) dépendamment du taux de chômage de la région et du nombre d’heures d’emploi assurables dans la période de référence.\nprestationmax: durée maximale pour les prestations\n\n\nOn s’intéresse d’abord au nombre de semaines qu’une personne passe au chômage. On suppose qu’aucun suivi n’est effectué par les employé(e)s de Service Canada au delà de la durée maximale de prestation.\n\nÀ quoi correspond la censure à droite dans l’étude?\nRapportez le nombre d’observations censurées.\n\nSupposons que l’on veuille comparer les courbes de survie selon le sexe et la formation (conjointement) en utilisant l’estimateur de Kaplan–Meier. Utilisez ce modèle pour répondre aux sous-questions suivantes:\n\nProduisez un graphique illustrant les courbes de survie des 10 sous-groupes.\nRapportez l’estimé de la survie à 30 semaines pour une femme de 62 ans qui dispose d’une formation technique (collégial) à l’aide de votre modèle.\n\nLa fonction survdiff ne permet pas de faire le test d’hypothèse d’égalité des courbes de survie en raison de la troncature à gauche, mais un modèle de régression à risques proportionnels de Cox permettrait de tester cette hypothèse.\n\nQuelles variables explicatives devriez-vous inclure pour ce faire dans le modèle et sous quelle forme?\n\nTestez l’égalité des courbes de survie à l’aide du test du log-rang (voir summary).\nÉcrivez les hypothèses nulle et alternative, la valeur de la statistique, la valeur-\\(p\\) et la conclusion du test.\n\nQuels sont les critères pour l’éligibilité aux paiements de l’assurance emploi? Expliquez en quoi l’estimation précédente est faussée à cause de ces omissions.\nDans le cadre de l’étude, il serait aussi opportun de considérer le temps avant que la personne ne retrouve un emploi. Construisez un modèle de Cox à risques proportionnels avec sexe, age, formation, montant. Utilisez ce dernier pour répondre aux questions suivantes.\n\nInterprétez l’effet de la rénumération hebdomadaire sur le risque de retrouver un emploi.\nToute chose étant égale par ailleurs, classez le temps médian avant le retour à l’emploi selon la formation, du plus long au plus court.\nÀ l’aide du test de score, vérifiez le postulat de risque proportionnels. Rapportez la valeur-\\(p\\) et les conclusions des tests."
  },
  {
    "objectID": "evaluations/final.html",
    "href": "evaluations/final.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "L’examen final est récapitulatif et d’une durée de 180 minutes. Aucune documentation écrite n’est permise.\nL’utilisation d’un ordinateur ou de tout autre matériel électronique est interdit. Une calculatrice non programmable est autorisée.\nTéléchargez le PDF de l’examen de pratique et le solutionnaire.\nL’examen compte pour 40% de la note finale du cours."
  },
  {
    "objectID": "evaluations/intra.html",
    "href": "evaluations/intra.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "L’examen intratrimestriel est d’une durée de 180 minutes. Aucune documentation écrite n’est permise.\nL’utilisation d’un ordinateur ou de tout autre matériel électronique est interdit. Une calculatrice non programmable est autorisée.\nTéléchargez le PDF de l’examen de pratique et le solutionnaire\nL’examen compte pour 30% de la note finale du cours."
  },
  {
    "objectID": "exercices/01-solution.html",
    "href": "exercices/01-solution.html",
    "title": "Analyse exploratoire",
    "section": "",
    "text": "On commence par charger la base de données\n\n\nCode\n# remotes::install_github(\"lbelzile/hecmulti\")\ndata(aerien, package = \"hecmulti\")\n# Consulter la fiche descriptive\n# ? hecmulti::aerien\n\n\n1. Formulez des questions en lien avec la description de la base de données préliminaires à l’analyse exploratoire.\nPour démarrer notre analyse exploratoire, on formule quelques questions. Par exemple:\n\nQuel est le profil socio-démographique des clients (âge, sexe, clientèle affaires)\nQuelle proportion de réponses au sondage sont manquantes et quel est le lien avec le type de vol (via distance_vol)? On peut penser que l’offre alimentaire dépend de la durée.\nDans quelle mesure le délai à l’arrivée impacte la satisfaction globale?\nQuel est le lien entre les réponses et le profil socio-démographique?\nEst-ce que les personnes qui voyagent en classe supérieure (ou pour des motifs d’affaires) sont plus ou moins critiques?\nEst-ce que les délais au départ ou à l’arrivée peuvent être négatifs (vol en avance)?\n\n2. Examinez la base de données; identifiez le type de variable et leur nature.\n\n\nCode\nstr(aerien)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   129880 obs. of  23 variables:\n $ sexe                           : Factor w/ 2 levels \"femme\",\"homme\": 1 1 2 2 1 2 1 1 2 1 ...\n $ loyaute_consommateur           : Factor w/ 2 levels \"non\",\"oui\": 2 2 1 2 2 2 2 2 2 2 ...\n $ age                            : int  52 36 20 44 49 16 77 43 47 46 ...\n $ type_deplacement               : Factor w/ 2 levels \"affaires\",\"personnel\": 1 1 1 1 1 1 1 1 1 1 ...\n $ classe                         : Factor w/ 3 levels \"affaire\",\"eco\",..: 2 1 2 1 2 2 1 1 2 1 ...\n $ distance_vol                   : num  160 2863 192 3377 1182 ...\n $ service_internet_en_vol        : int  5 1 2 0 2 3 5 2 5 2 ...\n $ temps_arrivee_depart_convenable: int  4 1 0 0 3 3 5 2 2 2 ...\n $ facilite_reservation_en_ligne  : int  3 3 2 0 4 3 5 2 2 2 ...\n $ localisation_porte             : int  4 1 4 2 3 3 5 2 2 2 ...\n $ nourriture                     : int  3 5 2 3 4 5 3 4 5 3 ...\n $ preenregistrement_en_ligne     : int  4 4 2 4 1 5 5 4 5 4 ...\n $ confort_siege                  : int  3 5 2 4 2 3 5 5 5 4 ...\n $ divertissement_en_vol          : int  5 4 2 1 2 5 5 4 5 4 ...\n $ service_embarquement           : int  5 4 4 1 2 4 5 4 2 4 ...\n $ service_espace_jambes          : int  5 4 1 1 2 3 5 4 2 4 ...\n $ gestion_bagages                : int  5 4 3 1 2 1 5 4 5 4 ...\n $ service_enregistrement         : int  2 3 2 3 4 1 4 5 3 5 ...\n $ service_vol                    : int  5 4 2 1 2 2 5 4 3 4 ...\n $ proprete                       : int  5 5 2 4 4 5 3 3 5 4 ...\n $ delai_depart_min               : int  50 0 0 0 0 0 0 77 1 28 ...\n $ delai_arrivee_min              : int  44 0 0 6 20 0 0 65 0 14 ...\n $ satisfaction                   : Factor w/ 2 levels \"non\",\"oui\": 2 2 1 2 2 2 2 2 2 2 ...\n\n\nCode\nsummary(aerien)\n\n\n    sexe       loyaute_consommateur      age         type_deplacement\n femme:65899   non: 23780           Min.   : 7.00   affaires :89693  \n homme:63981   oui:106100           1st Qu.:27.00   personnel:40187  \n                                    Median :40.00                    \n                                    Mean   :39.43                    \n                                    3rd Qu.:51.00                    \n                                    Max.   :85.00                    \n                                                                     \n     classe       distance_vol  service_internet_en_vol\n affaire:62160   Min.   :  31   Min.   :0.000          \n eco    :58309   1st Qu.: 414   1st Qu.:2.000          \n ecoplus: 9411   Median : 844   Median :3.000          \n                 Mean   :1190   Mean   :2.729          \n                 3rd Qu.:1744   3rd Qu.:4.000          \n                 Max.   :4983   Max.   :5.000          \n                                                       \n temps_arrivee_depart_convenable facilite_reservation_en_ligne\n Min.   :0.000                   Min.   :0.000                \n 1st Qu.:2.000                   1st Qu.:2.000                \n Median :3.000                   Median :3.000                \n Mean   :3.058                   Mean   :2.757                \n 3rd Qu.:4.000                   3rd Qu.:4.000                \n Max.   :5.000                   Max.   :5.000                \n                                                              \n localisation_porte   nourriture    preenregistrement_en_ligne confort_siege  \n Min.   :0.000      Min.   :0.000   Min.   :0.000              Min.   :0.000  \n 1st Qu.:2.000      1st Qu.:2.000   1st Qu.:2.000              1st Qu.:2.000  \n Median :3.000      Median :3.000   Median :3.000              Median :4.000  \n Mean   :2.977      Mean   :3.205   Mean   :3.253              Mean   :3.441  \n 3rd Qu.:4.000      3rd Qu.:4.000   3rd Qu.:4.000              3rd Qu.:5.000  \n Max.   :5.000      Max.   :5.000   Max.   :5.000              Max.   :5.000  \n                                                                              \n divertissement_en_vol service_embarquement service_espace_jambes\n Min.   :0.000         Min.   :0.000        Min.   :0.000        \n 1st Qu.:2.000         1st Qu.:2.000        1st Qu.:2.000        \n Median :4.000         Median :4.000        Median :4.000        \n Mean   :3.358         Mean   :3.383        Mean   :3.351        \n 3rd Qu.:4.000         3rd Qu.:4.000        3rd Qu.:4.000        \n Max.   :5.000         Max.   :5.000        Max.   :5.000        \n                                                                 \n gestion_bagages service_enregistrement  service_vol       proprete    \n Min.   :1.000   Min.   :0.000          Min.   :0.000   Min.   :0.000  \n 1st Qu.:3.000   1st Qu.:3.000          1st Qu.:3.000   1st Qu.:2.000  \n Median :4.000   Median :3.000          Median :4.000   Median :3.000  \n Mean   :3.632   Mean   :3.306          Mean   :3.642   Mean   :3.286  \n 3rd Qu.:5.000   3rd Qu.:4.000          3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.000          Max.   :5.000   Max.   :5.000  \n                                                                       \n delai_depart_min  delai_arrivee_min satisfaction\n Min.   :   0.00   Min.   :   0.00   non:73452   \n 1st Qu.:   0.00   1st Qu.:   0.00   oui:56428   \n Median :   0.00   Median :   0.00               \n Mean   :  14.71   Mean   :  15.09               \n 3rd Qu.:  12.00   3rd Qu.:  13.00               \n Max.   :1592.00   Max.   :1584.00               \n                   NA's   :393                   \n\n\nLa plupart des variables, dont les colonnes 7 à 20 qui contiennent les réponses au questionnaire, sont des variables entières (int) formées d’échelles de Likert de 1 à 5; les valeurs manquantes encodées 0. Le sexe, la classe, le type de déplacement sont des variables catégorielles nominales. La satisfaction globale et l’indice de fidélité du consommateur sont binaires et ordinales. La distance du vol est continue et encodée comme numérique (dbl). Les délais sont encodées avec des valeurs entières (valeur arrondie).\n3. Y a-t-il des valeurs manquantes? Si oui, pour quelles variables? Serait-il logique de les imputer par leur moyenne, dans le cas présent?\nOui, il y a des valeurs manquantes. Selon la description (voir la section Détails sous ?hecmulti::aerien), toutes les valeurs 0 dans les questionnaires encodent des chanmps non applicables. Les imputer par la moyenne serait donc illogique, puisque certains services (divertissement, nourriture) ne sont pas offerts dans ces vols. Selon ce qui nous intéresse, on pourrait faire une segmentation manuelle de la base de données pour prendre le sous-ensemble qui nous intéresse.\nIl serait possible en revanche d’imputer l’information pour les valeurs de délai (393 valeurs manquantes).\n4. Calculez les statistiques descriptives pour les variables continues et produisez des tableaux de fréquence ou de contingence pour les variables catégorielles. Que remarquez-vous?\nA minima, il faut transformer les zéros en valeurs manquantes pour éviter de fausser les résultats. Il serait aussi préférable de transformer les scores des items du questionnaire en variables catégorielles.\n\n\nCode\nlibrary(dplyr)\n# Remplacer valeurs manquantes (0 en NA)\naerien &lt;- aerien |&gt; \n  mutate_at(7:20, ~na_if(., 0))\n  # Statistiques descriptives\nsummary(aerien)\n\n\n    sexe       loyaute_consommateur      age         type_deplacement\n femme:65899   non: 23780           Min.   : 7.00   affaires :89693  \n homme:63981   oui:106100           1st Qu.:27.00   personnel:40187  \n                                    Median :40.00                    \n                                    Mean   :39.43                    \n                                    3rd Qu.:51.00                    \n                                    Max.   :85.00                    \n                                                                     \n     classe       distance_vol  service_internet_en_vol\n affaire:62160   Min.   :  31   Min.   :1.000          \n eco    :58309   1st Qu.: 414   1st Qu.:2.000          \n ecoplus: 9411   Median : 844   Median :3.000          \n                 Mean   :1190   Mean   :2.814          \n                 3rd Qu.:1744   3rd Qu.:4.000          \n                 Max.   :4983   Max.   :5.000          \n                                NA's   :3916           \n temps_arrivee_depart_convenable facilite_reservation_en_ligne\n Min.   :1.000                   Min.   :1.000                \n 1st Qu.:2.000                   1st Qu.:2.000                \n Median :3.000                   Median :3.000                \n Mean   :3.223                   Mean   :2.883                \n 3rd Qu.:4.000                   3rd Qu.:4.000                \n Max.   :5.000                   Max.   :5.000                \n NA's   :6681                    NA's   :5682                 \n localisation_porte   nourriture    preenregistrement_en_ligne confort_siege  \n Min.   :1.000      Min.   :1.000   Min.   :1.000              Min.   :1.000  \n 1st Qu.:2.000      1st Qu.:2.000   1st Qu.:2.000              1st Qu.:2.000  \n Median :3.000      Median :3.000   Median :4.000              Median :4.000  \n Mean   :2.977      Mean   :3.208   Mean   :3.332              Mean   :3.441  \n 3rd Qu.:4.000      3rd Qu.:4.000   3rd Qu.:4.000              3rd Qu.:5.000  \n Max.   :5.000      Max.   :5.000   Max.   :5.000              Max.   :5.000  \n NA's   :1          NA's   :132     NA's   :3080               NA's   :1      \n divertissement_en_vol service_embarquement service_espace_jambes\n Min.   :1.000         Min.   :1.000        Min.   :1.000        \n 1st Qu.:2.000         1st Qu.:2.000        1st Qu.:2.000        \n Median :4.000         Median :4.000        Median :4.000        \n Mean   :3.359         Mean   :3.383        Mean   :3.366        \n 3rd Qu.:4.000         3rd Qu.:4.000        3rd Qu.:4.000        \n Max.   :5.000         Max.   :5.000        Max.   :5.000        \n NA's   :18            NA's   :5            NA's   :598          \n gestion_bagages service_enregistrement  service_vol       proprete    \n Min.   :1.000   Min.   :1.000          Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:3.000          1st Qu.:3.000   1st Qu.:2.000  \n Median :4.000   Median :3.000          Median :4.000   Median :3.000  \n Mean   :3.632   Mean   :3.306          Mean   :3.642   Mean   :3.287  \n 3rd Qu.:5.000   3rd Qu.:4.000          3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.000          Max.   :5.000   Max.   :5.000  \n                 NA's   :1              NA's   :5       NA's   :14     \n delai_depart_min  delai_arrivee_min satisfaction\n Min.   :   0.00   Min.   :   0.00   non:73452   \n 1st Qu.:   0.00   1st Qu.:   0.00   oui:56428   \n Median :   0.00   Median :   0.00               \n Mean   :  14.71   Mean   :  15.09               \n 3rd Qu.:  12.00   3rd Qu.:  13.00               \n Max.   :1592.00   Max.   :1584.00               \n                   NA's   :393                   \n\n\nCode\n# Voir décompte pour éléments du questionnaire\naerien |&gt; \n  mutate_at(7:20, ~factor(., ordered = TRUE)) |&gt;\n  # Statistiques descriptives\n  summary()\n\n\n    sexe       loyaute_consommateur      age         type_deplacement\n femme:65899   non: 23780           Min.   : 7.00   affaires :89693  \n homme:63981   oui:106100           1st Qu.:27.00   personnel:40187  \n                                    Median :40.00                    \n                                    Mean   :39.43                    \n                                    3rd Qu.:51.00                    \n                                    Max.   :85.00                    \n                                                                     \n     classe       distance_vol  service_internet_en_vol\n affaire:62160   Min.   :  31   1   :22328             \n eco    :58309   1st Qu.: 414   2   :32320             \n ecoplus: 9411   Median : 844   3   :32185             \n                 Mean   :1190   4   :24775             \n                 3rd Qu.:1744   5   :14356             \n                 Max.   :4983   NA's: 3916             \n                                                       \n temps_arrivee_depart_convenable facilite_reservation_en_ligne\n 1   :19409                      1   :21886                   \n 2   :21534                      2   :30051                   \n 3   :22378                      3   :30393                   \n 4   :31880                      4   :24444                   \n 5   :27998                      5   :17424                   \n NA's: 6681                      NA's: 5682                   \n                                                              \n localisation_porte nourriture   preenregistrement_en_ligne confort_siege\n 1   :21991         1   :16051   1   :13261                 1   :15108   \n 2   :24296         2   :27383   2   :21934                 2   :18529   \n 3   :35717         3   :27794   3   :27117                 3   :23328   \n 4   :30466         4   :30563   4   :38468                 4   :39756   \n 5   :17409         5   :27957   5   :26020                 5   :33158   \n NA's:    1         NA's:  132   NA's: 3080                 NA's:    1   \n                                                                         \n divertissement_en_vol service_embarquement service_espace_jambes\n 1   :15675            1   :14787           1   :12895           \n 2   :21968            2   :18351           2   :24540           \n 3   :23884            3   :28542           3   :25056           \n 4   :36791            4   :38703           4   :35886           \n 5   :31544            5   :29492           5   :30905           \n NA's:   18            NA's:    5           NA's:  598           \n                                                                 \n gestion_bagages service_enregistrement service_vol  proprete    \n 1: 9028         1   :16108             1   : 8862   1   :16729  \n 2:14362         2   :16102             2   :14308   2   :20113  \n 3:25851         3   :35453             3   :25316   3   :30639  \n 4:46761         4   :36333             4   :47323   4   :33969  \n 5:33878         5   :25883             5   :34066   5   :28416  \n                 NA's:    1             NA's:    5   NA's:   14  \n                                                                 \n delai_depart_min  delai_arrivee_min satisfaction\n Min.   :   0.00   Min.   :   0.00   non:73452   \n 1st Qu.:   0.00   1st Qu.:   0.00   oui:56428   \n Median :   0.00   Median :   0.00               \n Mean   :  14.71   Mean   :  15.09               \n 3rd Qu.:  12.00   3rd Qu.:  13.00               \n Max.   :1592.00   Max.   :1584.00               \n                   NA's   :393                   \n\n\nCode\n# Moyenne et écart-type par variable\n# selon niveau de satisfaction\naerien |&gt;\n  group_by(satisfaction) |&gt;\n  summarize(across(where(is.numeric), \n            ~ mean(.x, na.rm = TRUE),\n            .names = \"moy_{.col}\")) |&gt; \n  # Pivoter tableau\n  tidyr::pivot_longer(cols = -1,\n                      names_to = \"variable\",\n                      names_prefix = \"moy_\",\n                      values_to = \"moyenne\") |&gt;\n  arrange(variable) |&gt; # trier\n  knitr::kable(digits = 2)\n\n\n\n\n\nsatisfaction\nvariable\nmoyenne\n\n\n\n\nnon\nage\n37.65\n\n\noui\nage\n41.74\n\n\nnon\nconfort_siege\n3.04\n\n\noui\nconfort_siege\n3.97\n\n\nnon\ndelai_arrivee_min\n17.06\n\n\noui\ndelai_arrivee_min\n12.53\n\n\nnon\ndelai_depart_min\n16.41\n\n\noui\ndelai_depart_min\n12.51\n\n\nnon\ndistance_vol\n929.72\n\n\noui\ndistance_vol\n1529.54\n\n\nnon\ndivertissement_en_vol\n2.89\n\n\noui\ndivertissement_en_vol\n3.96\n\n\nnon\nfacilite_reservation_en_ligne\n2.62\n\n\noui\nfacilite_reservation_en_ligne\n3.24\n\n\nnon\ngestion_bagages\n3.37\n\n\noui\ngestion_bagages\n3.97\n\n\nnon\nlocalisation_porte\n2.98\n\n\noui\nlocalisation_porte\n2.97\n\n\nnon\nnourriture\n2.96\n\n\noui\nnourriture\n3.53\n\n\nnon\npreenregistrement_en_ligne\n2.71\n\n\noui\npreenregistrement_en_ligne\n4.15\n\n\nnon\nproprete\n2.93\n\n\noui\nproprete\n3.75\n\n\nnon\nservice_embarquement\n3.02\n\n\noui\nservice_embarquement\n3.86\n\n\nnon\nservice_enregistrement\n3.04\n\n\noui\nservice_enregistrement\n3.65\n\n\nnon\nservice_espace_jambes\n3.01\n\n\noui\nservice_espace_jambes\n3.83\n\n\nnon\nservice_internet_en_vol\n2.40\n\n\noui\nservice_internet_en_vol\n3.39\n\n\nnon\nservice_vol\n3.39\n\n\noui\nservice_vol\n3.97\n\n\nnon\ntemps_arrivee_depart_convenable\n3.29\n\n\noui\ntemps_arrivee_depart_convenable\n3.14\n\n\n\n\n\nOn note que 82 % des résultats sont pour des clients. L’âge médian est de 40 ans, et la majorité des déplacements sont pour affaires, soit environ 69 pourcent. Il y a presque autant d’hommes que de femmes. Les personnes qui disposent d’une carte fidélité voyagent davantage en classe affaire (presque la moitié des vols).\n\n\nCode\naerien |&gt; \n  group_by(loyaute_consommateur, \n           classe) |&gt; \n  summarise(cnt = n()) |&gt;\n  mutate(freq = formattable::percent(cnt / sum(cnt))) |&gt;\n  knitr::kable()\n\n\n\n\n\nloyaute_consommateur\nclasse\ncnt\nfreq\n\n\n\n\nnon\naffaire\n9231\n38.82%\n\n\nnon\neco\n13634\n57.33%\n\n\nnon\necoplus\n915\n3.85%\n\n\noui\naffaire\n52929\n49.89%\n\n\noui\neco\n44675\n42.11%\n\n\noui\necoplus\n8496\n8.01%\n\n\n\n\n\nIl y a une forte corrélation entre certains items du questionnaire, ce qui est logique parce que ces questions capturent souvent des concepts communs (services, enregistrement, confort).\n\n\nCode\ncorrplot::corrplot(\n  corr = cor(aerien[,7:20], \n             use = \"pairwise.complete.obs\"), \n  diag = FALSE,\n  type = \"upper\", \n  tl.pos = \"n\") # pas de nom\n\n\n\n\n\n5. Représentez graphiquement la distribution de quelques variables de la base de données selon les différents niveaux de satisfaction.\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_classic())\ng1 &lt;- ggplot(data = aerien, \n       aes(x = loyaute_consommateur,\n           fill = satisfaction)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"\", \n       x = \"loyauté du consommateur\")\n\ng2 &lt;- aerien |&gt;\n  count(service_internet_en_vol, \n        satisfaction) |&gt;\n  group_by(satisfaction) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ggplot(aes(x = service_internet_en_vol,\n             y = pct,\n           fill = satisfaction)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge2\") +\n  labs(y = \"\", \n       subtitle = \"Pourcentage des réponses\",\n       x = \"service internet en vol\")\n\ng3 &lt;- aerien |&gt;\n  count(preenregistrement_en_ligne, \n        satisfaction) |&gt;\n  group_by(satisfaction) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ggplot(aes(x = preenregistrement_en_ligne,\n             y = pct,\n           fill = satisfaction)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge2\") +\n  labs(y = \"\", \n       subtitle = \"Pourcentage des réponses\",\n       x = \"préenregistrement en ligne\")\n\ng4 &lt;- ggplot(data = aerien, \n       aes(x = delai_arrivee_min,\n           fill = satisfaction)) +\n  geom_histogram(boundary = 0,\n                 binwidth = 2,\n                 position = \"dodge2\") +\n  coord_trans(x = 'log1p',\n              y = 'log1p') +\n  labs(y = \"\", \n       x = \"délai à l'arrivée (en minutes)\")\n\ng5 &lt;- ggplot(data = aerien,\n             aes(x = delai_depart_min,\n                 y = delai_arrivee_min),\n             alpha = 0.5) +\n  geom_point() +\n  labs(x = \"délais au départ (en minutes)\",\n       y = \"délais à l'arrivée (en minutes)\") +\n  coord_trans(x = \"log1p\",\n              y = \"log1p\")\n\nlibrary(patchwork)\n# Imprimer plusieurs graphiques\n(g1 + g2) / (g3 + g4) +\n  plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n\n\n\n\nCode\ng5\n\n\n\n\n\nOn voit clairement qu’il y a de grandes disparités pour la satisfaction. Il est peu surprenant de constater que les personnes qui ne sont pas globalement satisfaites accordent des notes plus faibles aux différents items du questionnaire. Certains items (accès internet en vol, facilité du préenregistrement) semblent avec un impact important sur la satisfaction globale.\nÀ mesure que les délais au départ augmentent, la relation avec le délai à l’arrivée se renforce, mais la corrélation linéaire est plus faible pour les petits retards. On voit que seuls sont enregistrés les retards (un vol qui arrive à l’avance est comptabilisé comme étant à l’heure avec une valeur de zéro).\n6. Résumez les faits saillants de votre analyse exploratoire en cinq à 10 points.\n\nSans surprise, les personnes qui ne sont globalement pas satisfaites accordent des notes plus faibles dans le questionnaire. Leurs vols sont en moyenne sur de plus courtes distances et ont de plus long délais à l’arrivée.\nLe nombre de vols en classe affaire est surreprésenté dans la base de données.\nLa clientèle affaire détient davantage de compte de fidélité; ce segment est moins critique des vols, mais pourrait ne pas bénéficier des même services.\nLes délais au départ et à l’arrivée ne sont comptabilisés que s’ils sont positifs.\nLes délais de retard importants à l’arrivée et au départ sont davantage corrélés à mesure que le retard augmente."
  },
  {
    "objectID": "exercices/01-solution.html#solution-1.1",
    "href": "exercices/01-solution.html#solution-1.1",
    "title": "Analyse exploratoire",
    "section": "",
    "text": "On commence par charger la base de données\n\n\nCode\n# remotes::install_github(\"lbelzile/hecmulti\")\ndata(aerien, package = \"hecmulti\")\n# Consulter la fiche descriptive\n# ? hecmulti::aerien\n\n\n1. Formulez des questions en lien avec la description de la base de données préliminaires à l’analyse exploratoire.\nPour démarrer notre analyse exploratoire, on formule quelques questions. Par exemple:\n\nQuel est le profil socio-démographique des clients (âge, sexe, clientèle affaires)\nQuelle proportion de réponses au sondage sont manquantes et quel est le lien avec le type de vol (via distance_vol)? On peut penser que l’offre alimentaire dépend de la durée.\nDans quelle mesure le délai à l’arrivée impacte la satisfaction globale?\nQuel est le lien entre les réponses et le profil socio-démographique?\nEst-ce que les personnes qui voyagent en classe supérieure (ou pour des motifs d’affaires) sont plus ou moins critiques?\nEst-ce que les délais au départ ou à l’arrivée peuvent être négatifs (vol en avance)?\n\n2. Examinez la base de données; identifiez le type de variable et leur nature.\n\n\nCode\nstr(aerien)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   129880 obs. of  23 variables:\n $ sexe                           : Factor w/ 2 levels \"femme\",\"homme\": 1 1 2 2 1 2 1 1 2 1 ...\n $ loyaute_consommateur           : Factor w/ 2 levels \"non\",\"oui\": 2 2 1 2 2 2 2 2 2 2 ...\n $ age                            : int  52 36 20 44 49 16 77 43 47 46 ...\n $ type_deplacement               : Factor w/ 2 levels \"affaires\",\"personnel\": 1 1 1 1 1 1 1 1 1 1 ...\n $ classe                         : Factor w/ 3 levels \"affaire\",\"eco\",..: 2 1 2 1 2 2 1 1 2 1 ...\n $ distance_vol                   : num  160 2863 192 3377 1182 ...\n $ service_internet_en_vol        : int  5 1 2 0 2 3 5 2 5 2 ...\n $ temps_arrivee_depart_convenable: int  4 1 0 0 3 3 5 2 2 2 ...\n $ facilite_reservation_en_ligne  : int  3 3 2 0 4 3 5 2 2 2 ...\n $ localisation_porte             : int  4 1 4 2 3 3 5 2 2 2 ...\n $ nourriture                     : int  3 5 2 3 4 5 3 4 5 3 ...\n $ preenregistrement_en_ligne     : int  4 4 2 4 1 5 5 4 5 4 ...\n $ confort_siege                  : int  3 5 2 4 2 3 5 5 5 4 ...\n $ divertissement_en_vol          : int  5 4 2 1 2 5 5 4 5 4 ...\n $ service_embarquement           : int  5 4 4 1 2 4 5 4 2 4 ...\n $ service_espace_jambes          : int  5 4 1 1 2 3 5 4 2 4 ...\n $ gestion_bagages                : int  5 4 3 1 2 1 5 4 5 4 ...\n $ service_enregistrement         : int  2 3 2 3 4 1 4 5 3 5 ...\n $ service_vol                    : int  5 4 2 1 2 2 5 4 3 4 ...\n $ proprete                       : int  5 5 2 4 4 5 3 3 5 4 ...\n $ delai_depart_min               : int  50 0 0 0 0 0 0 77 1 28 ...\n $ delai_arrivee_min              : int  44 0 0 6 20 0 0 65 0 14 ...\n $ satisfaction                   : Factor w/ 2 levels \"non\",\"oui\": 2 2 1 2 2 2 2 2 2 2 ...\n\n\nCode\nsummary(aerien)\n\n\n    sexe       loyaute_consommateur      age         type_deplacement\n femme:65899   non: 23780           Min.   : 7.00   affaires :89693  \n homme:63981   oui:106100           1st Qu.:27.00   personnel:40187  \n                                    Median :40.00                    \n                                    Mean   :39.43                    \n                                    3rd Qu.:51.00                    \n                                    Max.   :85.00                    \n                                                                     \n     classe       distance_vol  service_internet_en_vol\n affaire:62160   Min.   :  31   Min.   :0.000          \n eco    :58309   1st Qu.: 414   1st Qu.:2.000          \n ecoplus: 9411   Median : 844   Median :3.000          \n                 Mean   :1190   Mean   :2.729          \n                 3rd Qu.:1744   3rd Qu.:4.000          \n                 Max.   :4983   Max.   :5.000          \n                                                       \n temps_arrivee_depart_convenable facilite_reservation_en_ligne\n Min.   :0.000                   Min.   :0.000                \n 1st Qu.:2.000                   1st Qu.:2.000                \n Median :3.000                   Median :3.000                \n Mean   :3.058                   Mean   :2.757                \n 3rd Qu.:4.000                   3rd Qu.:4.000                \n Max.   :5.000                   Max.   :5.000                \n                                                              \n localisation_porte   nourriture    preenregistrement_en_ligne confort_siege  \n Min.   :0.000      Min.   :0.000   Min.   :0.000              Min.   :0.000  \n 1st Qu.:2.000      1st Qu.:2.000   1st Qu.:2.000              1st Qu.:2.000  \n Median :3.000      Median :3.000   Median :3.000              Median :4.000  \n Mean   :2.977      Mean   :3.205   Mean   :3.253              Mean   :3.441  \n 3rd Qu.:4.000      3rd Qu.:4.000   3rd Qu.:4.000              3rd Qu.:5.000  \n Max.   :5.000      Max.   :5.000   Max.   :5.000              Max.   :5.000  \n                                                                              \n divertissement_en_vol service_embarquement service_espace_jambes\n Min.   :0.000         Min.   :0.000        Min.   :0.000        \n 1st Qu.:2.000         1st Qu.:2.000        1st Qu.:2.000        \n Median :4.000         Median :4.000        Median :4.000        \n Mean   :3.358         Mean   :3.383        Mean   :3.351        \n 3rd Qu.:4.000         3rd Qu.:4.000        3rd Qu.:4.000        \n Max.   :5.000         Max.   :5.000        Max.   :5.000        \n                                                                 \n gestion_bagages service_enregistrement  service_vol       proprete    \n Min.   :1.000   Min.   :0.000          Min.   :0.000   Min.   :0.000  \n 1st Qu.:3.000   1st Qu.:3.000          1st Qu.:3.000   1st Qu.:2.000  \n Median :4.000   Median :3.000          Median :4.000   Median :3.000  \n Mean   :3.632   Mean   :3.306          Mean   :3.642   Mean   :3.286  \n 3rd Qu.:5.000   3rd Qu.:4.000          3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.000          Max.   :5.000   Max.   :5.000  \n                                                                       \n delai_depart_min  delai_arrivee_min satisfaction\n Min.   :   0.00   Min.   :   0.00   non:73452   \n 1st Qu.:   0.00   1st Qu.:   0.00   oui:56428   \n Median :   0.00   Median :   0.00               \n Mean   :  14.71   Mean   :  15.09               \n 3rd Qu.:  12.00   3rd Qu.:  13.00               \n Max.   :1592.00   Max.   :1584.00               \n                   NA's   :393                   \n\n\nLa plupart des variables, dont les colonnes 7 à 20 qui contiennent les réponses au questionnaire, sont des variables entières (int) formées d’échelles de Likert de 1 à 5; les valeurs manquantes encodées 0. Le sexe, la classe, le type de déplacement sont des variables catégorielles nominales. La satisfaction globale et l’indice de fidélité du consommateur sont binaires et ordinales. La distance du vol est continue et encodée comme numérique (dbl). Les délais sont encodées avec des valeurs entières (valeur arrondie).\n3. Y a-t-il des valeurs manquantes? Si oui, pour quelles variables? Serait-il logique de les imputer par leur moyenne, dans le cas présent?\nOui, il y a des valeurs manquantes. Selon la description (voir la section Détails sous ?hecmulti::aerien), toutes les valeurs 0 dans les questionnaires encodent des chanmps non applicables. Les imputer par la moyenne serait donc illogique, puisque certains services (divertissement, nourriture) ne sont pas offerts dans ces vols. Selon ce qui nous intéresse, on pourrait faire une segmentation manuelle de la base de données pour prendre le sous-ensemble qui nous intéresse.\nIl serait possible en revanche d’imputer l’information pour les valeurs de délai (393 valeurs manquantes).\n4. Calculez les statistiques descriptives pour les variables continues et produisez des tableaux de fréquence ou de contingence pour les variables catégorielles. Que remarquez-vous?\nA minima, il faut transformer les zéros en valeurs manquantes pour éviter de fausser les résultats. Il serait aussi préférable de transformer les scores des items du questionnaire en variables catégorielles.\n\n\nCode\nlibrary(dplyr)\n# Remplacer valeurs manquantes (0 en NA)\naerien &lt;- aerien |&gt; \n  mutate_at(7:20, ~na_if(., 0))\n  # Statistiques descriptives\nsummary(aerien)\n\n\n    sexe       loyaute_consommateur      age         type_deplacement\n femme:65899   non: 23780           Min.   : 7.00   affaires :89693  \n homme:63981   oui:106100           1st Qu.:27.00   personnel:40187  \n                                    Median :40.00                    \n                                    Mean   :39.43                    \n                                    3rd Qu.:51.00                    \n                                    Max.   :85.00                    \n                                                                     \n     classe       distance_vol  service_internet_en_vol\n affaire:62160   Min.   :  31   Min.   :1.000          \n eco    :58309   1st Qu.: 414   1st Qu.:2.000          \n ecoplus: 9411   Median : 844   Median :3.000          \n                 Mean   :1190   Mean   :2.814          \n                 3rd Qu.:1744   3rd Qu.:4.000          \n                 Max.   :4983   Max.   :5.000          \n                                NA's   :3916           \n temps_arrivee_depart_convenable facilite_reservation_en_ligne\n Min.   :1.000                   Min.   :1.000                \n 1st Qu.:2.000                   1st Qu.:2.000                \n Median :3.000                   Median :3.000                \n Mean   :3.223                   Mean   :2.883                \n 3rd Qu.:4.000                   3rd Qu.:4.000                \n Max.   :5.000                   Max.   :5.000                \n NA's   :6681                    NA's   :5682                 \n localisation_porte   nourriture    preenregistrement_en_ligne confort_siege  \n Min.   :1.000      Min.   :1.000   Min.   :1.000              Min.   :1.000  \n 1st Qu.:2.000      1st Qu.:2.000   1st Qu.:2.000              1st Qu.:2.000  \n Median :3.000      Median :3.000   Median :4.000              Median :4.000  \n Mean   :2.977      Mean   :3.208   Mean   :3.332              Mean   :3.441  \n 3rd Qu.:4.000      3rd Qu.:4.000   3rd Qu.:4.000              3rd Qu.:5.000  \n Max.   :5.000      Max.   :5.000   Max.   :5.000              Max.   :5.000  \n NA's   :1          NA's   :132     NA's   :3080               NA's   :1      \n divertissement_en_vol service_embarquement service_espace_jambes\n Min.   :1.000         Min.   :1.000        Min.   :1.000        \n 1st Qu.:2.000         1st Qu.:2.000        1st Qu.:2.000        \n Median :4.000         Median :4.000        Median :4.000        \n Mean   :3.359         Mean   :3.383        Mean   :3.366        \n 3rd Qu.:4.000         3rd Qu.:4.000        3rd Qu.:4.000        \n Max.   :5.000         Max.   :5.000        Max.   :5.000        \n NA's   :18            NA's   :5            NA's   :598          \n gestion_bagages service_enregistrement  service_vol       proprete    \n Min.   :1.000   Min.   :1.000          Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:3.000          1st Qu.:3.000   1st Qu.:2.000  \n Median :4.000   Median :3.000          Median :4.000   Median :3.000  \n Mean   :3.632   Mean   :3.306          Mean   :3.642   Mean   :3.287  \n 3rd Qu.:5.000   3rd Qu.:4.000          3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :5.000          Max.   :5.000   Max.   :5.000  \n                 NA's   :1              NA's   :5       NA's   :14     \n delai_depart_min  delai_arrivee_min satisfaction\n Min.   :   0.00   Min.   :   0.00   non:73452   \n 1st Qu.:   0.00   1st Qu.:   0.00   oui:56428   \n Median :   0.00   Median :   0.00               \n Mean   :  14.71   Mean   :  15.09               \n 3rd Qu.:  12.00   3rd Qu.:  13.00               \n Max.   :1592.00   Max.   :1584.00               \n                   NA's   :393                   \n\n\nCode\n# Voir décompte pour éléments du questionnaire\naerien |&gt; \n  mutate_at(7:20, ~factor(., ordered = TRUE)) |&gt;\n  # Statistiques descriptives\n  summary()\n\n\n    sexe       loyaute_consommateur      age         type_deplacement\n femme:65899   non: 23780           Min.   : 7.00   affaires :89693  \n homme:63981   oui:106100           1st Qu.:27.00   personnel:40187  \n                                    Median :40.00                    \n                                    Mean   :39.43                    \n                                    3rd Qu.:51.00                    \n                                    Max.   :85.00                    \n                                                                     \n     classe       distance_vol  service_internet_en_vol\n affaire:62160   Min.   :  31   1   :22328             \n eco    :58309   1st Qu.: 414   2   :32320             \n ecoplus: 9411   Median : 844   3   :32185             \n                 Mean   :1190   4   :24775             \n                 3rd Qu.:1744   5   :14356             \n                 Max.   :4983   NA's: 3916             \n                                                       \n temps_arrivee_depart_convenable facilite_reservation_en_ligne\n 1   :19409                      1   :21886                   \n 2   :21534                      2   :30051                   \n 3   :22378                      3   :30393                   \n 4   :31880                      4   :24444                   \n 5   :27998                      5   :17424                   \n NA's: 6681                      NA's: 5682                   \n                                                              \n localisation_porte nourriture   preenregistrement_en_ligne confort_siege\n 1   :21991         1   :16051   1   :13261                 1   :15108   \n 2   :24296         2   :27383   2   :21934                 2   :18529   \n 3   :35717         3   :27794   3   :27117                 3   :23328   \n 4   :30466         4   :30563   4   :38468                 4   :39756   \n 5   :17409         5   :27957   5   :26020                 5   :33158   \n NA's:    1         NA's:  132   NA's: 3080                 NA's:    1   \n                                                                         \n divertissement_en_vol service_embarquement service_espace_jambes\n 1   :15675            1   :14787           1   :12895           \n 2   :21968            2   :18351           2   :24540           \n 3   :23884            3   :28542           3   :25056           \n 4   :36791            4   :38703           4   :35886           \n 5   :31544            5   :29492           5   :30905           \n NA's:   18            NA's:    5           NA's:  598           \n                                                                 \n gestion_bagages service_enregistrement service_vol  proprete    \n 1: 9028         1   :16108             1   : 8862   1   :16729  \n 2:14362         2   :16102             2   :14308   2   :20113  \n 3:25851         3   :35453             3   :25316   3   :30639  \n 4:46761         4   :36333             4   :47323   4   :33969  \n 5:33878         5   :25883             5   :34066   5   :28416  \n                 NA's:    1             NA's:    5   NA's:   14  \n                                                                 \n delai_depart_min  delai_arrivee_min satisfaction\n Min.   :   0.00   Min.   :   0.00   non:73452   \n 1st Qu.:   0.00   1st Qu.:   0.00   oui:56428   \n Median :   0.00   Median :   0.00               \n Mean   :  14.71   Mean   :  15.09               \n 3rd Qu.:  12.00   3rd Qu.:  13.00               \n Max.   :1592.00   Max.   :1584.00               \n                   NA's   :393                   \n\n\nCode\n# Moyenne et écart-type par variable\n# selon niveau de satisfaction\naerien |&gt;\n  group_by(satisfaction) |&gt;\n  summarize(across(where(is.numeric), \n            ~ mean(.x, na.rm = TRUE),\n            .names = \"moy_{.col}\")) |&gt; \n  # Pivoter tableau\n  tidyr::pivot_longer(cols = -1,\n                      names_to = \"variable\",\n                      names_prefix = \"moy_\",\n                      values_to = \"moyenne\") |&gt;\n  arrange(variable) |&gt; # trier\n  knitr::kable(digits = 2)\n\n\n\n\n\nsatisfaction\nvariable\nmoyenne\n\n\n\n\nnon\nage\n37.65\n\n\noui\nage\n41.74\n\n\nnon\nconfort_siege\n3.04\n\n\noui\nconfort_siege\n3.97\n\n\nnon\ndelai_arrivee_min\n17.06\n\n\noui\ndelai_arrivee_min\n12.53\n\n\nnon\ndelai_depart_min\n16.41\n\n\noui\ndelai_depart_min\n12.51\n\n\nnon\ndistance_vol\n929.72\n\n\noui\ndistance_vol\n1529.54\n\n\nnon\ndivertissement_en_vol\n2.89\n\n\noui\ndivertissement_en_vol\n3.96\n\n\nnon\nfacilite_reservation_en_ligne\n2.62\n\n\noui\nfacilite_reservation_en_ligne\n3.24\n\n\nnon\ngestion_bagages\n3.37\n\n\noui\ngestion_bagages\n3.97\n\n\nnon\nlocalisation_porte\n2.98\n\n\noui\nlocalisation_porte\n2.97\n\n\nnon\nnourriture\n2.96\n\n\noui\nnourriture\n3.53\n\n\nnon\npreenregistrement_en_ligne\n2.71\n\n\noui\npreenregistrement_en_ligne\n4.15\n\n\nnon\nproprete\n2.93\n\n\noui\nproprete\n3.75\n\n\nnon\nservice_embarquement\n3.02\n\n\noui\nservice_embarquement\n3.86\n\n\nnon\nservice_enregistrement\n3.04\n\n\noui\nservice_enregistrement\n3.65\n\n\nnon\nservice_espace_jambes\n3.01\n\n\noui\nservice_espace_jambes\n3.83\n\n\nnon\nservice_internet_en_vol\n2.40\n\n\noui\nservice_internet_en_vol\n3.39\n\n\nnon\nservice_vol\n3.39\n\n\noui\nservice_vol\n3.97\n\n\nnon\ntemps_arrivee_depart_convenable\n3.29\n\n\noui\ntemps_arrivee_depart_convenable\n3.14\n\n\n\n\n\nOn note que 82 % des résultats sont pour des clients. L’âge médian est de 40 ans, et la majorité des déplacements sont pour affaires, soit environ 69 pourcent. Il y a presque autant d’hommes que de femmes. Les personnes qui disposent d’une carte fidélité voyagent davantage en classe affaire (presque la moitié des vols).\n\n\nCode\naerien |&gt; \n  group_by(loyaute_consommateur, \n           classe) |&gt; \n  summarise(cnt = n()) |&gt;\n  mutate(freq = formattable::percent(cnt / sum(cnt))) |&gt;\n  knitr::kable()\n\n\n\n\n\nloyaute_consommateur\nclasse\ncnt\nfreq\n\n\n\n\nnon\naffaire\n9231\n38.82%\n\n\nnon\neco\n13634\n57.33%\n\n\nnon\necoplus\n915\n3.85%\n\n\noui\naffaire\n52929\n49.89%\n\n\noui\neco\n44675\n42.11%\n\n\noui\necoplus\n8496\n8.01%\n\n\n\n\n\nIl y a une forte corrélation entre certains items du questionnaire, ce qui est logique parce que ces questions capturent souvent des concepts communs (services, enregistrement, confort).\n\n\nCode\ncorrplot::corrplot(\n  corr = cor(aerien[,7:20], \n             use = \"pairwise.complete.obs\"), \n  diag = FALSE,\n  type = \"upper\", \n  tl.pos = \"n\") # pas de nom\n\n\n\n\n\n5. Représentez graphiquement la distribution de quelques variables de la base de données selon les différents niveaux de satisfaction.\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_classic())\ng1 &lt;- ggplot(data = aerien, \n       aes(x = loyaute_consommateur,\n           fill = satisfaction)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"\", \n       x = \"loyauté du consommateur\")\n\ng2 &lt;- aerien |&gt;\n  count(service_internet_en_vol, \n        satisfaction) |&gt;\n  group_by(satisfaction) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ggplot(aes(x = service_internet_en_vol,\n             y = pct,\n           fill = satisfaction)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge2\") +\n  labs(y = \"\", \n       subtitle = \"Pourcentage des réponses\",\n       x = \"service internet en vol\")\n\ng3 &lt;- aerien |&gt;\n  count(preenregistrement_en_ligne, \n        satisfaction) |&gt;\n  group_by(satisfaction) |&gt;\n  mutate(pct = n / sum(n)) |&gt;\n  ggplot(aes(x = preenregistrement_en_ligne,\n             y = pct,\n           fill = satisfaction)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge2\") +\n  labs(y = \"\", \n       subtitle = \"Pourcentage des réponses\",\n       x = \"préenregistrement en ligne\")\n\ng4 &lt;- ggplot(data = aerien, \n       aes(x = delai_arrivee_min,\n           fill = satisfaction)) +\n  geom_histogram(boundary = 0,\n                 binwidth = 2,\n                 position = \"dodge2\") +\n  coord_trans(x = 'log1p',\n              y = 'log1p') +\n  labs(y = \"\", \n       x = \"délai à l'arrivée (en minutes)\")\n\ng5 &lt;- ggplot(data = aerien,\n             aes(x = delai_depart_min,\n                 y = delai_arrivee_min),\n             alpha = 0.5) +\n  geom_point() +\n  labs(x = \"délais au départ (en minutes)\",\n       y = \"délais à l'arrivée (en minutes)\") +\n  coord_trans(x = \"log1p\",\n              y = \"log1p\")\n\nlibrary(patchwork)\n# Imprimer plusieurs graphiques\n(g1 + g2) / (g3 + g4) +\n  plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n\n\n\n\nCode\ng5\n\n\n\n\n\nOn voit clairement qu’il y a de grandes disparités pour la satisfaction. Il est peu surprenant de constater que les personnes qui ne sont pas globalement satisfaites accordent des notes plus faibles aux différents items du questionnaire. Certains items (accès internet en vol, facilité du préenregistrement) semblent avec un impact important sur la satisfaction globale.\nÀ mesure que les délais au départ augmentent, la relation avec le délai à l’arrivée se renforce, mais la corrélation linéaire est plus faible pour les petits retards. On voit que seuls sont enregistrés les retards (un vol qui arrive à l’avance est comptabilisé comme étant à l’heure avec une valeur de zéro).\n6. Résumez les faits saillants de votre analyse exploratoire en cinq à 10 points.\n\nSans surprise, les personnes qui ne sont globalement pas satisfaites accordent des notes plus faibles dans le questionnaire. Leurs vols sont en moyenne sur de plus courtes distances et ont de plus long délais à l’arrivée.\nLe nombre de vols en classe affaire est surreprésenté dans la base de données.\nLa clientèle affaire détient davantage de compte de fidélité; ce segment est moins critique des vols, mais pourrait ne pas bénéficier des même services.\nLes délais au départ et à l’arrivée ne sont comptabilisés que s’ils sont positifs.\nLes délais de retard importants à l’arrivée et au départ sont davantage corrélés à mesure que le retard augmente."
  },
  {
    "objectID": "exercices/02-solution.html",
    "href": "exercices/02-solution.html",
    "title": "Sélection de variables",
    "section": "",
    "text": "Faites une analyse exploratoire des variables explicatives:\n\nQuelles variables devraient êtres exclues de la modélisation? Justifiez votre réponse.\nComparez la variable réponse avec les autres variables: y a-t-il des transformations qui améliorerait l’ensemble de variables candidates: interactions, création de variables dychotomiques, transformations (racines carrée, transformation logarithmique, etc.)?\nVérifiez s’il y a des variables catégorielles encodées comme des variables numériques.\n\n\nOn remarque en consultant la documentation de la base de données à l’aide de ?college que le nombre de demandes d’admissions (napplications), le nombre d’admissions parmi ces applications nadmission et le nombre d’offres converties par les candidat.e.s, ninscrits, sont reliées et ne peuvent être employées.\nPlusieurs variables seront fortement corrélées parce qu’elles dépendent de la capacité d’accueil de l’établissement d’enseignement. Ainsi, on pourrait créer une variable qui représente le pourcentage de temps partiels pour les premiers cycles (plutôt que le décompte). Les variables pourcentdoctorat et pourcentterminal, sont fortement corrélées puisque la plupart des diplômes terminaux (incluant les titres professionnels, les doctorats de premier cycle en médecine, etc.) sont des PhD: le modèle choisira la variable la plus adéquate. Il n’y a pas de variables catégorielles hors variables binaires. Côté vérifications d’usage, on note que tauxdiplom et pourcentdoctorat sont supérieurs à 100%.\nOn devra retirer la variable catégorielle nom, qui a une modalité différente pour chaque observation.\n\n\nCode\nlibrary(dplyr)\nlibrary(hecmulti)\ndata(college, package = \"hecmulti\")\nstr(college)\n\n\ntibble [777 × 19] (S3: tbl_df/tbl/data.frame)\n $ prive                  : int [1:777] 1 1 1 1 1 1 1 1 1 1 ...\n $ napplications          : num [1:777] 1660 2186 1428 417 193 ...\n $ nadmission             : num [1:777] 1232 1924 1097 349 146 ...\n $ ninscrits              : num [1:777] 721 512 336 137 55 158 103 489 227 172 ...\n $ m10p                   : num [1:777] 23 16 22 60 16 38 17 37 30 21 ...\n $ m25p                   : num [1:777] 52 29 50 89 44 62 45 68 63 44 ...\n $ tempsplein1c           : num [1:777] 2885 2683 1036 510 249 ...\n $ tempspart1c            : num [1:777] 537 1227 99 63 869 ...\n $ fraiscolexternes       : num [1:777] 7440 12280 11250 12960 7560 ...\n $ fraisres               : num [1:777] 3300 6450 3750 5450 4120 ...\n $ fraislivres            : num [1:777] 450 750 400 450 800 500 500 450 300 660 ...\n $ fraisperso             : num [1:777] 2200 1500 1165 875 1500 ...\n $ pourcentdoctorat       : num [1:777] 70 29 53 92 76 67 90 89 79 40 ...\n $ pourcentterminal       : num [1:777] 78 30 66 97 72 73 93 100 84 41 ...\n $ ratioetudprof          : num [1:777] 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...\n $ pourcentdonationdiplome: num [1:777] 12 16 30 37 2 11 26 37 23 15 ...\n $ depenseparetud         : num [1:777] 7041 10527 8735 19016 10922 ...\n $ tauxdiplom             : num [1:777] 60 56 54 59 15 55 63 73 80 52 ...\n $ nom                    : Factor w/ 777 levels \"Abilene Christian University\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\n\nCode\nsummary(college)\n\n\n     prive        napplications     nadmission      ninscrits   \n Min.   :0.0000   Min.   :   81   Min.   :   72   Min.   :  35  \n 1st Qu.:0.0000   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242  \n Median :1.0000   Median : 1558   Median : 1110   Median : 434  \n Mean   :0.7272   Mean   : 3002   Mean   : 2019   Mean   : 780  \n 3rd Qu.:1.0000   3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902  \n Max.   :1.0000   Max.   :48094   Max.   :26330   Max.   :6392  \n                                                                \n      m10p            m25p        tempsplein1c    tempspart1c     \n Min.   : 1.00   Min.   :  9.0   Min.   :  139   Min.   :    1.0  \n 1st Qu.:15.00   1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0  \n Median :23.00   Median : 54.0   Median : 1707   Median :  353.0  \n Mean   :27.56   Mean   : 55.8   Mean   : 3700   Mean   :  855.3  \n 3rd Qu.:35.00   3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0  \n Max.   :96.00   Max.   :100.0   Max.   :31643   Max.   :21836.0  \n                                                                  \n fraiscolexternes    fraisres     fraislivres       fraisperso  \n Min.   : 2340    Min.   :1780   Min.   :  96.0   Min.   : 250  \n 1st Qu.: 7320    1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850  \n Median : 9990    Median :4200   Median : 500.0   Median :1200  \n Mean   :10441    Mean   :4358   Mean   : 549.4   Mean   :1341  \n 3rd Qu.:12925    3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700  \n Max.   :21700    Max.   :8124   Max.   :2340.0   Max.   :6800  \n                                                                \n pourcentdoctorat pourcentterminal ratioetudprof   pourcentdonationdiplome\n Min.   :  8.00   Min.   : 24.0    Min.   : 2.50   Min.   : 0.00          \n 1st Qu.: 62.00   1st Qu.: 71.0    1st Qu.:11.50   1st Qu.:13.00          \n Median : 75.00   Median : 82.0    Median :13.60   Median :21.00          \n Mean   : 72.66   Mean   : 79.7    Mean   :14.09   Mean   :22.74          \n 3rd Qu.: 85.00   3rd Qu.: 92.0    3rd Qu.:16.50   3rd Qu.:31.00          \n Max.   :103.00   Max.   :100.0    Max.   :39.80   Max.   :64.00          \n                                                                          \n depenseparetud    tauxdiplom                               nom     \n Min.   : 3186   Min.   : 10.00   Abilene Christian University:  1  \n 1st Qu.: 6751   1st Qu.: 53.00   Adelphi University          :  1  \n Median : 8377   Median : 65.00   Adrian College              :  1  \n Mean   : 9660   Mean   : 65.46   Agnes Scott College         :  1  \n 3rd Qu.:10830   3rd Qu.: 78.00   Alaska Pacific University   :  1  \n Max.   :56233   Max.   :118.00   Albertson College           :  1  \n                                  (Other)                     :771  \n\n\nCode\n# Est-ce que pourcentdoctorat &lt; pourcenterminal?\nsummary(with(college,\n             pourcentdoctorat/pourcentterminal))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1379  0.8824  0.9390  0.9112  0.9775  1.9167 \n\n\nCode\n# Non\ndb &lt;- college |&gt;\n  mutate(\n    tauxdiplom = pmin(tauxdiplom, 100),\n    pourcentdoctorat = pmin(pourcentdoctorat, 100),\n    pctpart1c = tempspart1c/(tempsplein1c+tempspart1c)) |&gt;\n  select(! c(nom, \n             tempsplein1c, \n             tempspart1c, \n             nadmission, \n             ninscrits))\n\n\n\nScindez la base de données en échantillon avec données d’entraînement (environ 2/3 des données) et échantillon de validation; utilisez le germe aléatoire 60602 via set.seed(60602).\n\nSélectionnez un modèle à l’aide d’une des méthodes couvertes, mais en basant votre choix sur l’erreur moyenne quadratique évaluée sur l’échantillon de validation.\n\n\nOn peut désormais considérer une séparation en tiers. Pour ce faire, je vais échantillonner des variables logiques vrais et faux avec une cote de 2 pour 1 et ensuite sélectionner les lignes qui correspondent.\n\n\nCode\nset.seed(60602)\ntest &lt;- sample(x = c(FALSE, TRUE),\n               size = nrow(db),\n               replace = TRUE, \n               prob = c(1/3, 2/3))\ndb_a &lt;- db[test,]\ndb_v &lt;- db[!test,]\n\n\nPour l’estimation, on pourrait simplement calculer l’erreur quadratique moyenne de validation pour tous les cinquante premiers modèles en considérant toutes les interactions d’ordre 2.\n\n\nCode\n# Modèle avec interactions de toutes les variables\nformule &lt;- formula(napplications ~ .^2)\n\n# Sélection séquentielle ascendante\nrec_seq &lt;- leaps::regsubsets(\n  x = formule, \n  data = db_a,\n  method = \"seqrep\",\n  nvmax = 50)\n\n\nSi regsubsets permet de recouvrer les différents modèles, il n’y a pas d’utilitaire pour obtenir le résumé et les prédictions. Le paquet hecmulti inclut une fonction pour faire les prédictions d’un modèle donné. À des fins d’illustration, on considère celui qui a le plus petit critère BIC intra-échantillon d’apprentissage.\n\n\nCode\n# Calculer les BIC \nbic_mod &lt;- summary(rec_seq)$bic\n# Prédire du modèle avec id variables\n# Ici, celui avec le plus petit BIC du lot\npredict(rec_seq, id = which.min(bic_mod))\n\n\nIci, on s’intéresse uniquement au calcul de l’erreur quadratique moyenne pour les modèles choisis par la procédure de recherche séquentielle. La fonction eval_EQM_regsubsets fait le calcul et nous retourne un vecteur avec toutes les mesures. On choisit le modèle qui minimise l’erreur.\n\n\nCode\n# Modèle avec la plus petite erreur \n# moyenne quadratique de validation externe\nreqm_seq_ve_list &lt;- \n  hecmulti::eval_EQM_regsubsets(\n  model = rec_seq, \n  select = 1:50,\n  formula = formule,\n  data = db_a,\n  newdata = db_v)\n# La fonction calcule la racine EQM sur \n# les données de validation\nmod_seq_ve &lt;- which.min(reqm_seq_ve_list)\nnvar_seq_ve &lt;- mod_seq_ve + 1 \n# nombre variables + ordonnée à l'origine.\nreqm_seq_ve &lt;- reqm_seq_ve_list[mod_seq_ve]\n\n\nUne logique similaire s’appliquerait avec le LASSO, même si c’est loin d’être la norme (on utilise d’ordinaire la validation croisée). Ici, on ajusterait le modèle avec plusieurs valeurs de \\(\\lambda\\), puis on calculerait l’erreur quadratique moyenne de validation. On prendra la valeur de la pénalisation \\(\\lambda\\) qui minimise cette dernière. Spécifier la séquence de valeurs de \\(\\lambda\\) à essayer nécessite un peu d’essai/erreur.\n\n\nCode\n# Obtenir X et y\nXmat_a &lt;- model.matrix(formule, data = db_a)[,-1]\ny_a &lt;- db_a$napplications\nlambda_seq &lt;- exp(seq(from = log(0.1),\n                  to = log(50), \n                  length.out = 100L))\nlasso &lt;- glmnet::glmnet(\n    x = Xmat_a,\n    y = y_a,\n    alpha = 1, \n    lambda = lambda_seq)\n# Prédictions et calcul de l'EQM\n# On pourrait remplacer `newx` par \n# d'autres données (validation externe)\neqm_lasso_externe &lt;- rep(0, length(lambda_seq))\nfor(i in seq_along(lambda_seq)){\n  pred &lt;- predict(\n    object = lasso, \n    s = lambda_seq[i], \n    newx = model.matrix(formule, data = db_v)[,-1])\n  eqm_lasso_externe[i] &lt;- \n    mean((pred - db_v$napplications)^2)\n}\nminl &lt;- which.min(eqm_lasso_externe)\nlambda_opt &lt;- lambda_seq[minl]\n# Nombre de coefficients non nuls\nnvar_lasso_ve &lt;- Matrix::nnzero(lasso$beta[,minl, drop = FALSE])\nreqm_lasso_ve &lt;- sqrt(eqm_lasso_externe[minl])\n\n\n\nRépétez la sélection, cette fois en prenant comme critère pour l’erreur moyenne quadratique évaluée par validation croisée (aléatoire) à cinq plis.\n\nOn peut utiliser boot::cv.glm ou caret::train avec les différentes méthodes pour ajuster les modèles. Je fais varier le nombre maximal de variables pour retourner les différentes solutions. Comme le résultat de la validation croisée est aléatoire, on peut répéter cette étape pour réduire l’incertitude de prédiction et obtenir une meilleur valeur de l’écart-type.\n\n\nCode\n# Matrice du modèle complet, moins ordonnée à l'origine\nXmat &lt;- model.matrix(formule, data = db)[,-1]\n# Variable réponse\ny &lt;- db$napplications\nseq_vc &lt;- \n  caret::train(form = formule,\n               data = db,\n               method = 'leapSeq',\n             tuneGrid = expand.grid(nvmax = 1:50),\n             trControl = caret::trainControl(\n               method = \"repeatedcv\",\n               number = 10,\n               repeats = 10))\n# Graphique de la racine de l'EQM en fonction de\n# nvmax\nplot(seq_vc)\n\n\n\n\n\nCode\n# Meilleur modèle\nmod_seq_vc &lt;- which.min(seq_vc$results$RMSE)\nreqm_seq_vc &lt;- min(seq_vc$results$RMSE)\nnvar_seq_vc &lt;- seq_vc$results$nvmax[mod_seq_vc]\n## Calcul des prédictions\n# predict(seq_vc, newdata = db)\n\n\nLes notes de cours donnent une approche pas à pas avec glmnet pour ajuster le LASSO en choisissant la valeur de \\(\\lambda\\) par validation croisée, mais on peut changer le type de model dans caret et utiliser les mêmes fonctionnalités.\n\n\nCode\n# Autre approche que glmnet\nlasso_vc &lt;- \n  caret::train(form = formule,\n               data = db,\n               method = 'glmnet',\n             tuneGrid = expand.grid(\n               alpha = 1,\n               lambda = lambda_seq),\n             trControl = caret::trainControl(\n               method = \"repeatedcv\",\n               number = 10,\n               repeats = 10))\nplot(lasso_vc)\n\n\n\n\n\nCode\n# Meilleur modèle\ncoefs_lasso_cv &lt;- coef(object = lasso_vc$finalModel, \n              s = lasso_vc$bestTune$lambda)\n# Nombre de paramètres non-nuls pour la solution\nnvar_lasso_vc &lt;- Matrix::nnzero(coefs_lasso_cv)\n# Estimation de la racine de l'erreur quadratique moyenne\nreqm_lasso_vc &lt;- min(lasso_vc$results$RMSE)\n## Calcul des prédictions - comme avec n'importe quel modèle\n# predict(lasso_vc, newdata = db)\n\n\n\nCréez un tableau avec le nombre de coefficients non-nuls de votre modèle final et un estimé de l’erreur moyenne quadratique obtenu par validation externe ou croisée.\n\nIl suffit de colliger notre estimation de l’erreur et le nombre de coefficients.\n\n\nCode\nresultats &lt;-data.frame(\n  erreur = rep(c(\"validation croisée\",\n                  \"validation externe\"),\n                  each = 2),\n  methode = rep(c(\"sequentielle\",\"lasso\"), \n               length.out = 4),\n  reqm = c(reqm_seq_vc,\n           reqm_lasso_vc,\n           reqm_seq_ve,\n           reqm_lasso_ve),\n  npar = c(nvar_seq_vc,\n           nvar_lasso_vc,\n           nvar_seq_ve,\n           nvar_lasso_ve))\nknitr::kable(resultats)\n\n\n\n\n\nerreur\nmethode\nreqm\nnpar\n\n\n\n\nvalidation croisée\nsequentielle\n2559.108\n18\n\n\nvalidation croisée\nlasso\n2490.672\n56\n\n\nvalidation externe\nsequentielle\n2324.801\n12\n\n\nvalidation externe\nlasso\n2276.097\n105\n\n\n\n\n\n\nCommentez sur le meilleur modèle parmi les combinaisons.\n\nSi la taille de la base de données est plus conséquente, la validation croisée reste préférable à la validation externe, surtout si on répète cette méthode plusieurs fois pour réduire l’incertitude.1 Le nombre de paramètres dans le modèle LASSO peut sembler élevé, mais il faut garder en tête que les autres paramètres sont rétrécis vers zéro même s’ils sont non-nuls.\nImpossible de savoir quel est le meilleur modèle: le modèle avec la plus petite erreur quadratique moyenne est notre meilleur choix ici à défaut d’autres observations. Rappelez-vous qu’un modèle peut donner une bonne performance, mais être choisi parce qu’il surajuste une valeur aberrante.\nLe but de l’exercice était davantage de démontrer le travail que de faire une prédiction convaincante. Puisqu’on essaie de prédire une variable de dénombrement, un modèle plus adapté (modèle de régression binomiale négative, ou un modèle pour le log du nombre d’admission) pourrait probablement être plus adéquat dans la mesure où la variance de la réponse dépend de la taille de l’école."
  },
  {
    "objectID": "exercices/02-solution.html#exercice-2.1",
    "href": "exercices/02-solution.html#exercice-2.1",
    "title": "Sélection de variables",
    "section": "",
    "text": "Faites une analyse exploratoire des variables explicatives:\n\nQuelles variables devraient êtres exclues de la modélisation? Justifiez votre réponse.\nComparez la variable réponse avec les autres variables: y a-t-il des transformations qui améliorerait l’ensemble de variables candidates: interactions, création de variables dychotomiques, transformations (racines carrée, transformation logarithmique, etc.)?\nVérifiez s’il y a des variables catégorielles encodées comme des variables numériques.\n\n\nOn remarque en consultant la documentation de la base de données à l’aide de ?college que le nombre de demandes d’admissions (napplications), le nombre d’admissions parmi ces applications nadmission et le nombre d’offres converties par les candidat.e.s, ninscrits, sont reliées et ne peuvent être employées.\nPlusieurs variables seront fortement corrélées parce qu’elles dépendent de la capacité d’accueil de l’établissement d’enseignement. Ainsi, on pourrait créer une variable qui représente le pourcentage de temps partiels pour les premiers cycles (plutôt que le décompte). Les variables pourcentdoctorat et pourcentterminal, sont fortement corrélées puisque la plupart des diplômes terminaux (incluant les titres professionnels, les doctorats de premier cycle en médecine, etc.) sont des PhD: le modèle choisira la variable la plus adéquate. Il n’y a pas de variables catégorielles hors variables binaires. Côté vérifications d’usage, on note que tauxdiplom et pourcentdoctorat sont supérieurs à 100%.\nOn devra retirer la variable catégorielle nom, qui a une modalité différente pour chaque observation.\n\n\nCode\nlibrary(dplyr)\nlibrary(hecmulti)\ndata(college, package = \"hecmulti\")\nstr(college)\n\n\ntibble [777 × 19] (S3: tbl_df/tbl/data.frame)\n $ prive                  : int [1:777] 1 1 1 1 1 1 1 1 1 1 ...\n $ napplications          : num [1:777] 1660 2186 1428 417 193 ...\n $ nadmission             : num [1:777] 1232 1924 1097 349 146 ...\n $ ninscrits              : num [1:777] 721 512 336 137 55 158 103 489 227 172 ...\n $ m10p                   : num [1:777] 23 16 22 60 16 38 17 37 30 21 ...\n $ m25p                   : num [1:777] 52 29 50 89 44 62 45 68 63 44 ...\n $ tempsplein1c           : num [1:777] 2885 2683 1036 510 249 ...\n $ tempspart1c            : num [1:777] 537 1227 99 63 869 ...\n $ fraiscolexternes       : num [1:777] 7440 12280 11250 12960 7560 ...\n $ fraisres               : num [1:777] 3300 6450 3750 5450 4120 ...\n $ fraislivres            : num [1:777] 450 750 400 450 800 500 500 450 300 660 ...\n $ fraisperso             : num [1:777] 2200 1500 1165 875 1500 ...\n $ pourcentdoctorat       : num [1:777] 70 29 53 92 76 67 90 89 79 40 ...\n $ pourcentterminal       : num [1:777] 78 30 66 97 72 73 93 100 84 41 ...\n $ ratioetudprof          : num [1:777] 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...\n $ pourcentdonationdiplome: num [1:777] 12 16 30 37 2 11 26 37 23 15 ...\n $ depenseparetud         : num [1:777] 7041 10527 8735 19016 10922 ...\n $ tauxdiplom             : num [1:777] 60 56 54 59 15 55 63 73 80 52 ...\n $ nom                    : Factor w/ 777 levels \"Abilene Christian University\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\n\nCode\nsummary(college)\n\n\n     prive        napplications     nadmission      ninscrits   \n Min.   :0.0000   Min.   :   81   Min.   :   72   Min.   :  35  \n 1st Qu.:0.0000   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242  \n Median :1.0000   Median : 1558   Median : 1110   Median : 434  \n Mean   :0.7272   Mean   : 3002   Mean   : 2019   Mean   : 780  \n 3rd Qu.:1.0000   3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902  \n Max.   :1.0000   Max.   :48094   Max.   :26330   Max.   :6392  \n                                                                \n      m10p            m25p        tempsplein1c    tempspart1c     \n Min.   : 1.00   Min.   :  9.0   Min.   :  139   Min.   :    1.0  \n 1st Qu.:15.00   1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0  \n Median :23.00   Median : 54.0   Median : 1707   Median :  353.0  \n Mean   :27.56   Mean   : 55.8   Mean   : 3700   Mean   :  855.3  \n 3rd Qu.:35.00   3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0  \n Max.   :96.00   Max.   :100.0   Max.   :31643   Max.   :21836.0  \n                                                                  \n fraiscolexternes    fraisres     fraislivres       fraisperso  \n Min.   : 2340    Min.   :1780   Min.   :  96.0   Min.   : 250  \n 1st Qu.: 7320    1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850  \n Median : 9990    Median :4200   Median : 500.0   Median :1200  \n Mean   :10441    Mean   :4358   Mean   : 549.4   Mean   :1341  \n 3rd Qu.:12925    3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700  \n Max.   :21700    Max.   :8124   Max.   :2340.0   Max.   :6800  \n                                                                \n pourcentdoctorat pourcentterminal ratioetudprof   pourcentdonationdiplome\n Min.   :  8.00   Min.   : 24.0    Min.   : 2.50   Min.   : 0.00          \n 1st Qu.: 62.00   1st Qu.: 71.0    1st Qu.:11.50   1st Qu.:13.00          \n Median : 75.00   Median : 82.0    Median :13.60   Median :21.00          \n Mean   : 72.66   Mean   : 79.7    Mean   :14.09   Mean   :22.74          \n 3rd Qu.: 85.00   3rd Qu.: 92.0    3rd Qu.:16.50   3rd Qu.:31.00          \n Max.   :103.00   Max.   :100.0    Max.   :39.80   Max.   :64.00          \n                                                                          \n depenseparetud    tauxdiplom                               nom     \n Min.   : 3186   Min.   : 10.00   Abilene Christian University:  1  \n 1st Qu.: 6751   1st Qu.: 53.00   Adelphi University          :  1  \n Median : 8377   Median : 65.00   Adrian College              :  1  \n Mean   : 9660   Mean   : 65.46   Agnes Scott College         :  1  \n 3rd Qu.:10830   3rd Qu.: 78.00   Alaska Pacific University   :  1  \n Max.   :56233   Max.   :118.00   Albertson College           :  1  \n                                  (Other)                     :771  \n\n\nCode\n# Est-ce que pourcentdoctorat &lt; pourcenterminal?\nsummary(with(college,\n             pourcentdoctorat/pourcentterminal))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1379  0.8824  0.9390  0.9112  0.9775  1.9167 \n\n\nCode\n# Non\ndb &lt;- college |&gt;\n  mutate(\n    tauxdiplom = pmin(tauxdiplom, 100),\n    pourcentdoctorat = pmin(pourcentdoctorat, 100),\n    pctpart1c = tempspart1c/(tempsplein1c+tempspart1c)) |&gt;\n  select(! c(nom, \n             tempsplein1c, \n             tempspart1c, \n             nadmission, \n             ninscrits))\n\n\n\nScindez la base de données en échantillon avec données d’entraînement (environ 2/3 des données) et échantillon de validation; utilisez le germe aléatoire 60602 via set.seed(60602).\n\nSélectionnez un modèle à l’aide d’une des méthodes couvertes, mais en basant votre choix sur l’erreur moyenne quadratique évaluée sur l’échantillon de validation.\n\n\nOn peut désormais considérer une séparation en tiers. Pour ce faire, je vais échantillonner des variables logiques vrais et faux avec une cote de 2 pour 1 et ensuite sélectionner les lignes qui correspondent.\n\n\nCode\nset.seed(60602)\ntest &lt;- sample(x = c(FALSE, TRUE),\n               size = nrow(db),\n               replace = TRUE, \n               prob = c(1/3, 2/3))\ndb_a &lt;- db[test,]\ndb_v &lt;- db[!test,]\n\n\nPour l’estimation, on pourrait simplement calculer l’erreur quadratique moyenne de validation pour tous les cinquante premiers modèles en considérant toutes les interactions d’ordre 2.\n\n\nCode\n# Modèle avec interactions de toutes les variables\nformule &lt;- formula(napplications ~ .^2)\n\n# Sélection séquentielle ascendante\nrec_seq &lt;- leaps::regsubsets(\n  x = formule, \n  data = db_a,\n  method = \"seqrep\",\n  nvmax = 50)\n\n\nSi regsubsets permet de recouvrer les différents modèles, il n’y a pas d’utilitaire pour obtenir le résumé et les prédictions. Le paquet hecmulti inclut une fonction pour faire les prédictions d’un modèle donné. À des fins d’illustration, on considère celui qui a le plus petit critère BIC intra-échantillon d’apprentissage.\n\n\nCode\n# Calculer les BIC \nbic_mod &lt;- summary(rec_seq)$bic\n# Prédire du modèle avec id variables\n# Ici, celui avec le plus petit BIC du lot\npredict(rec_seq, id = which.min(bic_mod))\n\n\nIci, on s’intéresse uniquement au calcul de l’erreur quadratique moyenne pour les modèles choisis par la procédure de recherche séquentielle. La fonction eval_EQM_regsubsets fait le calcul et nous retourne un vecteur avec toutes les mesures. On choisit le modèle qui minimise l’erreur.\n\n\nCode\n# Modèle avec la plus petite erreur \n# moyenne quadratique de validation externe\nreqm_seq_ve_list &lt;- \n  hecmulti::eval_EQM_regsubsets(\n  model = rec_seq, \n  select = 1:50,\n  formula = formule,\n  data = db_a,\n  newdata = db_v)\n# La fonction calcule la racine EQM sur \n# les données de validation\nmod_seq_ve &lt;- which.min(reqm_seq_ve_list)\nnvar_seq_ve &lt;- mod_seq_ve + 1 \n# nombre variables + ordonnée à l'origine.\nreqm_seq_ve &lt;- reqm_seq_ve_list[mod_seq_ve]\n\n\nUne logique similaire s’appliquerait avec le LASSO, même si c’est loin d’être la norme (on utilise d’ordinaire la validation croisée). Ici, on ajusterait le modèle avec plusieurs valeurs de \\(\\lambda\\), puis on calculerait l’erreur quadratique moyenne de validation. On prendra la valeur de la pénalisation \\(\\lambda\\) qui minimise cette dernière. Spécifier la séquence de valeurs de \\(\\lambda\\) à essayer nécessite un peu d’essai/erreur.\n\n\nCode\n# Obtenir X et y\nXmat_a &lt;- model.matrix(formule, data = db_a)[,-1]\ny_a &lt;- db_a$napplications\nlambda_seq &lt;- exp(seq(from = log(0.1),\n                  to = log(50), \n                  length.out = 100L))\nlasso &lt;- glmnet::glmnet(\n    x = Xmat_a,\n    y = y_a,\n    alpha = 1, \n    lambda = lambda_seq)\n# Prédictions et calcul de l'EQM\n# On pourrait remplacer `newx` par \n# d'autres données (validation externe)\neqm_lasso_externe &lt;- rep(0, length(lambda_seq))\nfor(i in seq_along(lambda_seq)){\n  pred &lt;- predict(\n    object = lasso, \n    s = lambda_seq[i], \n    newx = model.matrix(formule, data = db_v)[,-1])\n  eqm_lasso_externe[i] &lt;- \n    mean((pred - db_v$napplications)^2)\n}\nminl &lt;- which.min(eqm_lasso_externe)\nlambda_opt &lt;- lambda_seq[minl]\n# Nombre de coefficients non nuls\nnvar_lasso_ve &lt;- Matrix::nnzero(lasso$beta[,minl, drop = FALSE])\nreqm_lasso_ve &lt;- sqrt(eqm_lasso_externe[minl])\n\n\n\nRépétez la sélection, cette fois en prenant comme critère pour l’erreur moyenne quadratique évaluée par validation croisée (aléatoire) à cinq plis.\n\nOn peut utiliser boot::cv.glm ou caret::train avec les différentes méthodes pour ajuster les modèles. Je fais varier le nombre maximal de variables pour retourner les différentes solutions. Comme le résultat de la validation croisée est aléatoire, on peut répéter cette étape pour réduire l’incertitude de prédiction et obtenir une meilleur valeur de l’écart-type.\n\n\nCode\n# Matrice du modèle complet, moins ordonnée à l'origine\nXmat &lt;- model.matrix(formule, data = db)[,-1]\n# Variable réponse\ny &lt;- db$napplications\nseq_vc &lt;- \n  caret::train(form = formule,\n               data = db,\n               method = 'leapSeq',\n             tuneGrid = expand.grid(nvmax = 1:50),\n             trControl = caret::trainControl(\n               method = \"repeatedcv\",\n               number = 10,\n               repeats = 10))\n# Graphique de la racine de l'EQM en fonction de\n# nvmax\nplot(seq_vc)\n\n\n\n\n\nCode\n# Meilleur modèle\nmod_seq_vc &lt;- which.min(seq_vc$results$RMSE)\nreqm_seq_vc &lt;- min(seq_vc$results$RMSE)\nnvar_seq_vc &lt;- seq_vc$results$nvmax[mod_seq_vc]\n## Calcul des prédictions\n# predict(seq_vc, newdata = db)\n\n\nLes notes de cours donnent une approche pas à pas avec glmnet pour ajuster le LASSO en choisissant la valeur de \\(\\lambda\\) par validation croisée, mais on peut changer le type de model dans caret et utiliser les mêmes fonctionnalités.\n\n\nCode\n# Autre approche que glmnet\nlasso_vc &lt;- \n  caret::train(form = formule,\n               data = db,\n               method = 'glmnet',\n             tuneGrid = expand.grid(\n               alpha = 1,\n               lambda = lambda_seq),\n             trControl = caret::trainControl(\n               method = \"repeatedcv\",\n               number = 10,\n               repeats = 10))\nplot(lasso_vc)\n\n\n\n\n\nCode\n# Meilleur modèle\ncoefs_lasso_cv &lt;- coef(object = lasso_vc$finalModel, \n              s = lasso_vc$bestTune$lambda)\n# Nombre de paramètres non-nuls pour la solution\nnvar_lasso_vc &lt;- Matrix::nnzero(coefs_lasso_cv)\n# Estimation de la racine de l'erreur quadratique moyenne\nreqm_lasso_vc &lt;- min(lasso_vc$results$RMSE)\n## Calcul des prédictions - comme avec n'importe quel modèle\n# predict(lasso_vc, newdata = db)\n\n\n\nCréez un tableau avec le nombre de coefficients non-nuls de votre modèle final et un estimé de l’erreur moyenne quadratique obtenu par validation externe ou croisée.\n\nIl suffit de colliger notre estimation de l’erreur et le nombre de coefficients.\n\n\nCode\nresultats &lt;-data.frame(\n  erreur = rep(c(\"validation croisée\",\n                  \"validation externe\"),\n                  each = 2),\n  methode = rep(c(\"sequentielle\",\"lasso\"), \n               length.out = 4),\n  reqm = c(reqm_seq_vc,\n           reqm_lasso_vc,\n           reqm_seq_ve,\n           reqm_lasso_ve),\n  npar = c(nvar_seq_vc,\n           nvar_lasso_vc,\n           nvar_seq_ve,\n           nvar_lasso_ve))\nknitr::kable(resultats)\n\n\n\n\n\nerreur\nmethode\nreqm\nnpar\n\n\n\n\nvalidation croisée\nsequentielle\n2559.108\n18\n\n\nvalidation croisée\nlasso\n2490.672\n56\n\n\nvalidation externe\nsequentielle\n2324.801\n12\n\n\nvalidation externe\nlasso\n2276.097\n105\n\n\n\n\n\n\nCommentez sur le meilleur modèle parmi les combinaisons.\n\nSi la taille de la base de données est plus conséquente, la validation croisée reste préférable à la validation externe, surtout si on répète cette méthode plusieurs fois pour réduire l’incertitude.1 Le nombre de paramètres dans le modèle LASSO peut sembler élevé, mais il faut garder en tête que les autres paramètres sont rétrécis vers zéro même s’ils sont non-nuls.\nImpossible de savoir quel est le meilleur modèle: le modèle avec la plus petite erreur quadratique moyenne est notre meilleur choix ici à défaut d’autres observations. Rappelez-vous qu’un modèle peut donner une bonne performance, mais être choisi parce qu’il surajuste une valeur aberrante.\nLe but de l’exercice était davantage de démontrer le travail que de faire une prédiction convaincante. Puisqu’on essaie de prédire une variable de dénombrement, un modèle plus adapté (modèle de régression binomiale négative, ou un modèle pour le log du nombre d’admission) pourrait probablement être plus adéquat dans la mesure où la variance de la réponse dépend de la taille de l’école."
  },
  {
    "objectID": "exercices/02-solution.html#footnotes",
    "href": "exercices/02-solution.html#footnotes",
    "title": "Sélection de variables",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nRien ne nous empêche de faire de même avec la validation externe, soit dit en passant.↩︎"
  },
  {
    "objectID": "exercices/03-solution.html",
    "href": "exercices/03-solution.html",
    "title": "Régression logistique",
    "section": "",
    "text": "Les données logistclient contiennent des données simulées pour un cas fictif de promotion pour des clients.\n\nEstimez le modèle logistique pour la probabilité que promo=1 avec les variables explicatives nachats, sexe et tclient.\n\n\n\nCode\nlibrary(hecmulti)\ndata(logistclient, package = \"hecmulti\")\n# Modèle logistique\nmod &lt;- glm(promo ~ nachats + sexe + tclient, \n           family = binomial(link = \"logit\"),\n           data = logistclient)\n# Coefficients du modèle et statistique de Wald\nsummary(mod)\n\n\n\nCall:\nglm(formula = promo ~ nachats + sexe + tclient, family = binomial(link = \"logit\"), \n    data = logistclient)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -0.98615    0.24634  -4.003 6.25e-05 ***\nnachats             0.20775    0.03509   5.921 3.20e-09 ***\nsexe               -0.26796    0.13872  -1.932   0.0534 .  \ntclientoccasionnel  0.06873    0.15638   0.439   0.6603    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1384.5  on 999  degrees of freedom\nResidual deviance: 1330.6  on 996  degrees of freedom\nAIC: 1338.6\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nInterprétez les coefficients du modèle à l’échelle de la cote en terme de pourcentage d’augmentation ou de diminution.\n\n\n\nCode\nexp(coef(mod)) # Rapport de cote\n\n\n       (Intercept)            nachats               sexe tclientoccasionnel \n         0.3730111          1.2309041          0.7649396          1.0711432 \n\n\n\nLa cote pour l’offre promotionnelle (oui versus non) des hommes est 23.5% plus faible que celle des femmes, ceteris paribus\nLa cote des clients occasionnels est 7.11% supérieure à celle des clients fréquents, ceteris paribus. De manière équivalente, le rapport de cotes pour tclient fréquent sur occasionnel est de1/1.0711 = 0.934: les clients fréquents ont une cote 6.6% inférieure à celle des clients occasionnels, toute chose étant égale par ailleurs.\nLa cote de nachats augmente de 23.1% pour chaque augmentation du nombre d’achats dans le dernier mois, ceteris paribus\n\n\nTestez si l’effet de nachats est statistiquement significatif à niveau \\(\\alpha = 0.05\\).\n\n\nL’intervalle de confiance à 95% pour le rapport de cote de nachats, basé sur la vraisemblance profilée, est de \\([1.15, 1.32]\\); comme 1 est exclu, cette différence est statistiquement significative.\n\nOn obtiendrait la même conclusion avec la statistique du test de rapport de vraisemblance, ici \\(37.237\\) pour 1 degré de liberté. La probabilité, si \\(\\beta_{\\text{nachats}}=0\\), d’obtenir une telle différence d’ajustement est inférieure à \\(10^{-4}\\), bien en deça du seuil de significativité. On rejette l’hypothèse nulle et on conclut que le nombre d’achat est important pour expliquer si une personne s’est prévalue de l’offre promotionnelle.\n\n\nCode\nexp(confint(mod)) #IC pour rapport de cote\n\n\n                       2.5 %    97.5 %\n(Intercept)        0.2292101 0.6025441\nnachats            1.1500139 1.3197437\nsexe               0.5825467 1.0037294\ntclientoccasionnel 0.7889239 1.4568553\n\n\nCode\ncar::Anova(mod, type = 3) # tests de rapport de vraisemblance\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: promo\n        LR Chisq Df Pr(&gt;Chisq)    \nnachats   37.237  1  1.046e-09 ***\nsexe       3.737  1    0.05322 .  \ntclient    0.193  1    0.66011    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChoisissez un point de coupure pour la classification pour maximiser le taux de bonne classification.\n\nPour le point de coupure choisi, construisez une matrice de confusion.\nCalculez la sensibilité, la spécificité et le taux de bonne classification manuellement. Vérifiez vos réponses avec la sortie du tableau.\n\n\n\n\nCode\nset.seed(60602)\n# Prédictions par validation croisée \n# (moyenne de 10 réplications, K=10 plis)\npred &lt;- hecmulti::predvc(mod)\n# Extraire la variable réponse binaire 0/1\nresp &lt;- logistclient$promo\n\n\nNotez que les prédictions obtenues par validation croisée sont aléatoires, donc les résultats (aire sous la courbe, valeur-\\(p\\) du test d’adéquation, etc.) peuvent varier si vous n’utilisez pas le même germe aléatoire.\nOn prend le modèle ajusté avec glm et on calcule la prédiction à l’aide de la validation croisée à 10 groupes, répétée 10 fois. La fonction predvc retourne la moyenne des prédictions (ici, des probabilités) pour chacune des 1000 observations.\n\n\nCode\nlibrary(ggplot2)\ntableau &lt;- perfo_logistique(prob = pred, \n                             resp = resp)\n# Graphique du taux de bonne classification\n# selon le point de coupure\nggplot(data = tableau, \n       aes(x = coupe, y = pcorrect)) + \n  geom_line() + \n  theme_classic() + \n  scale_y_continuous(limits = c(0, 100),\n                     expand = c(0,0)) + \n  labs(x = \"point de coupure\",\n       y = \"\",\n       subtitle = \"Taux de bonne classification\")\n\n\n\n\n\nCode\nopt &lt;- which.max(tableau$pcorrect)\nknitr::kable(tableau[opt,], digits = 2)\n\n\n\n\n\n\ncoupe\nVP\nVN\nFP\nFN\npcorrect\nsensi\nspeci\nfpos\nfneg\n\n\n\n\n45\n0.45\n316\n281\n240\n163\n59.7\n65.97\n53.93\n43.17\n36.71\n\n\n\n\n\nEnsuite, il suffit de passer les valeurs de la variable réponse et nos probabilités de succès prédites aux différentes fonctions.\nSi on considère des points de coupure de 0.01 à 0.99 en incréments de 0.01, on obtient un point de coupure optimal à 0.45. On note que le taux de bonne classification change assez peu au final.\n\n\n\n\nTableau 1: Matrice de confusion avec point de coupure optimal\n\n\n\n\\(Y=1\\)\n\\(Y=0\\)\n\n\n\n\n\\(\\widehat{Y}=1\\)\n316\n240\n\n\n\\(\\widehat{Y}=0\\)\n163\n281\n\n\n\n\n\n\nAinsi, si on fait les calculs à la main, on estime\n\nla sensibilité, \\(\\mathsf{VP}/(\\mathsf{VP} + \\mathsf{FN})\\), soit 316 / (316 + 163) ou 0.66.\nla spécificité, \\(\\mathsf{VN}/(\\mathsf{VN} + \\mathsf{FP})\\), soit 281 / (281 + 240) ou 0.539.\nle taux de bonne classification \\(\\mathsf{VN} + \\mathsf{VP}/(\\mathsf{VN} + \\mathsf{VP} + \\mathsf{FN} + \\mathsf{FP})\\), soit (316 + 281) / 1000 ou 0.597.\n\nCes valeurs coincident, à arrondi près, avec ce qui est reporté dans le tableau.\n\nProduisez un graphique de la fonction d’efficacité du récepteur (courbe ROC) et rapportez l’aire sous la courbe estimée à l’aide de la validation croisée.\n\n\n\nCode\nroc &lt;- courbe_roc(prob = pred, resp = resp)\n\n\n\n\n\nOn obtient une estimation de l’aire sous la courbe de 0.616.\n\nCalculez la statistique de Spiegelhalter (1986) pour la calibration du modèle. Y a-t-il des preuves de surajustement?\n\n\n\nCode\nhecmulti::calibration(prob = pred, resp = resp)\n\n\nTest de calibration de Spiegelhalter (1986)\nStatistique de test: 0.55 \nvaleur-p: 0.581\n\n\nL’hypothèse nulle est que le modèle est calibré; ici, la valeur-\\(p\\) est près de 0.5, donc on ne rejette pas l’hypothèse nulle et on conclut qu’il n’y a pas de preuve de surajustement."
  },
  {
    "objectID": "exercices/03-solution.html#exercice-3.1",
    "href": "exercices/03-solution.html#exercice-3.1",
    "title": "Régression logistique",
    "section": "",
    "text": "Les données logistclient contiennent des données simulées pour un cas fictif de promotion pour des clients.\n\nEstimez le modèle logistique pour la probabilité que promo=1 avec les variables explicatives nachats, sexe et tclient.\n\n\n\nCode\nlibrary(hecmulti)\ndata(logistclient, package = \"hecmulti\")\n# Modèle logistique\nmod &lt;- glm(promo ~ nachats + sexe + tclient, \n           family = binomial(link = \"logit\"),\n           data = logistclient)\n# Coefficients du modèle et statistique de Wald\nsummary(mod)\n\n\n\nCall:\nglm(formula = promo ~ nachats + sexe + tclient, family = binomial(link = \"logit\"), \n    data = logistclient)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -0.98615    0.24634  -4.003 6.25e-05 ***\nnachats             0.20775    0.03509   5.921 3.20e-09 ***\nsexe               -0.26796    0.13872  -1.932   0.0534 .  \ntclientoccasionnel  0.06873    0.15638   0.439   0.6603    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1384.5  on 999  degrees of freedom\nResidual deviance: 1330.6  on 996  degrees of freedom\nAIC: 1338.6\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nInterprétez les coefficients du modèle à l’échelle de la cote en terme de pourcentage d’augmentation ou de diminution.\n\n\n\nCode\nexp(coef(mod)) # Rapport de cote\n\n\n       (Intercept)            nachats               sexe tclientoccasionnel \n         0.3730111          1.2309041          0.7649396          1.0711432 \n\n\n\nLa cote pour l’offre promotionnelle (oui versus non) des hommes est 23.5% plus faible que celle des femmes, ceteris paribus\nLa cote des clients occasionnels est 7.11% supérieure à celle des clients fréquents, ceteris paribus. De manière équivalente, le rapport de cotes pour tclient fréquent sur occasionnel est de1/1.0711 = 0.934: les clients fréquents ont une cote 6.6% inférieure à celle des clients occasionnels, toute chose étant égale par ailleurs.\nLa cote de nachats augmente de 23.1% pour chaque augmentation du nombre d’achats dans le dernier mois, ceteris paribus\n\n\nTestez si l’effet de nachats est statistiquement significatif à niveau \\(\\alpha = 0.05\\).\n\n\nL’intervalle de confiance à 95% pour le rapport de cote de nachats, basé sur la vraisemblance profilée, est de \\([1.15, 1.32]\\); comme 1 est exclu, cette différence est statistiquement significative.\n\nOn obtiendrait la même conclusion avec la statistique du test de rapport de vraisemblance, ici \\(37.237\\) pour 1 degré de liberté. La probabilité, si \\(\\beta_{\\text{nachats}}=0\\), d’obtenir une telle différence d’ajustement est inférieure à \\(10^{-4}\\), bien en deça du seuil de significativité. On rejette l’hypothèse nulle et on conclut que le nombre d’achat est important pour expliquer si une personne s’est prévalue de l’offre promotionnelle.\n\n\nCode\nexp(confint(mod)) #IC pour rapport de cote\n\n\n                       2.5 %    97.5 %\n(Intercept)        0.2292101 0.6025441\nnachats            1.1500139 1.3197437\nsexe               0.5825467 1.0037294\ntclientoccasionnel 0.7889239 1.4568553\n\n\nCode\ncar::Anova(mod, type = 3) # tests de rapport de vraisemblance\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: promo\n        LR Chisq Df Pr(&gt;Chisq)    \nnachats   37.237  1  1.046e-09 ***\nsexe       3.737  1    0.05322 .  \ntclient    0.193  1    0.66011    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nChoisissez un point de coupure pour la classification pour maximiser le taux de bonne classification.\n\nPour le point de coupure choisi, construisez une matrice de confusion.\nCalculez la sensibilité, la spécificité et le taux de bonne classification manuellement. Vérifiez vos réponses avec la sortie du tableau.\n\n\n\n\nCode\nset.seed(60602)\n# Prédictions par validation croisée \n# (moyenne de 10 réplications, K=10 plis)\npred &lt;- hecmulti::predvc(mod)\n# Extraire la variable réponse binaire 0/1\nresp &lt;- logistclient$promo\n\n\nNotez que les prédictions obtenues par validation croisée sont aléatoires, donc les résultats (aire sous la courbe, valeur-\\(p\\) du test d’adéquation, etc.) peuvent varier si vous n’utilisez pas le même germe aléatoire.\nOn prend le modèle ajusté avec glm et on calcule la prédiction à l’aide de la validation croisée à 10 groupes, répétée 10 fois. La fonction predvc retourne la moyenne des prédictions (ici, des probabilités) pour chacune des 1000 observations.\n\n\nCode\nlibrary(ggplot2)\ntableau &lt;- perfo_logistique(prob = pred, \n                             resp = resp)\n# Graphique du taux de bonne classification\n# selon le point de coupure\nggplot(data = tableau, \n       aes(x = coupe, y = pcorrect)) + \n  geom_line() + \n  theme_classic() + \n  scale_y_continuous(limits = c(0, 100),\n                     expand = c(0,0)) + \n  labs(x = \"point de coupure\",\n       y = \"\",\n       subtitle = \"Taux de bonne classification\")\n\n\n\n\n\nCode\nopt &lt;- which.max(tableau$pcorrect)\nknitr::kable(tableau[opt,], digits = 2)\n\n\n\n\n\n\ncoupe\nVP\nVN\nFP\nFN\npcorrect\nsensi\nspeci\nfpos\nfneg\n\n\n\n\n45\n0.45\n316\n281\n240\n163\n59.7\n65.97\n53.93\n43.17\n36.71\n\n\n\n\n\nEnsuite, il suffit de passer les valeurs de la variable réponse et nos probabilités de succès prédites aux différentes fonctions.\nSi on considère des points de coupure de 0.01 à 0.99 en incréments de 0.01, on obtient un point de coupure optimal à 0.45. On note que le taux de bonne classification change assez peu au final.\n\n\n\n\nTableau 1: Matrice de confusion avec point de coupure optimal\n\n\n\n\\(Y=1\\)\n\\(Y=0\\)\n\n\n\n\n\\(\\widehat{Y}=1\\)\n316\n240\n\n\n\\(\\widehat{Y}=0\\)\n163\n281\n\n\n\n\n\n\nAinsi, si on fait les calculs à la main, on estime\n\nla sensibilité, \\(\\mathsf{VP}/(\\mathsf{VP} + \\mathsf{FN})\\), soit 316 / (316 + 163) ou 0.66.\nla spécificité, \\(\\mathsf{VN}/(\\mathsf{VN} + \\mathsf{FP})\\), soit 281 / (281 + 240) ou 0.539.\nle taux de bonne classification \\(\\mathsf{VN} + \\mathsf{VP}/(\\mathsf{VN} + \\mathsf{VP} + \\mathsf{FN} + \\mathsf{FP})\\), soit (316 + 281) / 1000 ou 0.597.\n\nCes valeurs coincident, à arrondi près, avec ce qui est reporté dans le tableau.\n\nProduisez un graphique de la fonction d’efficacité du récepteur (courbe ROC) et rapportez l’aire sous la courbe estimée à l’aide de la validation croisée.\n\n\n\nCode\nroc &lt;- courbe_roc(prob = pred, resp = resp)\n\n\n\n\n\nOn obtient une estimation de l’aire sous la courbe de 0.616.\n\nCalculez la statistique de Spiegelhalter (1986) pour la calibration du modèle. Y a-t-il des preuves de surajustement?\n\n\n\nCode\nhecmulti::calibration(prob = pred, resp = resp)\n\n\nTest de calibration de Spiegelhalter (1986)\nStatistique de test: 0.55 \nvaleur-p: 0.581\n\n\nL’hypothèse nulle est que le modèle est calibré; ici, la valeur-\\(p\\) est près de 0.5, donc on ne rejette pas l’hypothèse nulle et on conclut qu’il n’y a pas de preuve de surajustement."
  },
  {
    "objectID": "exercices/03-solution.html#exercice-3.2",
    "href": "exercices/03-solution.html#exercice-3.2",
    "title": "Régression logistique",
    "section": "Exercice 3.2",
    "text": "Exercice 3.2\n\nInterprétez le coefficient pour l’ordonnée à l’origine \\(\\alpha\\) en terme de pourcentage d’augmentation ou de diminution de la cote par rapport à la référence jouer à l’extérieur.\n\n\n\nCode\ndata(lnh_BT, package = \"hecmulti\")\n# Ajuster le modèle de régression logistique\nmod &lt;- glm(vainqueur ~ ., data = lnh_BT, family = binomial)\n# Extraire les coefficients et les IC\nexpcoef &lt;- exp(coef(mod))\nic &lt;- confint(mod)\n\n\nL’ordonnée à l’origine (premier coefficient) représente l’avantage de jouer à domicile (si \\(\\alpha&gt;0\\)), peu importe les équipes qui jouent. Le coefficient \\(\\widehat{\\alpha} = 0\\) la cote pour l’équipe à domicile est 18.3 % plus élevée que celle de l’équipe en déplacement.\n\nCalculez un intervalle de confiance de niveau 95% pour l’ordonnée à l’origine et déterminez si jouer à domicile impacte significativement le score.\n\nL’intervalle de confiance de vraisemblance profilée est [0.1,0.3] et n’inclut pas zéro: l’effet est donc significatif. Alternativement, l’intervalle pour l’effet multiplicatif de la cote est [1.1,1.3] différent de 1.\n\nFournissez un tableau avec le classement des cinq premières équipes qui ont la plus grande chance de succès selon le modèle.\n\nIl suffit de prendre les cinq équipes qui ont les plus grands coefficients de régression (en vérifiant que ces derniers sont supérieurs à zéro, le coefficient fantôme de la catégorie de référence).\n\n\nCode\n# Calculer les coefficients (moins ordonnée à l'origine)\n# trier et garder les cinq plus grandes valeurs\nclassements &lt;- sort(x = c(\"Anaheim_Ducks\" = 0, coef(mod)[-1]),\n                    decreasing = TRUE)\ntop5 &lt;- head(names(classements), 5)\n# Créer un tableau avec les noms\n# remplacer les barres de soulignement par des espaces\nknitr::kable(x = gsub(\"_\",\" \", top5), \n             col.names = \"équipe\",)\n\n\n\n\nTableau 2: Classement des cinq meilleurs équipes de la LNH.\n\n\néquipe\n\n\n\n\nFlorida Panthers\n\n\nColorado Avalanche\n\n\nCarolina Hurricanes\n\n\nToronto Maple Leafs\n\n\nMinnesota Wild\n\n\n\n\n\n\n\nPour chaque match, utilisez le modèle logistique pour prédire l’équipe gagnante.\n\nConstruisez une matrice de confusion (1 pour une victoire de l’équipe à domicile, 0 sinon) avec un point de coupure de 0.5 (assignation à l’événement ou à la classe la plus probable) et rapportez cette dernière.\nCalculez le taux de bonne classification, la sensibilité et la spécificité à partir de votre matrice de confusion.\n\n\n\n\nCode\nset.seed(60602)\n# Prédire par validation croisée avec point de coupure 0.5\npred &lt;- hecmulti::predvc(modele = mod)\nclassif &lt;- pred &gt; 0.5\n# Créer une matrice de confusion TRUE=1, FALSE=0\ntable(prediction = as.integer(classif), \n      vainqueur = as.integer(lnh_BT$vainqueur))\n\n\n          vainqueur\nprediction   0   1\n         0 340 218\n         1 268 486\n\n\nLe taux de bonne classification est de 63%, la sensibilité de 69% et la spécificité de 55.9%\n\nProduisez un graphique de la fonction d’efficacité du récepteur et rapportez l’aire sous la courbe. Commentez sur la qualité prédictive globale du modèle.\n\n\n\nCode\nroc &lt;- hecmulti::courbe_roc(prob = pred, resp = lnh_BT$vainqueur)\n\n\n\n\n\nL’aire sous la courbe est de 0.66, soit plus qu’une assignation aléatoire de 0.5. Le modèle a un faible pouvoir prédictif, mais ça peut suffire pour des paris sportifs si le modèle bat la concurrence."
  },
  {
    "objectID": "exercices/03-solution.html#exercice-3.3",
    "href": "exercices/03-solution.html#exercice-3.3",
    "title": "Régression logistique",
    "section": "Exercice 3.3",
    "text": "Exercice 3.3\nOn s’intéresse à la satisfaction de clients par rapport à un produit. Cette dernière est mesurée à l’aide d’une échelle de Likert, allant de très insatisfait (1) à très satisfait (5). Les 1000 observations se trouvent dans la base de données multinom du paquet hecmulti.\nModélisez la satisfaction des clients en fonction de l’âge, du niveau d’éducation, du sexe et du niveau de revenu.\n\nEst-ce que le modèle de régression multinomiale ordinale à cote proportionnelles est une simplification adéquate du modèle de régression multinomiale logistique? Si oui, utilisez ce modèle pour la suite. Sinon, ajustez le modèle de régression multinomiale logistique avec 1 comme catégorie de référence, 1 pour revenu et sec pour éducation1 et utilisez ce dernier pour répondre aux autres questions.\n\nLes niveaux des facteurs non-ordonnés (catégories) dans R sont classés en ordre alphanumérique. Il faut donc modifier la catégorie de référence uniquement pour le niveau d’éducation avant d’ajuster le modèle.\n\n\nCode\ndata(multinom, package = \"hecmulti\")\ndb &lt;- multinom |&gt;\n  dplyr::mutate(educ = relevel(educ, ref = \"sec\"),\n                y = ordered(y))\nmod1 &lt;- nnet::multinom(\n  y ~ sexe + educ + revenu + age,\n  data = db,\n  trace = FALSE)\n\n\nLe modèle a 32 coefficients, dont deux pour éducation et revenu et un pour sexe et age, par niveau.\nPour ajuster le modèle à cotes proportionnelles, il faut d’abord convertir la variable réponse en variable ordinale à l’aide de ordered si ce n’est pas déjà la classe de la variable. Cela permettra de spécifier l’ordre des modalités.\n\n\nCode\n# Ajuster modèle à cote proportionnelle\nmod0 &lt;- MASS::polr(\n  ordered(y) ~ sexe + educ + revenu + age,\n  data = db)\n# Calculer statistique de test\n# (rapport de vraisemblance)\nstat &lt;- deviance(mod0) - deviance(mod1)\n# À comparer à une loi khi-deux avec \n# npar1-npar0 degrés de liberté\nnpar0 &lt;- length(coef(mod0)) + length(mod0$zeta)\nnpar1 &lt;- length(coef(mod1))\n# Calcul de la valeur-p\n# Probabilité que khi-deux (df) excède 'stat'\npchisq(stat, \n       df = npar1 - npar0, \n       lower.tail = FALSE)\n\n\n[1] 0.0008817018\n\n\nLe modèle ordinal a 10 paramètres, contre 28 pour le modèle multinomial logistique. On peut faire un test du rapport de vraisemblance en comparant la différence des log-vraisemblance des deux modèles emboîtés: la valeur de la statistique est 42.702. La valeur-\\(p\\) estimée est \\(8.8 \\times 10^{-4}\\)$, donc on rejette l’hypothèse nulle et on conclut que le modèle à cote proportionnelle n’est pas adéquat.\n\nInterprétez l’effet des variables éducation et sexe pour la catégorie 2 (par rapport à 1).\n\nIl suffit de regarder les coefficients \\(\\exp(\\widehat{\\beta}_{\\text{sexe}}), \\ldots\\) associés et les interpréter en termes de rapport de cote, pour une régression logistique ordinaire.\n\n\nCode\n# Coefficient correspondants à sexe et éducation\nexp(coef(mod1)[\"2\", 2:4])\n\n\n     sexe educcegep   educuni \n 1.424053  1.104006  1.316095 \n\n\n\nLa cote pour les femmes pour insatisfait par rapport à très insatisfait est 42.4% plus élevée que pour les hommes, toute chose étant égale par ailleurs.\nLa cote pour les individus qui ont un diplôme collégial pour insatisfait par rapport à très insatisfait est 10.4% plus élevée que pour ceux qui on un diplôme secondaire, toute chose étant égale par ailleurs.\nLa cote pour les individus qui ont un diplôme universitaire pour insatisfait par rapport à très insatisfait est 31.6% plus élevée que pour ceux qui on un diplôme secondaire, toute chose étant égale par ailleurs.\n\n\nEst-ce que le modèle avec une probabilité constante pour chaque item est adéquat lorsque comparé au modèle qui inclut toutes les covariables?\n\nPour répondre à cette question, on ajuste le modèle multinomial logistique avec uniquement une constante. Les probabilités prédites sont simplement la proportion empirique des observations de l’échantillon: on peut ainsi vérifier que le modèle a convergé en comparant les prédictions et ces proportions. Une petite différence numérique est possible puisque le modèle multinomial logistique est ajusté à l’aide d’une procédure d’optimisation numérique itérative.\n\n\nCode\n# Ajuster modèle avec proba constante\nmod0 &lt;- nnet::multinom(\n  y ~ 1, \n  # ordonnée à l'origine seulement\n  data = db,\n  trace = FALSE)\n# Vérifier convergence\npred0 &lt;- predict(\n  object = mod0,\n  # Entrer une bd avec une ligne\n  newdata = db[1,], \n  type = \"prob\")\n# Calculer la proportion de chaque categ.\nproportions_y &lt;- table(db$y)/nrow(db)\n# Calculer différences\npred0 - proportions_y\n\n\n\n            1             2             3             4             5 \n 2.278825e-05  1.157814e-05 -6.983907e-06 -5.040094e-06 -2.234239e-05 \n\n\nOn peut ensuite calculer la statistique de rapport de vraisemblance en comparant les déviances des deux modèles.\n\n\nCode\n# Calculer statistique de test\n# (rapport de vraisemblance)\nstat &lt;- deviance(mod0) - deviance(mod1)\n# À comparer à une loi khi-deux avec \n# npar1-npar0 degrés de liberté\nnpar0 &lt;- length(coef(mod0)) + length(mod0$zeta)\nnpar1 &lt;- length(coef(mod1))\n# Calcul de la valeur-p\n# Probabilité que khi-deux (df) excède 'stat'\npchisq(stat, \n       df = npar1 - npar0, \n       lower.tail = FALSE)\n\n\n[1] 0.0002755707\n\n\nLa statistique pour le test du rapport de vraisemblance que tous les coefficients associés aux covariables sont nuls (24 paramètres supplémentaires) est 55.41, et si le modèle sans covariable était vrai, cette statistique serait approximativement \\(\\chi^2_{24}\\). La valeur-\\(p\\) arrondie est 0.0003, on rejette l’hypothèse nulle que tous les coefficients associés aux variables explicatives sont nuls. On conclut qu’au moins une covariable est utile pour prédire une cote par rapport au modèle avec une probabilité constante.\n\nEst-ce que l’effet de la variable âge est globalement significatif?\n\nPuisqu’on modélise quatre rapport de cotes à l’aide d’un modèle logistique, d’où \\(\\beta_{\\texttt{age}_2}=\\beta_{\\texttt{age}_3}=\\beta_{\\texttt{age}_4}=\\beta_{\\texttt{age}_5}=0.\\) On peut obtenir la valeur-\\(p\\) avec le tableau d’analyse de déviance, qui rapporte la valeur du test de rapport de vraisemblance. On conclut que l’âge impacte la probabilité des différents items de satisfaction.\n\n\nCode\ncar::Anova(mod1, type = 3)\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: y\n       LR Chisq Df Pr(&gt;Chisq)    \nsexe    14.6448  4  0.0054975 ** \neduc     9.7083  8  0.2860960    \nrevenu   7.8239  8  0.4508547    \nage     23.1285  4  0.0001194 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nFournissez un intervalle de confiance à niveau 95% pour l’effet multiplicatif d’une augmentation d’une unité de la variable âge pour chacune des cote par rapport à très insatisfait (1). Que concluez-vous sur l’effet de âge pour les réponses 2 à 5 par rapport à 1?\n\nLes intervalles de confiance sont obtenus en prenant l’exponentielle des intervalles de confiance profilée pour les coefficients associés à age:\n\n\nCode\nexp(confint(mod1)[\"age\",,])\n\n\n               2         3         4         5\n2.5 %  0.9745999 0.8723827 0.9415834 0.9879372\n97.5 % 1.0108252 0.9640311 0.9893561 1.0205983\n\n\n\n\\([0.975; 1.011]\\) pour \\(\\beta_{\\texttt{age}_{2|1}}\\) (pas significatif),\n\\([0.871; 0.963]\\) pour \\(\\beta_{\\texttt{age}_{3|1}}\\) (significatif),\n\\([0.941; 0.989]\\) pour \\(\\beta_{\\texttt{age}_{4|1}}\\) (significatif) et\n\\([0.988; 1.021]\\) pour \\(\\beta_{\\texttt{age}_{5|1}}\\) (pas significatif).\n\n\nÉcrivez l’équation de la cote ajustée pour satisfait (4) par rapport à très insatisfait (1).\n\nPour obtenir l’équation ajustée, on utilise uniquement les coefficients pour \\(Y=4\\) dans le tableau des coefficients.\n\n\nCode\nround(coef(mod1)[\"4\",], 3)\n\n\n(Intercept)        sexe   educcegep     educuni     revenu2     revenu3 \n     -0.502       0.613       0.676       0.800      -0.038       0.009 \n        age \n     -0.035 \n\n\n\\[\\begin{align*}\n\\frac{\\Pr(Y=4 \\mid\\boldsymbol{X})}{\\Pr(Y=1 \\mid \\boldsymbol{X})} & =\\exp(-0.502 - 0.035\\texttt{age} + 0.676 \\texttt{cegep} + 0.8\\texttt{uni} \\\\&-0.038 \\texttt{revenu}_2 +0.009 \\texttt{revenu}_3 + 0.613 \\texttt{sexe})\n\\end{align*}\\]\n\nPrédisez la probabilité qu’un homme de 30 ans qui a un diplôme collégial et qui fait partie de la classe moyenne sélectionne une catégorie donnée. Quelle modalité est la plus susceptible?\n\n\n\nCode\n# Profil du client\nprofil &lt;- data.frame(\n  sexe = 0, \n  age = 30, \n  educ = \"cegep\", \n  revenu = \"2\")\n# Probabilité du score de satisfication\npredict(mod1, \n        newdata = profil, \n        type = \"prob\")\n\n\n         1          2          3          4          5 \n0.32094804 0.22162733 0.03852415 0.12699328 0.29190719 \n\n\nCode\n# Modalité la plus susceptible\npredict(mod1, \n        newdata = profil)\n\n\n[1] 1\nLevels: 1 2 3 4 5\n\n\nLes probabilités prédites sont (0.321; 0.222; 0.039; 0.127; 0.292). La modalité la plus susceptible est donc très insatisfait (1)."
  },
  {
    "objectID": "exercices/03-solution.html#footnotes",
    "href": "exercices/03-solution.html#footnotes",
    "title": "Régression logistique",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nUtilisez la fonction relevel pour changer la catégorie de référence, avec relevel(educ, ref = 'sec').↩︎"
  },
  {
    "objectID": "exercices/04-solution.html",
    "href": "exercices/04-solution.html",
    "title": "Analyse de survie",
    "section": "",
    "text": "Fotopoulos & Louri (2000) considèrent les facteurs de risque pour les nouvelles compagnies manufacturières en Grèce établies entre 1982 et 1984. Utilisez le Tableau 1 de l’article pour pour répondre aux questions suivantes:\n\nQuel type de mécanisme de censure est présent dans ces données?\n\nCensure par intervalle (nombre connu à une année près, seule l’année du début d’activités et le nombre de bilans sont connus des auteurs) et censure à droite (certaines firmes sont encore en activité en 1992 à la fin de la période de collecte de données).\nOn pourrait ignorer la censure par intervalle en associant chaque intervalle de la forme \\([t, t+1)\\) à la valeur \\(t\\).\n\nDonnez une estimation de la probabilité qu’une entreprise survive trois ans et plus.\n\nOn veut la survie \\(\\Pr(T \\geq 3)\\) L’estimation est donnée dans la colonne Cumulative survival rate du Tableau 1, soit 0.6393.\n\nDonnez une estimation de la probabilité qu’une entreprise survive entre 4 et 5 ans, soit l’intervalle [4,5) ans.\n\nPuisque la fonction de survie empirique est constante, cela revient à la probabilité de défaillance dans l’intervalle [4,5) ans, soit \\[\\begin{align*}\n\\Pr\\{T \\in [4,5)\\} &= \\Pr(T \\ge 5) - \\Pr(T \\ge 4) \\\\&= 0.5525 - 0.5023 \\\\&= 0.0502.\n\\end{align*}\\] Si on traçait la courbe de Kaplan–Meier, cette probabilité correspond à la hauteur de la contre-marche entre 4 et 5 ans.\n\nQuel pourcentage des observations sont censurées?\n\nIl y a 219 observations au total et \\(38=13+25\\) firmes sont toujours en activité et donc leur temps de survie est censuré; le pourcentage est 38/219 ou 17.35%.\n\nÊtes-vous en mesure de nous fournir une estimation du troisième quartile de la fonction de survie? Justifiez votre réponse.\n\nNon, puisque les plus grandes temps observés sont censurées et l’estimation de la fonction de survie s’arrête à 0.2921; la fonction de survie ne traverse pas la valeur horizontale de 0.25\nLe Tableau 3 de l’article fournit les estimations d’un modèle à risques proportionnels de Cox.\n\nComparez les modèles (1) et (2). Est-ce que l’effet de cohorte impacte la survie?\n\nOui. On peut voir cela de plusieurs façons puisque (1) les deux coefficients associés sont significativement différents de zéro (tests de Wald) avec, pour COHORT_82, \\(Z = -0.5518/0.2081=-2.651\\) et une valeur-\\(p\\) basée sur l’approximation normale de 0.008 (idem pour COHORT_83).\nOn peut aussi construire une statistique du rapport de vraisemblance avec \\[-2\\ell_{(1)} + 2 \\ell_{(2)} =1503.442-1471.353 = 32.089.\\] Or, l’inclusion des cohortes ajoute deux paramètres, donc cette statistique est à comparer avec une loi \\(\\chi^2_2\\).\n\n\nCode\n# Test de Wald: beta_cohort82 = 0\n2*pnorm(-0.5518/0.2081)\n\n\n[1] 0.008010906\n\n\nCode\n# Test de Wald: beta_cohort83 = 0\n2*pnorm(-0.5464/0.2021)\n\n\n[1] 0.006859031\n\n\nCode\n# Test de rapport de vraisemblance\npchisq(1503.442 - 1471.353, \n       df = 2, \n       lower.tail = FALSE)\n\n\n[1] 1.076371e-07\n\n\n\nPour le modèle (3), décrivez le plus précisément possible l’effet des variables capital (FIXED_ASSET) et le montant de la dette (DEBT) sur la durée de vie des entreprises.\n\nToute chose étant égale par ailleurs, une augmentation du ratio du passif total sur le total des actifs (DEBT) de 1 augmente le risque de 119.3%; le rapport de risque étant multiplié par \\(\\exp(0.83)=2.293\\).\nCeteris paribus, une augmentation du ratio des immobilisations sur le total des actifs de 1 diminue le risque d’environ 59.8%, avec un rapport de risque de \\(\\exp(-0.9091) =0.4028\\)."
  },
  {
    "objectID": "exercices/04-solution.html#exercice-4.1",
    "href": "exercices/04-solution.html#exercice-4.1",
    "title": "Analyse de survie",
    "section": "",
    "text": "Fotopoulos & Louri (2000) considèrent les facteurs de risque pour les nouvelles compagnies manufacturières en Grèce établies entre 1982 et 1984. Utilisez le Tableau 1 de l’article pour pour répondre aux questions suivantes:\n\nQuel type de mécanisme de censure est présent dans ces données?\n\nCensure par intervalle (nombre connu à une année près, seule l’année du début d’activités et le nombre de bilans sont connus des auteurs) et censure à droite (certaines firmes sont encore en activité en 1992 à la fin de la période de collecte de données).\nOn pourrait ignorer la censure par intervalle en associant chaque intervalle de la forme \\([t, t+1)\\) à la valeur \\(t\\).\n\nDonnez une estimation de la probabilité qu’une entreprise survive trois ans et plus.\n\nOn veut la survie \\(\\Pr(T \\geq 3)\\) L’estimation est donnée dans la colonne Cumulative survival rate du Tableau 1, soit 0.6393.\n\nDonnez une estimation de la probabilité qu’une entreprise survive entre 4 et 5 ans, soit l’intervalle [4,5) ans.\n\nPuisque la fonction de survie empirique est constante, cela revient à la probabilité de défaillance dans l’intervalle [4,5) ans, soit \\[\\begin{align*}\n\\Pr\\{T \\in [4,5)\\} &= \\Pr(T \\ge 5) - \\Pr(T \\ge 4) \\\\&= 0.5525 - 0.5023 \\\\&= 0.0502.\n\\end{align*}\\] Si on traçait la courbe de Kaplan–Meier, cette probabilité correspond à la hauteur de la contre-marche entre 4 et 5 ans.\n\nQuel pourcentage des observations sont censurées?\n\nIl y a 219 observations au total et \\(38=13+25\\) firmes sont toujours en activité et donc leur temps de survie est censuré; le pourcentage est 38/219 ou 17.35%.\n\nÊtes-vous en mesure de nous fournir une estimation du troisième quartile de la fonction de survie? Justifiez votre réponse.\n\nNon, puisque les plus grandes temps observés sont censurées et l’estimation de la fonction de survie s’arrête à 0.2921; la fonction de survie ne traverse pas la valeur horizontale de 0.25\nLe Tableau 3 de l’article fournit les estimations d’un modèle à risques proportionnels de Cox.\n\nComparez les modèles (1) et (2). Est-ce que l’effet de cohorte impacte la survie?\n\nOui. On peut voir cela de plusieurs façons puisque (1) les deux coefficients associés sont significativement différents de zéro (tests de Wald) avec, pour COHORT_82, \\(Z = -0.5518/0.2081=-2.651\\) et une valeur-\\(p\\) basée sur l’approximation normale de 0.008 (idem pour COHORT_83).\nOn peut aussi construire une statistique du rapport de vraisemblance avec \\[-2\\ell_{(1)} + 2 \\ell_{(2)} =1503.442-1471.353 = 32.089.\\] Or, l’inclusion des cohortes ajoute deux paramètres, donc cette statistique est à comparer avec une loi \\(\\chi^2_2\\).\n\n\nCode\n# Test de Wald: beta_cohort82 = 0\n2*pnorm(-0.5518/0.2081)\n\n\n[1] 0.008010906\n\n\nCode\n# Test de Wald: beta_cohort83 = 0\n2*pnorm(-0.5464/0.2021)\n\n\n[1] 0.006859031\n\n\nCode\n# Test de rapport de vraisemblance\npchisq(1503.442 - 1471.353, \n       df = 2, \n       lower.tail = FALSE)\n\n\n[1] 1.076371e-07\n\n\n\nPour le modèle (3), décrivez le plus précisément possible l’effet des variables capital (FIXED_ASSET) et le montant de la dette (DEBT) sur la durée de vie des entreprises.\n\nToute chose étant égale par ailleurs, une augmentation du ratio du passif total sur le total des actifs (DEBT) de 1 augmente le risque de 119.3%; le rapport de risque étant multiplié par \\(\\exp(0.83)=2.293\\).\nCeteris paribus, une augmentation du ratio des immobilisations sur le total des actifs de 1 diminue le risque d’environ 59.8%, avec un rapport de risque de \\(\\exp(-0.9091) =0.4028\\)."
  },
  {
    "objectID": "exercices/04-solution.html#exercice-4.2",
    "href": "exercices/04-solution.html#exercice-4.2",
    "title": "Analyse de survie",
    "section": "Exercice 4.2",
    "text": "Exercice 4.2\nUn commerce de chaussures de Montréal veut optimiser son inventaire afin de maximiser ses profits et fait appel à votre société de conseil. La base de données chaussures contient des observations fictives et les variables suivantes:\n\nstatut: variable catégorielle, 0 s’il est vendu, 1 si l’article est toujours en stock, 2 s’il est déstocké (les modèles invendus après 40 mois en magasins sont passés aux pertes et profits).\ntemps: temps de stockage de l’article (en mois).\nprix: prix de vente réelle de l’article (avec rabais si applicable), arrondi à l’unité près.\nsexe: variable catégorielle, 0 pour modèle pour homme, 1 pour femme.\n\nNotre objectif premier est d’estimer le temps qu’un article passe en stock avant d’être vendu.\n\nQue représente la censure dans cet exemple?\n\nLes données censurées à droites correspondent aux paires de chaussure qui sont toujours en stock à la fin de la période d’observation ou qui sont invendues après 40 mois.\n\nEstimez le temps de stockage à l’aide d’un modèle de Kaplan–Meier et rapportez les estimés des quartiles.\n\nOn note d’abord que la colonne temps début à 1, ce qui indique que les chaussures sont vendues dans le premier mois. Un temps de \\(t\\) mois correspond à la plage \\([t-1, t)\\) mois en stock.\n\n\nCode\nlibrary(survival)\ndata(chaussures, package = \"hecmulti\")\n# censure si 1 ou 2\n# statut == 0 =&gt; TRUE (1) si vendu, FALSE (0) sinon\nkm &lt;- survfit(\n  formula = Surv(time = temps, \n                 event = statut == 0) ~ 1,\n  type = \"kaplan-meier\",\n  data = chaussures)\n\nplot(km)\n\n\n\n\n\nCode\n# Estimation des quartiles\nquantile(km)$quantile\n\n\n25 50 75 \n 5  8 12 \n\n\nLes quartiles sont 5 mois (25% des chaussures sont en inventaire moins de ce temps), 8 mois (temps médian en stock) et 12 mois (75% des chaussures sont vendues dans l’année de leur achat).\n\nAjustez un modèle à risque proportionnel de Cox pour la durée de stockage en fonction du sexe et du prix de vente.\n\nRapportez et interprétez les coefficients des variables.\nEst-ce que les effets estimés sont significatifs?\n\n\n\n\nCode\nmodcox &lt;- coxph(\n  Surv(time = temps, \n       event = statut == 0) ~ sexe + prix,\n  data = chaussures)\nsummary(modcox)\n\n\nCall:\ncoxph(formula = Surv(time = temps, event = statut == 0) ~ sexe + \n    prix, data = chaussures)\n\n  n= 6807, number of events= 5880 \n\n           coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    \nsexe -0.1640023  0.8487400  0.0262256  -6.254 4.01e-10 ***\nprix -0.0135855  0.9865064  0.0009587 -14.170  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nsexe    0.8487      1.178    0.8062    0.8935\nprix    0.9865      1.014    0.9847    0.9884\n\nConcordance= 0.589  (se = 0.005 )\nLikelihood ratio test= 247.8  on 2 df,   p=&lt;2e-16\nWald test            = 239.1  on 2 df,   p=&lt;2e-16\nScore (logrank) test = 238.9  on 2 df,   p=&lt;2e-16\n\n\nChaque augmentation du prix d’un dollar mène à une diminution de risque de vente de 1.35%. De manière équivalente, une augmentation d’un dollar multiplie le risque par 0.9865, soit une diminution de 12.7% pour une différence de prix de 10$. Cet effet est significativement différent de zéro (statistique de Wald de \\(z = -14.17\\), valeur-\\(p\\) négligeable).\nPour une paire de chaussure du même prix, le risque de vendre la paire de chaussures pour femmes est de 15.13% inférieur à celui d’une paire pour hommes. Cette différence est significative à niveau 1% (statistique de Wald \\(z=-6.254\\)). Les souliers pour femmes restent plus longtemps en stock que ceux des hommes au même prix.\n\nTracez les estimés des courbes de survie d’une chaussure de l’année dont le prix de vente est 120$ pour les modèles pour homme et pour femme.\n\n\n\nCode\n# Nouvelle base de données avec profil des chaussures\nndb &lt;- data.frame(sexe = c(0, 1),\n                  prix = rep(120, 2))\n# Obtention des prédictions de la survie\npred &lt;- survfit(modcox, \n                newdata = ndb, \n                type = \"kaplan-meier\")\n# Graphique R de basedes prédictions\nplot(pred, \n     col = c(\"red\", \"blue\"), # couleur\n     lty = 1:2, # type de ligne\n     xlab = \"temps (en mois)\", #axe des abcisses\n     ylab = \"survie\") #axe des ordonnées\n\n\n\n\n\nFigure 1: Estimé de la fonction de survie pour une paire de chaussure de 120$ pour hommes (traits pleins rouges) et femme (traitillé bleu)\n\n\n\n\nOn vous informe que, pour éliminer les invendus lors de l’arrivée de nouveaux modèles, l’entreprise offre une réduction de 20% après 15 mois.\n\nAjustez un modèle de Cox qui prendra en compte cette nouvelle information. Rapportez les estimés des paramètres de votre modèle; est-ce que vos interprétations changent?\n\nPour ce faire il faut modifier la base de données et casser la contribution en deux selon que le temps soit supérieur à 15 mois ou pas.\nOn obtient donc une plage de \\((0,t]\\) pour les chaussures vendues en dessous de 15 mois (aucun changement).\nPour les chaussures mises au rabais on a une plage de \\((0, 15]\\) avec censure à droite (dans chaussure1 ci dessous) et finalement \\((15,t]\\) avec l’événement pour les autres chaussures (chaussures2).\n\n\nCode\nlibrary(dplyr)\nchaussures1 &lt;- chaussures |&gt;\n  dplyr::mutate(\n    temps0 = 0, \n    # censure si chaussures invendues après 15 semaines\n    statut = ifelse(temps &gt; 15, 1, statut),\n    # enlever rabais pour obtenir prix initial\n    prix = ifelse(temps &gt; 15, prix/0.8, prix),\n    temps = pmin(temps, 15))\nchaussures2 &lt;- chaussures |&gt;\n  dplyr::filter(temps &gt; 15) |&gt; \n  dplyr::mutate(temps0 = 15) \nmodcox2 &lt;- coxph(\n  Surv(time = temps0, \n       time2 = temps,\n       event = statut == 0) ~ sexe + prix,\n  data = dplyr::full_join(chaussures1, \n                          chaussures2))\nsummary(modcox2)\n\n\nCall:\ncoxph(formula = Surv(time = temps0, time2 = temps, event = statut == \n    0) ~ sexe + prix, data = dplyr::full_join(chaussures1, chaussures2))\n\n  n= 7553, number of events= 5880 \n\n          coef exp(coef)  se(coef)       z Pr(&gt;|z|)    \nsexe -0.116920  0.889657  0.026250  -4.454 8.43e-06 ***\nprix -0.042049  0.958823  0.001034 -40.665  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nsexe    0.8897      1.124    0.8450    0.9366\nprix    0.9588      1.043    0.9569    0.9608\n\nConcordance= 0.675  (se = 0.004 )\nLikelihood ratio test= 1922  on 2 df,   p=&lt;2e-16\nWald test            = 1686  on 2 df,   p=&lt;2e-16\nScore (logrank) test = 1683  on 2 df,   p=&lt;2e-16\n\n\nLa différence est à peine perceptible, une différence de quatres chiffres après la virgule pour \\(\\exp(\\widehat{\\beta}_{\\text{prix}})\\). Les coefficients sont toujours significatifs.\n\nOn pourrait considérer un modèle à risque non-proportionnels contenant une interaction entre le prix et le temps de manière à ajuster le même modèle. Expliquez comment cela pourrait être fait de manière à obtenir les mêmes estimés des paramètres.\n\nOn peut ajuster un modèle à risque non-proportionnel avec le terme \\[\\texttt{tprix} = \\texttt{prix} -0.2\\texttt{prix}\\mathrm{I}(t &gt; 15),\\] avec le modèle de Cox \\[\\begin{align*}\nh(t; \\texttt{sexe}, \\texttt{prix}) = h_0(t) \\exp \\left\\{\\beta_{\\texttt{sexe}}\\texttt{sexe} + \\beta_{\\texttt{tprix}} \\texttt{tprix}\\right\\}.\n\\end{align*}\\] Notez que, si on inclut \\(\\texttt{prix}\\) et un terme d’interaction, on se retrouverait avec deux coefficients pour le prix plutôt qu’un seul.\n\n\nCode\n# Calculer prix initial\nchaussures &lt;- chaussures |&gt;\n  dplyr::mutate(\n    prixinit = ifelse(temps &gt; 15, \n                      prix/0.8, \n                      prix))\nmodcox3 &lt;- coxph(\n  Surv(time = temps,\n       event = statut == 0) ~ sexe + tt(prixinit),\n  data = chaussures,\n  tt = function(x, t, ...){ x-0.2*I(t&gt;15)*x})\nsummary(modcox3)\n\n\nCall:\ncoxph(formula = Surv(time = temps, event = statut == 0) ~ sexe + \n    tt(prixinit), data = chaussures, tt = function(x, t, ...) {\n    x - 0.2 * I(t &gt; 15) * x\n})\n\n  n= 6807, number of events= 5880 \n\n                  coef exp(coef)  se(coef)       z Pr(&gt;|z|)    \nsexe         -0.116920  0.889657  0.026250  -4.454 8.43e-06 ***\ntt(prixinit) -0.042049  0.958823  0.001034 -40.665  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             exp(coef) exp(-coef) lower .95 upper .95\nsexe            0.8897      1.124    0.8450    0.9366\ntt(prixinit)    0.9588      1.043    0.9569    0.9608\n\nConcordance= 0.675  (se = 0.004 )\nLikelihood ratio test= 1922  on 2 df,   p=&lt;2e-16\nWald test            = 1686  on 2 df,   p=&lt;2e-16\nScore (logrank) test = 1683  on 2 df,   p=&lt;2e-16\n\n\nUne fois qu’on a obtenu les estimations, on peut vérifier que les coefficients sont les mêmes\n\n\nCode\nisTRUE(all.equal(\n  coef(modcox3), \n  coef(modcox2), \n  check.attributes = FALSE))\n\n\n[1] TRUE"
  },
  {
    "objectID": "exercices/05-solution.html",
    "href": "exercices/05-solution.html",
    "title": "Analyse factorielle",
    "section": "",
    "text": "Les observations de la base de données bjffacto sont tirées de l’article Bastian et al. (2014) et sont rattachées à une expérience en psychologie visant à corroborer l’hypothèse qu’il y a une coopération accrue entre individus sujets à une expérience traumatisante. La moitié des participant(e)s a dû plonger sa main dans un bain d’eau glacé, tandis que l’autre moitié a dû faire la même chose dans un bain d’eau tiède; les deux groupes devaient ensuite faire un jeu visant à identifier leur niveau de coopération.\nLa variable condition indique le groupe expérimental (zéro pour groupe contrôle, un pour douleur).\nIndication: utilisez la matrice de corrélation pour vos analyses factorielles\n\nPourquoi n’est-il pas nécessaire de standardiser les variables avant de procéder à l’analyse exploratoire? Justifiez votre réponse\n\nCe sont toutes des échelles de Likert avec le même nombre de modalités, donc comparables entre elles. La différence entre la matrice de corrélation et la matrice de covariance réside dans l’importance accordée aux variables. On mettra moins de poids dans le deuxième cas sur les items plus consensuels (pour lesquels l’écart-type est inférieur).\nIl est important pour la suite de retirer la variable condition, qui n’est pas un item du questionnaire (le but étant de calucler les différences d’échelles selon le groupe expérimental).\n\nEffectuez une analyse factorielle exploratoire à l’aide de la méthode des composantes principales.\n\nEn utilisant le critère de Kaiser (valeurs propres) ou le diagramme d’éboulis, déterminez un nombre adéquat de facteurs à employer.\nProduisez un diagramme d’éboulis et rapportez ce dernier.\nQuel pourcentage de la variance totale est expliquée par votre combinaison de facteurs?\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(hecmulti)\ndb &lt;- bjffacto |&gt; select(!condition)\ncovdb &lt;- cor(db)\ndecompo &lt;- eigen(covdb)\neboulis(decompo)\n\n\n\n\n\nCode\nnkaiser &lt;- sum(decompo$values &gt; 1)\nvar_cumu &lt;- with(decompo,\n                 cumsum(values)/sum(values))\n\n\nLe critère des valeurs propres de Kaiser (variances des composantes principales supérieures à 1) indique 5 facteurs, qui conjointement expliquent 74.3 pourcent de la variance totale des items du questionnaire.\nLe diagramme d’éboulis indique un coude à partir de deux facteurs.\n\n\nCode\ncp_2 &lt;- factocp(covmat = covdb, \n                nfact = 2)\nprint(cp_2, cutoff = 0.5)\n\n\n\nLoadings:\n      F1     F2    \n [1,] -0.719       \n [2,]        -0.557\n [3,] -0.761       \n [4,]        -0.537\n [5,] -0.742       \n [6,]        -0.849\n [7,]        -0.805\n [8,]        -0.570\n [9,] -0.872       \n[10,] -0.803       \n[11,]        -0.578\n[12,] -0.551       \n[13,]        -0.763\n[14,] -0.775       \n[15,]        -0.685\n[16,] -0.788       \n[17,] -0.688       \n[18,]        -0.596\n[19,] -0.835       \n[20,]        -0.799\n\n                  F1    F2\nSS loadings    5.990 4.787\nProportion Var 0.299 0.239\nCumulative Var 0.299 0.539\n\n\n\n\nCode\ncp_2 &lt;- factocp(x = db,\n                nfact = 2)\nprint(cp_2, cutoff = 0.5)\n\n\n\nLoadings:\n             F1     F2    \ninterested   -0.719       \ndistressed          -0.557\nexcited      -0.761       \nupset               -0.537\nstrong       -0.742       \nguilty              -0.849\nscared              -0.805\nhostile             -0.570\nenthusiastic -0.872       \nproud        -0.803       \nirritable           -0.578\nalert        -0.551       \nashamed             -0.763\ninspired     -0.775       \nnervous             -0.685\ndetermined   -0.788       \nattentive    -0.688       \njittery             -0.596\nactive       -0.835       \nafraid              -0.799\n\n                  F1    F2\nSS loadings    5.990 4.787\nProportion Var 0.299 0.239\nCumulative Var 0.299 0.539\n\n\nRappelez-vous que, peu importe le signe de la corrélation avec le facteur, il est surtout important qu’on considère des variables avec le même signe. Il est possible selon la formulation de la question que le signe soit opposé même si l’échelle est la même, si par exemple: «je fais confiance à mes gestionnaires» versus «je ne suis pas convaincu par l’équipe dirigeante» alors, on pourra inverser l’échelle avant d’aggréger et de créer une échelle.\nLogiquement, il faut aussi que les échelles soient constituées de variables semblables: on ne fera pas la moyenne d’un item mesuré sur \\([1, 100]\\) et d’un autre sur \\([-10, 10]\\)!\n\n\nRépétez la procédure, cette fois avec la méthode d’estimation par maximum de vraisemblance.\n\nRapportez les valeurs des critères d’information (AIC et BIC) pour \\(m=2, \\ldots, 6\\) facteurs dans un tableau.\nQuel nombre optimal de facteurs ces différents critères retournent-ils?\nY a-t-il un problème avec la solution de l’un d’entre eux?\n\n\nAvec 20 variables explicatives, on ne considérera pas plus de cinq facteurs. Si on ajuste le modèle de manière répété jusqu’à ce maximum, le modèle sélectionné par le AIC contient cinq variables (mais c’est un cas de Heywood), tandis que le BIC retourne le modèle avec deux facteurs.\n\n\nCode\nknitr::kable(\n  ajustement_factanal(db, factors = 1:5), \n  digits = 2)\n\n\n\n\n\nk\nAIC\nBIC\npval\nnpar\nheywood\n\n\n\n\n1\n855.45\n935.01\n&lt; 2.2e-16\n40\n0\n\n\n2\n676.10\n793.45\n5.430e-09\n59\n0\n\n\n3\n653.16\n806.31\n2.687e-06\n77\n0\n\n\n4\n626.10\n813.06\n0.0009868\n94\n0\n\n\n5\n607.76\n826.55\n0.0356262\n110\n1\n\n\n\n\n\nSi on ajuste le modèle avec deux facteurs, on obtient la même classification des variables par facteur qu’avec le modèle ajusté par la méthode des composantes principales (matrice de corrélation).\n\n\nCode\nfa_2 &lt;- factanal(x = db, factors = 2)\nprint(fa_2, cutoff = 0.4)\n\n\n\nCall:\nfactanal(x = db, factors = 2)\n\nUniquenesses:\n  interested   distressed      excited        upset       strong       guilty \n       0.531        0.796        0.426        0.804        0.529        0.176 \n      scared      hostile enthusiastic        proud    irritable        alert \n       0.294        0.692        0.212        0.403        0.765        0.735 \n     ashamed     inspired      nervous   determined    attentive      jittery \n       0.427        0.386        0.614        0.434        0.602        0.643 \n      active       afraid \n       0.303        0.463 \n\nLoadings:\n             Factor1 Factor2\ninterested    0.682         \ndistressed            0.424 \nexcited       0.757         \nupset                 0.440 \nstrong        0.686         \nguilty                0.896 \nscared                0.839 \nhostile               0.511 \nenthusiastic  0.884         \nproud         0.763         \nirritable             0.485 \nalert         0.492         \nashamed               0.756 \ninspired      0.770         \nnervous               0.612 \ndetermined    0.742         \nattentive     0.622         \njittery               0.505 \nactive        0.831         \nafraid                0.730 \n\n               Factor1 Factor2\nSS loadings      5.546   4.218\nProportion Var   0.277   0.211\nCumulative Var   0.277   0.488\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 272.3 on 151 degrees of freedom.\nThe p-value is 5.43e-09 \n\n\nLa solution à quatre facteurs n’est pas intéressante ici, puisque plusieurs variables sont fortement corrélées avec des facteurs. On pourrait les associer et créer des échelles, mais on créera une corrélation entre elles (du fait qu’elles sont construites à partir des mêmes variables) qui n’est pas forcément désirable.\nLa solution à cinq facteurs (corrélation) fait ressortir davantage de détail en séparant les émotions: degré d’engagement (facteur 1), fébrilité et peur (facteur 2), force et degré d’alerte (facteur 3), peur et honte (facteur 4) et détresse (facteur 5). Comme la frontière n’est pas claire, la solution à deux facteurs pourrait être préférée ici.\n\nComparez les regroupements obtenus avec les deux méthodes: est-ce que les regroupements sont semblables (c’est-à-dire, est-ce que les variables retournées dans les regroupements sont associées aux mêmes facteurs)?\n\nOui pour la solution à deux facteurs avec la matrice de corrélation.\n\nÉtiquetez les facteurs obtenus avec la méthode des composantes principales et \\(m=2\\) facteurs. Expliquez brièvement leur signification. Utilisez le seuil de coupure \\(r=0.5\\) pour les chargements avec rotation varimax pour déterminer si une variable fait partie d’un facteur.\n\nAvec deux facteurs et un point de coupure de 0.5, on retrouve les émotions positives et négatives si on utilise la matrice de corrélation.\n\nFacteur 1 (émotions positives): interested, excited, strong, enthusiastic, proud, alert, inspired, determined, attentive et active\nFacteur 2 (émotions négatives): distressed, upset, guilty, scared, hostile, irritable, ashamed, nervous, jittery et afraid\n\n\nCréez des échelles à partir des facteurs et calculez leur cohérence interne: rapportez le \\(\\alpha\\) de Cronbach pour chacun des facteurs.\n\n\n\nCode\nvars &lt;- apply(cp_2$loadings, 2, function(x){\n  which(abs(x) &gt; 0.5)}, simplify = FALSE)\nalphaCronbach &lt;- sapply(vars, function(index){\n  hecmulti::alphaC(db[, index])\n})\nalphaCronbach\n\n\n       F1        F2 \n0.9157199 0.8448532 \n\n\nLes deux échelles incluent des variables fortement corrélées et leur cohérence interne est suffisante pour que l’on aggrège en formant des échelles.\n\nRetournez un tableau de statistiques descriptives (moyenne et écart-type uniquement) pour chaque échelle, par condition expérimental (condition). Arrondissez à deux chiffres après la virgule et commentez sur les différences entre groupes, le cas échéant.\n\n\n\nCode\ndb_echelles &lt;- data.frame(\n  ech1 = rowMeans(db[,vars[[1]]]),\n  ech2 = rowMeans(db[,vars[[2]]]),\n  condition = bjffacto$condition) |&gt;\n  dplyr::mutate(condition = case_match(\n    condition,\n    0 ~ \"contrôle\",\n    1 ~ \"douleur\"))\nstatsdescript &lt;- db_echelles |&gt;\n  dplyr::group_by(condition) |&gt;\n  dplyr::summarize_all(\n    .funs = list(\n      moyenne = mean, \n      ecarttype = sd))\nknitr::kable(statsdescript, \n             digits = 2,\n             col.names = c(\"condition\", \n                           \"moyenne E1\", \n                           \"moyenne E2\", \n                           \"écart-type E1\",\n                           \"écart-type E2\"))\n\n\n\n\nTableau 1: Moyenne et écart-type des échelles par condition expérimentale\n\n\ncondition\nmoyenne E1\nmoyenne E2\nécart-type E1\nécart-type E2\n\n\n\n\ncontrôle\n2.80\n1.27\n0.83\n0.37\n\n\ndouleur\n3.05\n1.34\n0.82\n0.45\n\n\n\n\n\n\nOn peut considérer les différences entres groupes à l’aide d’un test de Welch pour deux échantillons (échantillons aléatoires indépendants). Aucune des différences n’est statistiquement significative à niveau 5%.\n\n\nCode\nt.test(ech1 ~ condition, data = db_echelles)\nt.test(ech2 ~ condition, data = db_echelles)\n\n\n\n\nTableau 2: Tests-t pour deux échantillons par condition expérimentale pour chacune des échelles.\n\n\n\n\n(a) Échelle 1\n\n\nstatistique\nddl\nvaleur-p\ndifférence\nborne inf.\nborne sup.\n\n\n\n\n-1.09\n51.99\n0.28\n-0.24\n-0.7\n0.21\n\n\n\n\n\n\n\n\n(b) Échelle 2\n\n\nstatistique\nddl\nvaleur-p\ndifférence\nborne inf.\nborne sup.\n\n\n\n\n-0.6\n49.81\n0.55\n-0.07\n-0.29\n0.16"
  },
  {
    "objectID": "exercices/05-solution.html#exercice-5.1",
    "href": "exercices/05-solution.html#exercice-5.1",
    "title": "Analyse factorielle",
    "section": "",
    "text": "Les observations de la base de données bjffacto sont tirées de l’article Bastian et al. (2014) et sont rattachées à une expérience en psychologie visant à corroborer l’hypothèse qu’il y a une coopération accrue entre individus sujets à une expérience traumatisante. La moitié des participant(e)s a dû plonger sa main dans un bain d’eau glacé, tandis que l’autre moitié a dû faire la même chose dans un bain d’eau tiède; les deux groupes devaient ensuite faire un jeu visant à identifier leur niveau de coopération.\nLa variable condition indique le groupe expérimental (zéro pour groupe contrôle, un pour douleur).\nIndication: utilisez la matrice de corrélation pour vos analyses factorielles\n\nPourquoi n’est-il pas nécessaire de standardiser les variables avant de procéder à l’analyse exploratoire? Justifiez votre réponse\n\nCe sont toutes des échelles de Likert avec le même nombre de modalités, donc comparables entre elles. La différence entre la matrice de corrélation et la matrice de covariance réside dans l’importance accordée aux variables. On mettra moins de poids dans le deuxième cas sur les items plus consensuels (pour lesquels l’écart-type est inférieur).\nIl est important pour la suite de retirer la variable condition, qui n’est pas un item du questionnaire (le but étant de calucler les différences d’échelles selon le groupe expérimental).\n\nEffectuez une analyse factorielle exploratoire à l’aide de la méthode des composantes principales.\n\nEn utilisant le critère de Kaiser (valeurs propres) ou le diagramme d’éboulis, déterminez un nombre adéquat de facteurs à employer.\nProduisez un diagramme d’éboulis et rapportez ce dernier.\nQuel pourcentage de la variance totale est expliquée par votre combinaison de facteurs?\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(hecmulti)\ndb &lt;- bjffacto |&gt; select(!condition)\ncovdb &lt;- cor(db)\ndecompo &lt;- eigen(covdb)\neboulis(decompo)\n\n\n\n\n\nCode\nnkaiser &lt;- sum(decompo$values &gt; 1)\nvar_cumu &lt;- with(decompo,\n                 cumsum(values)/sum(values))\n\n\nLe critère des valeurs propres de Kaiser (variances des composantes principales supérieures à 1) indique 5 facteurs, qui conjointement expliquent 74.3 pourcent de la variance totale des items du questionnaire.\nLe diagramme d’éboulis indique un coude à partir de deux facteurs.\n\n\nCode\ncp_2 &lt;- factocp(covmat = covdb, \n                nfact = 2)\nprint(cp_2, cutoff = 0.5)\n\n\n\nLoadings:\n      F1     F2    \n [1,] -0.719       \n [2,]        -0.557\n [3,] -0.761       \n [4,]        -0.537\n [5,] -0.742       \n [6,]        -0.849\n [7,]        -0.805\n [8,]        -0.570\n [9,] -0.872       \n[10,] -0.803       \n[11,]        -0.578\n[12,] -0.551       \n[13,]        -0.763\n[14,] -0.775       \n[15,]        -0.685\n[16,] -0.788       \n[17,] -0.688       \n[18,]        -0.596\n[19,] -0.835       \n[20,]        -0.799\n\n                  F1    F2\nSS loadings    5.990 4.787\nProportion Var 0.299 0.239\nCumulative Var 0.299 0.539\n\n\n\n\nCode\ncp_2 &lt;- factocp(x = db,\n                nfact = 2)\nprint(cp_2, cutoff = 0.5)\n\n\n\nLoadings:\n             F1     F2    \ninterested   -0.719       \ndistressed          -0.557\nexcited      -0.761       \nupset               -0.537\nstrong       -0.742       \nguilty              -0.849\nscared              -0.805\nhostile             -0.570\nenthusiastic -0.872       \nproud        -0.803       \nirritable           -0.578\nalert        -0.551       \nashamed             -0.763\ninspired     -0.775       \nnervous             -0.685\ndetermined   -0.788       \nattentive    -0.688       \njittery             -0.596\nactive       -0.835       \nafraid              -0.799\n\n                  F1    F2\nSS loadings    5.990 4.787\nProportion Var 0.299 0.239\nCumulative Var 0.299 0.539\n\n\nRappelez-vous que, peu importe le signe de la corrélation avec le facteur, il est surtout important qu’on considère des variables avec le même signe. Il est possible selon la formulation de la question que le signe soit opposé même si l’échelle est la même, si par exemple: «je fais confiance à mes gestionnaires» versus «je ne suis pas convaincu par l’équipe dirigeante» alors, on pourra inverser l’échelle avant d’aggréger et de créer une échelle.\nLogiquement, il faut aussi que les échelles soient constituées de variables semblables: on ne fera pas la moyenne d’un item mesuré sur \\([1, 100]\\) et d’un autre sur \\([-10, 10]\\)!\n\n\nRépétez la procédure, cette fois avec la méthode d’estimation par maximum de vraisemblance.\n\nRapportez les valeurs des critères d’information (AIC et BIC) pour \\(m=2, \\ldots, 6\\) facteurs dans un tableau.\nQuel nombre optimal de facteurs ces différents critères retournent-ils?\nY a-t-il un problème avec la solution de l’un d’entre eux?\n\n\nAvec 20 variables explicatives, on ne considérera pas plus de cinq facteurs. Si on ajuste le modèle de manière répété jusqu’à ce maximum, le modèle sélectionné par le AIC contient cinq variables (mais c’est un cas de Heywood), tandis que le BIC retourne le modèle avec deux facteurs.\n\n\nCode\nknitr::kable(\n  ajustement_factanal(db, factors = 1:5), \n  digits = 2)\n\n\n\n\n\nk\nAIC\nBIC\npval\nnpar\nheywood\n\n\n\n\n1\n855.45\n935.01\n&lt; 2.2e-16\n40\n0\n\n\n2\n676.10\n793.45\n5.430e-09\n59\n0\n\n\n3\n653.16\n806.31\n2.687e-06\n77\n0\n\n\n4\n626.10\n813.06\n0.0009868\n94\n0\n\n\n5\n607.76\n826.55\n0.0356262\n110\n1\n\n\n\n\n\nSi on ajuste le modèle avec deux facteurs, on obtient la même classification des variables par facteur qu’avec le modèle ajusté par la méthode des composantes principales (matrice de corrélation).\n\n\nCode\nfa_2 &lt;- factanal(x = db, factors = 2)\nprint(fa_2, cutoff = 0.4)\n\n\n\nCall:\nfactanal(x = db, factors = 2)\n\nUniquenesses:\n  interested   distressed      excited        upset       strong       guilty \n       0.531        0.796        0.426        0.804        0.529        0.176 \n      scared      hostile enthusiastic        proud    irritable        alert \n       0.294        0.692        0.212        0.403        0.765        0.735 \n     ashamed     inspired      nervous   determined    attentive      jittery \n       0.427        0.386        0.614        0.434        0.602        0.643 \n      active       afraid \n       0.303        0.463 \n\nLoadings:\n             Factor1 Factor2\ninterested    0.682         \ndistressed            0.424 \nexcited       0.757         \nupset                 0.440 \nstrong        0.686         \nguilty                0.896 \nscared                0.839 \nhostile               0.511 \nenthusiastic  0.884         \nproud         0.763         \nirritable             0.485 \nalert         0.492         \nashamed               0.756 \ninspired      0.770         \nnervous               0.612 \ndetermined    0.742         \nattentive     0.622         \njittery               0.505 \nactive        0.831         \nafraid                0.730 \n\n               Factor1 Factor2\nSS loadings      5.546   4.218\nProportion Var   0.277   0.211\nCumulative Var   0.277   0.488\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 272.3 on 151 degrees of freedom.\nThe p-value is 5.43e-09 \n\n\nLa solution à quatre facteurs n’est pas intéressante ici, puisque plusieurs variables sont fortement corrélées avec des facteurs. On pourrait les associer et créer des échelles, mais on créera une corrélation entre elles (du fait qu’elles sont construites à partir des mêmes variables) qui n’est pas forcément désirable.\nLa solution à cinq facteurs (corrélation) fait ressortir davantage de détail en séparant les émotions: degré d’engagement (facteur 1), fébrilité et peur (facteur 2), force et degré d’alerte (facteur 3), peur et honte (facteur 4) et détresse (facteur 5). Comme la frontière n’est pas claire, la solution à deux facteurs pourrait être préférée ici.\n\nComparez les regroupements obtenus avec les deux méthodes: est-ce que les regroupements sont semblables (c’est-à-dire, est-ce que les variables retournées dans les regroupements sont associées aux mêmes facteurs)?\n\nOui pour la solution à deux facteurs avec la matrice de corrélation.\n\nÉtiquetez les facteurs obtenus avec la méthode des composantes principales et \\(m=2\\) facteurs. Expliquez brièvement leur signification. Utilisez le seuil de coupure \\(r=0.5\\) pour les chargements avec rotation varimax pour déterminer si une variable fait partie d’un facteur.\n\nAvec deux facteurs et un point de coupure de 0.5, on retrouve les émotions positives et négatives si on utilise la matrice de corrélation.\n\nFacteur 1 (émotions positives): interested, excited, strong, enthusiastic, proud, alert, inspired, determined, attentive et active\nFacteur 2 (émotions négatives): distressed, upset, guilty, scared, hostile, irritable, ashamed, nervous, jittery et afraid\n\n\nCréez des échelles à partir des facteurs et calculez leur cohérence interne: rapportez le \\(\\alpha\\) de Cronbach pour chacun des facteurs.\n\n\n\nCode\nvars &lt;- apply(cp_2$loadings, 2, function(x){\n  which(abs(x) &gt; 0.5)}, simplify = FALSE)\nalphaCronbach &lt;- sapply(vars, function(index){\n  hecmulti::alphaC(db[, index])\n})\nalphaCronbach\n\n\n       F1        F2 \n0.9157199 0.8448532 \n\n\nLes deux échelles incluent des variables fortement corrélées et leur cohérence interne est suffisante pour que l’on aggrège en formant des échelles.\n\nRetournez un tableau de statistiques descriptives (moyenne et écart-type uniquement) pour chaque échelle, par condition expérimental (condition). Arrondissez à deux chiffres après la virgule et commentez sur les différences entre groupes, le cas échéant.\n\n\n\nCode\ndb_echelles &lt;- data.frame(\n  ech1 = rowMeans(db[,vars[[1]]]),\n  ech2 = rowMeans(db[,vars[[2]]]),\n  condition = bjffacto$condition) |&gt;\n  dplyr::mutate(condition = case_match(\n    condition,\n    0 ~ \"contrôle\",\n    1 ~ \"douleur\"))\nstatsdescript &lt;- db_echelles |&gt;\n  dplyr::group_by(condition) |&gt;\n  dplyr::summarize_all(\n    .funs = list(\n      moyenne = mean, \n      ecarttype = sd))\nknitr::kable(statsdescript, \n             digits = 2,\n             col.names = c(\"condition\", \n                           \"moyenne E1\", \n                           \"moyenne E2\", \n                           \"écart-type E1\",\n                           \"écart-type E2\"))\n\n\n\n\nTableau 1: Moyenne et écart-type des échelles par condition expérimentale\n\n\ncondition\nmoyenne E1\nmoyenne E2\nécart-type E1\nécart-type E2\n\n\n\n\ncontrôle\n2.80\n1.27\n0.83\n0.37\n\n\ndouleur\n3.05\n1.34\n0.82\n0.45\n\n\n\n\n\n\nOn peut considérer les différences entres groupes à l’aide d’un test de Welch pour deux échantillons (échantillons aléatoires indépendants). Aucune des différences n’est statistiquement significative à niveau 5%.\n\n\nCode\nt.test(ech1 ~ condition, data = db_echelles)\nt.test(ech2 ~ condition, data = db_echelles)\n\n\n\n\nTableau 2: Tests-t pour deux échantillons par condition expérimentale pour chacune des échelles.\n\n\n\n\n(a) Échelle 1\n\n\nstatistique\nddl\nvaleur-p\ndifférence\nborne inf.\nborne sup.\n\n\n\n\n-1.09\n51.99\n0.28\n-0.24\n-0.7\n0.21\n\n\n\n\n\n\n\n\n(b) Échelle 2\n\n\nstatistique\nddl\nvaleur-p\ndifférence\nborne inf.\nborne sup.\n\n\n\n\n-0.6\n49.81\n0.55\n-0.07\n-0.29\n0.16"
  },
  {
    "objectID": "exercices/05-solution.html#exercice-5.2",
    "href": "exercices/05-solution.html#exercice-5.2",
    "title": "Analyse factorielle",
    "section": "Exercice 5.2",
    "text": "Exercice 5.2\nLes données sondage_entreprise contiennent les résultats d’un sondage effectué par une compagnie auprès de ses employés.\n1. Produisez des statistiques descriptives pour les variables q8 à q17.\n\n\nCode\ndata(sondage_entreprise, package = \"hecmulti\")\nstr(sondage_entreprise) #aperçu de la base de données\n\n\ntibble [482 × 10] (S3: tbl_df/tbl/data.frame)\n $ q8 : int [1:482] 4 3 5 3 3 4 1 3 2 3 ...\n $ q9 : int [1:482] 3 3 5 3 3 4 1 4 2 3 ...\n $ q10: int [1:482] 4 3 5 2 3 4 1 5 4 5 ...\n $ q11: int [1:482] 4 3 5 2 3 5 1 4 4 4 ...\n $ q12: int [1:482] 4 3 5 3 3 5 1 4 3 3 ...\n $ q13: int [1:482] 4 2 5 4 4 5 1 4 3 4 ...\n $ q14: int [1:482] 2 3 5 4 3 5 1 4 3 3 ...\n $ q15: int [1:482] 3 3 5 3 3 5 1 3 3 2 ...\n $ q16: int [1:482] 3 3 5 3 3 3 1 3 3 2 ...\n $ q17: int [1:482] 4 3 5 3 3 4 1 3 3 4 ...\n\n\nOn peut utiliser summary pour obtenir les statistiques descriptives, mais les échelles de Likert sont toutes de 1 à 5.\n\n\n\n\nTableau 3: Statistiques descriptives des questions du sondage en entreprise.\n\n\nmoyenne\nécart-type\nhistogramme\n\n\n\n\n3.44\n0.91\n▁▁▇▅▂\n\n\n3.39\n0.97\n▁▂▇▆▂\n\n\n4.33\n0.73\n▁▁▂▇▇\n\n\n4.10\n0.84\n▁▁▂▇▆\n\n\n3.83\n0.89\n▁▁▇▇▆\n\n\n4.02\n0.84\n▁▁▃▇▆\n\n\n3.88\n0.96\n▁▁▅▇▅\n\n\n3.54\n0.85\n▁▁▇▅▂\n\n\n3.54\n0.91\n▁▁▇▆▃\n\n\n3.51\n0.84\n▁▁▇▅▂\n\n\n\n\n\n\n\n\nOn peut voir sur le corrélogramme que toutes les variables sont très corrélées entre elles.\n\n\nCode\ncorrplot::corrplot(corr = cor(sondage_entreprise),\n                   type = \"upper\", \n                   diag = FALSE)\n\n\n\n\n\n2. Combien y a-t-il de répondants? Déterminez si ce nombre est suffisant pour effectuer une analyse factorielle.\n\n\nCode\nnrow(sondage_entreprise)\n\n\nIl y a 482 observations et 10 variables. C’est suffisant pour une analyse factorielle, mais le nombre de facteurs ne sera probablement pas très élevé.\n3. Utilisez la méthode d’estimation par composantes principales et le critère de Kaiser pour ajuster le modèle d’analyse factorielle. Combien de facteurs serait-il raisonnable de retenir?\n\n\nCode\nlibrary(hecmulti)\n# Composantes principales, critère de Kaiser\nafk &lt;- factocp(sondage_entreprise, \n               cor = TRUE)\nspectral &lt;- eigen(cor(sondage_entreprise))\nhecmulti::eboulis(spectral)\n\n\n\n\n\nCode\nprint(afk, cutoff = 0.5)\n\n\n\nLoadings:\n    F1     F2    \nq8  -0.836       \nq9  -0.850       \nq10        -0.814\nq11        -0.619\nq12        -0.716\nq13        -0.821\nq14 -0.652       \nq15 -0.633       \nq16 -0.677       \nq17 -0.713       \n\n                  F1    F2\nSS loadings    3.698 2.797\nProportion Var 0.370 0.280\nCumulative Var 0.370 0.649\n\n\nLe critère de Kaiser (valeurs propres supérieures à 1) avec la matrice de corrélation suggère deux facteurs. Le diagramme d’éboulis donne deux facteurs également avec la matrice de corrélation.\nOn peut constater en étudiant les chargements que le premier facteur est très corrélé avec plusieurs variables. En augmentant le point de coupure, on obtient un regroupement avec grosso-modo pour le facteur 1 (Q8-Q9 et Q14-Q17) les questions qui se réfèrent à la réputation sociale de l’entreprise et la balance pour le facteur 2 (Q10-Q13) pour la réputation sur le plan de la gestion. Seule la question Q15 semble un peu à part\n4. Répétez l’exercice, cette fois avec la méthode du maximum de vraisemblance\n- **Est-ce qu'un de ces modèles ajustés est un cas de quasi-Heywood?**\n- **Combien de facteurs les critères d'information recommendent-ils?**\n- **Si vous ajoutez des facteurs, est-ce que votre interprétation change?**\n\n\nCode\nres_tableau &lt;- ajustement_factanal(\n  factors = 1:4, \n  covmat = cor(sondage_entreprise),\n  n.obs = nrow(sondage_entreprise))\nknitr::kable(res_tableau)\n\n\n\n\n\nk\nAIC\nBIC\npval\nnpar\nheywood\n\n\n\n\n1\n2671.696\n2755.255\n&lt; 2.2e-16\n20\n0\n\n\n2\n2469.875\n2591.036\n&lt; 2.2e-16\n29\n0\n\n\n3\n2401.128\n2555.712\n6.646e-05\n37\n0\n\n\n4\n2374.255\n2558.084\n0.5227\n44\n1\n\n\n\n\n\n\n\nOn voit que le modèle AIC suggère un modèle à quatre facteurs (un cas de quasi-Heywood), tandis que le BIC suggère plutôt trois facteurs. Le test du rapport de vraisemblance comparant la corrélation empirique au modèle d’analyse factorielle indique que le modèle est raisonnable à partir de quatre. Le problème de convergence nous amène à interpréter les résultats pour le modèle à quatre facteurs avec un grin de sel.\nPuisque la corrélation est très élevée, il est difficile ici d’interpréter les facteurs résultants parce que plusieurs variables sont corrélées avec de multiples facteurs. On peut augmenter le point de coupure à 0.5 pour déterminer les amalgames suivants:\n\nLe facteur 1 incluant Q8, Q9, Q16 et Q17 font référence à la réputation sociale et éthique de l’entreprise\nle facteur 2 comprenant Q10-Q13 correspond à la perception financière (innovation, gestion, finances)\nle facteur 3 regroupe Q9, Q11 et Q14 (éthique, confiance et innovation)\n\nL’interprétation change puisque ce qui était dans le premier facteur se retrouve dans le troisième, avec Q9 qui est présente dans les deux. En pratique, on pourrait décider de l’associer à l’une ou l’autre des échelles, selon la cohérence interne, et la logique.\n5. Créez des échelles et vérifiez leur cohérence interne. \n\n\nCode\ne1 &lt;- sondage_entreprise |&gt; \n  dplyr::select(c(q8:q9, q14:q17))\nalphaC(e1)\n\n\n[1] 0.8768725\n\n\nCode\nechelle1 &lt;- rowMeans(e1)\n\ne2 &lt;- sondage_entreprise |&gt; \n  dplyr::select(c(q10:q13))\nalphaC(e2)\n\n\n[1] 0.8292554\n\n\nCode\nechelle2 &lt;- rowMeans(e1)\n\n\nOn voit que les deux échelles créées ont un \\(\\alpha\\) de Cronbach de plus de 0.8, donc les échelles sont cohérentes. C’est en grande partie dû à la forte corrélation observée entre tous les items"
  },
  {
    "objectID": "exercices/06-solution.html",
    "href": "exercices/06-solution.html",
    "title": "Analyse de regroupements",
    "section": "",
    "text": "Les données fictives regroupement1 sont inspirées de Hsu & Lee (2002). Ces données contiennent des échelles pour certains éléments d’un questionnaire. Ce dernier a été élaboré afin d’évaluer l’importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l’aide d’une échelle de Likert à cinq points, allant de extrêmement important (5) à pas du tout important (1).\nLes variables représentent les activités sociales, les politiques de l’opérateur et références, la flexibilité des horaires, la santé et sécurité, le matériel publicitaire et la réputation.\n\nDoit-on standardiser les données avant d’effectuer l’analyse?\n\n\n\nCode\ndata(regroupements1, package = \"hecmulti\")\nstr(regroupements1)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   150 obs. of  6 variables:\n $ x1: num  3.9 2.8 3.6 2.8 2.5 2.5 2.5 2.2 3.1 3.1 ...\n $ x2: num  4.1 4.6 5 5 3.9 3.3 4.3 5 3.6 5 ...\n $ x3: num  3.6 5 4.3 4.4 3.6 3.6 2.6 2.5 4.5 3.7 ...\n $ x4: num  4.4 4.6 3.6 3.7 4.2 2.6 3.9 3.9 3.9 2.6 ...\n $ x5: num  4 4.5 3.5 3.2 3.2 3.9 4.2 5 4.9 5 ...\n $ x6: num  5 4 4.4 3.5 4 4.5 5 4.9 5 3.9 ...\n\n\nCode\nsummary(regroupements1)\n\n\n       x1              x2              x3              x4       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.100   1st Qu.:2.400   1st Qu.:1.925   1st Qu.:2.525  \n Median :2.800   Median :3.200   Median :3.000   Median :3.150  \n Mean   :2.933   Mean   :3.218   Mean   :2.957   Mean   :3.197  \n 3rd Qu.:3.775   3rd Qu.:4.100   3rd Qu.:3.900   3rd Qu.:3.900  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n       x5              x6       \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.400   1st Qu.:1.925  \n Median :3.450   Median :3.400  \n Mean   :3.321   Mean   :3.170  \n 3rd Qu.:4.200   3rd Qu.:4.400  \n Max.   :5.000   Max.   :5.000  \n\n\nLes données sont toutes sur la même échelle de Likert donc la standardisation n’est pas nécessaire apriori.\n\nFaites une analyse en composantes principales et projetez les observations sur un nuage de points avec les deux premières composantes principales.\n\n\n\nCode\nacp &lt;- princomp(regroupements1, cor = FALSE)\nhecmulti::eboulis(acp)\n\n\n\n\n\nCode\n# 2 à 3 composantes principales suffisantes\nlibrary(ggplot2)\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2)) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\") +\n  theme_classic()\n\n\n\n\n\nCode\n# Chargements\nacp$loadings\n\n\n\nLoadings:\n   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6\nx1  0.247  0.918  0.202                0.229\nx2  0.410        -0.498  0.758              \nx3  0.459         0.187         0.616 -0.609\nx4  0.343  0.140 -0.505 -0.568 -0.440 -0.304\nx5  0.410 -0.213  0.648  0.119 -0.588       \nx6  0.525 -0.300        -0.292  0.266  0.690\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.167  0.167  0.167  0.167  0.167  0.167\nCumulative Var  0.167  0.333  0.500  0.667  0.833  1.000\n\n\nOn voit plus ou moins clairement trois regroupements. Les chargements révèlent que la première composante principale est grosso modo une moyenne globale des scores, mais que le score pour les activités sociales ressort davantage dans la deuxième composante, comparativement au matériel publicitaire et à la réputation.\n\nUtilisez l’algorithme des \\(K\\)-moyennes en faisant varier le nombre de groupes de 1 à 10. Utilisez une dizaine d’initialisations aléatoires.\n\nSélectionnez un nombre de regroupement adéquat\nRetournez le nombre d’observation par groupe pour la valeur de \\(K\\) choisie.\nRapportez les statistiques descriptives (moyennes, etc.) de chaque segment\nInterprétez les profils obtenus.\n\n\n\n\nCode\nset.seed(60602)\nKmax &lt;- 10L # nombre maximum de groupes\nkmoy &lt;- list() # liste pour stocker les résultats des segmentations\nfor(k in seq_len(Kmax)){\n kmoy[[k]] &lt;- kmeans(x = regroupements1, \n                     centers = k,  # nombre de regroupements initiaux\n                     nstart = 10L) # nombre d'initialisation aléatoires\n}\n\n\nOn peut extraire le critère de la fonction objective (SCD): l’homogénéité est calculée comme la somme des carrés des distances intra-groupes. Cela nous permet d’utiliser les diagnostics pour la sélection avec le \\(R^2\\) en cherchant le coude à partir duquel aucun changement n’est visible.\n\n\nCode\nscd_intra &lt;- sapply(kmoy, function(x){x$tot.withinss})\n# Diagnostics pour sélection - clairement trois groupes\nhecmulti::homogeneite(scd = scd_intra)\n\n\n\n\n\nLes graphiques de \\(R^2\\) et le \\(R^2\\) semi-partiel indiquent tous deux clairement que trois regroupements est le choix le plus logique. On peut étiqueter les observations et visualiser les regroupements obtenus en projetant sur les deux premières composantes principales. Les groupes sont bien démarqués et il semble que l’initialisation aléatoire ait été adéquate.\n\n\nCode\n# Extraire les identifiants des regroupements\netiquettes_kmoy3 &lt;- kmoy[[3]]$cluster\n# Faire un graphique avec les composantes principales\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2, \n                     col = factor(etiquettes_kmoy3))) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\",\n       col = \"regroupement\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 1: Représentation des regroupements des \\(K\\)-moyennes projetés sur les deux premières composantes principales des données.\n\n\n\n\nLe nombre peut être extrait de la liste avec $size. On a 45 observations dans le groupe 1, 28 dans le groupe 2 et 77 dans le groupe 3. Au vu de la taille de la base de donnée, ces nombres sont relativement équilibrés.\nOn peut maintenant extraire les moyennes des trois regroupements. Le Tableau 1 montre le résultat arrondi à un chiffre après la virgule.\n\n\nCode\nknitr::kable(kmoy[[3]]$centers, digits = 1)\n\n\n\n\nTableau 1: Moyennes des trois regroupements obtenus avec les \\(K\\)-moyennes.\n\n\nx1\nx2\nx3\nx4\nx5\nx6\n\n\n\n\n1.7\n2.0\n1.6\n2.1\n2.1\n1.6\n\n\n4.5\n2.9\n2.6\n3.1\n3.0\n2.6\n\n\n3.0\n4.0\n3.9\n3.9\n4.2\n4.3\n\n\n\n\n\n\nLe groupe 1 est le groupe où les sujets ont les valeurs, en moyenne, les plus faibles pour les six variables. Le groupe 2 est celui où les sujets ont les valeurs, en moyenne, les plus élevées pour les 6 variables sauf pour la variable X1 (activité sociale). Le groupe 3 est celui où les sujets ont, en moyenne, la valeur la plus élevée de la variable X1 et des valeurs moyennes inférieures au groupe 3 mais supérieures au groupe 2 pour les cinq autres variables.\nDans l’article, les auteurs ont baptisé les sujets du groupe 1, les « indépendants », ceux du groupe 2, les « dépendants » et ceux du groupe 3, les « sociables ».\n\nFaites une segmentation avec d’autres méthodes. Est-ce que la segmentation est plus satisfaisante? Justifiez votre raisonnement.\n\nLa segmentation obtenue est très satisfaisante et interprétable, mais nous essayerons d’autres méthodes pour démontrer leur utilisation.\nLes méthodes basées sur la densité ne fonctionnent pas bien parce que les scores sont relativement rapprochés.\n\n\nCode\n# Regarder les plus proches voisins\ndbscan::kNNdistplot(regroupements1, \n                    minPts = 5)\nabline(h = 1.7, col = \"red\")\n\n\n\n\n\nCode\ndbscan_regroup &lt;- dbscan::dbscan(\n  x = regroupements1,\n  minPts = 5,\n  eps = 1.7)\ndbscan_regroup\n\n\nDBSCAN clustering for 150 objects.\nParameters: eps = 1.7, minPts = 5\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 2 cluster(s) and 6 noise points.\n\n 0  1  2 \n 6 99 45 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\n\nLa solution retournée fusionne deux regroupements et plusieurs valeurs sont traitées comme des points isolés.\n\n\nCode\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2, \n                     col = factor(dbscan_regroup$cluster + 1))) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\",\n       col = \"regroupement\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2: Représentation des regroupements DBSCAN projetés sur les deux premières composantes principales des données.\n\n\n\n\nAvec une petite taille d’échantillons, les méthodes de regroupements hiérarchiques sont compétitives et on a le luxe de pouvoir en essayer plusieurs.\n\n\nCode\ndist_euc &lt;- dist(regroupements1, method = \"euclidean\")\nrh_simple &lt;- fastcluster::hclust(dist_euc, method = \"single\")\nrh_complet &lt;- fastcluster::hclust(dist_euc, method = \"complete\")\nrh_ward &lt;- fastcluster::hclust(dist_euc, method = \"ward.D2\")\nrh_bary &lt;- fastcluster::hclust(dist_euc, method = \"centroid\")\nrh_moy &lt;- fastcluster::hclust(dist_euc, method = \"average\")\nrh_med &lt;- fastcluster::hclust(dist_euc, method = \"median\")\nrh_genie &lt;- genieclust::gclust(d = regroupements1)\nrh_energie &lt;- energy::energy.hclust(d = dist_euc)\n\n\nPour visualiser le dendrogramme, la méthode plot permet de tracer l’arborescence. Il suffit ensuite d’encadrer avec rect.hclust() en spécifiant le nombre de groupes. Par exemple,\n\n\nCode\nplot(rh_ward, labels = FALSE)\nrect.hclust(rh_ward, k = 3)\n\n\nOn peut tracer les dendrogrammes (Figure 3) pour essayer de voir combien de groupes sont raisonnables. Les plus proches voisins (liaison simple) et le barycentre retournent deux groupes, les autres critères trois et en l’apparence tous de taille similaire (à l’exclusion de la distance médiane qui donne une segmentation inacceptable avec plusieurs observations isolées).\n\n\n\n\n\nFigure 3: Dendrogrammes des regroupements hiérarchiques selon les mesures de distance et de liaison entre groupes (plus proches voisins, voisins les plus éloignées, distances moyenne et médiane, distance entre barycentres, critère de Ward, distance d’énergie et GENIE).\n\n\n\n\nUne fois qu’on a sélectionné le nombre de groupes, on utilise cutree pour élaguer l’arbre et obtenir la segmentation. La procédure retourne le vecteur avec les étiquettes.\nPuisque les étiquettes peuvent être permutées, on peut calculer l’indice de Rand pour voir la concordance entre les regroupements en fournissant à la fonction flexclust::randIndex() les deux vecteurs d’étiquettes.\nLes regroupements obtenus avec les \\(K\\)-moyennes coïncident à 95% pour la distance énergie, 93% pour la méthode de Ward et 93% pour GENIE. Le résultat est cohérent.\n\n\nCode\nmoy_ward &lt;- cbind(regroupements1, \n                  etiquette = cutree(rh_ward, k = 3)) |&gt;\n  dplyr::group_by(etiquette) |&gt;\n  dplyr::summarize_all(.funs = mean)\n# Initialisation avec regroupements pour K-moyennes\n# kmeans(x = regroupements1, centers = moy_ward[,-1])\n\n\nAvec la méthode de Ward, les fonctions objectives sont similaires et il serait envisageable, seulement si la taille de l’échantillon est petite, d’utiliser la solution de Ward pour initialiser les \\(K\\)-moyennes. Ces dernières permettent une réassignation et ne peuvent qu’améliorer le critère objectif.\nOn pourrait aussi envisager d’autres mesures de dissemblance, ici avec la distance de Manhattan, ce qui donne les \\(K\\)-médianes.\n\n\nCode\nlibrary(flexclust, quietly = TRUE)\nkmedianes &lt;- kcca(x = regroupements1,\n                  family = kccaFamily(\"kmedians\"),\n                  control = list(initcent = \"kmeanspp\"),\n                  k = 3)\nkmedianes@clusinfo # décompte par groupe\n\n\n  size  av_dist max_dist separation\n1   31 3.416129     5.60        4.5\n2   73 3.293151     5.10        4.8\n3   46 3.223913     5.95        4.4\n\n\nCode\nkmedianes@centers  # médianes des groupes\n\n\n      x1  x2  x3  x4   x5  x6\n[1,] 4.6 3.0 2.9 3.1 3.20 2.6\n[2,] 3.1 4.1 3.9 3.9 4.20 4.4\n[3,] 1.7 2.0 1.5 2.2 1.95 1.4\n\n\nEncore une fois, les regroupements sont à 91% les mêmes que pour les \\(K\\)-moyennes et l’interprétation est la même.\n\n\nCode\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2, \n                     col = factor(kmedianes@cluster))) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\",\n       col = \"regroupement\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4: Représentation des regroupements des \\(K\\)-médianes projetés sur les deux premières composantes principales des données.\n\n\n\n\nOn pourrait aussi utiliser la méthode des \\(K\\)-médoïdes.\n\n\nCode\nlibrary(cluster)\nkmedoides &lt;- pam(regroupements1, k = 3)\n# plot(kmedoides)\nsilhouette &lt;- silhouette(x = kmedoides, dist = dist_euc)\nplot(silhouette)\n\n\n\n\n\nCode\nkmedoides$medoids\n\n\n     x1  x2  x3  x4  x5  x6\n36  2.9 3.7 3.9 3.6 4.0 4.5\n87  4.2 2.9 3.0 3.1 2.5 2.4\n142 1.7 1.9 1.6 2.3 1.3 1.1\n\n\nL’algorithme de partition autour des médoïdes (PAM) retourne un résultat similaire aux \\(K\\)-moyennes si on fixe le nombre de rgroupements à trois. La méthode plot par défaut retourne une projection sur les deux composantes principales. Les silhouettes montrent des profils globalement homogènes, sauf pour deux observations du groupe 2.\nLa plupart des méthodes employées retournent, peu ou prou, le même résultat que les \\(K\\)-moyennes. Ces données simulées sont un cas d’école; en pratique, il faudra davantage se fier à son jugement pratique pour voir si certains regroupements sont fortement débalancés avec plusieurs données isolées, difficilement interprétables ou voire trop similaires.\nSi la solutions des \\(K\\)-moyennes et cie est satisfaisante, l’assignation probabiliste plutôt que rigide pourrait être souhaitable. Les mélanges de modèles Gaussiens offrent cette option. Ici, on ajuste tous les modèles avec \\(K=1, \\ldots, 10\\) regroupements pour toutes les structures de covariance et on choisit le modèle avec le plus petit BIC.\n\n\nCode\nlibrary(mclust)\n\n\nPackage 'mclust' version 6.0.0\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\nCode\nmmg &lt;- Mclust(regroupements1, G = 1:10)\nsummary(mmg)\n\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust EII (spherical, equal volume) model with 3 components: \n\n log-likelihood   n df       BIC       ICL\n      -1092.714 150 21 -2290.651 -2293.147\n\nClustering table:\n 1  2  3 \n77 28 45 \n\n\nCode\n# Critère BIC selon le nombre de groupes \n# et la structure de covariance\nplot(mmg, what = \"BIC\")\n\n\n\n\n\nOn peut clairement voir que trois regroupements est le nombre idéal. Les modèles plus complexes à coefficients variables sont pénalisés davantage et difficilement estimables vu la taille du jeu de données.\nOn voit clairement les points en bordure de l’espace qui pourraient réalistement être assignés à l’un ou l’autre des regroupements.\n\n\nCode\n## Nuages de points 2x2\nplot(mmg, what = \"uncertainty\")\n\n\n\n\n\nLes points plus larges sont ceux qui sont plus incertains. Ce degré d’incertitude vaut un moins la probabilité d’appartenir à la classe assignée. On peut extraire de la sortie les probabilités de chaque classe en bonus.\n\n\nCode\nmmg$uncertainty # très faible ici, regroupements bien déterminés\n# Probabilité de chaque observation par classe\nmmg$z\n\n\nLes regroupements sont exactement les mêmes que ceux obtenus avec les \\(K\\)-moyennes.\n\n\nCode\n# Indice de Rand - adéquation entre partitions\nflexclust::randIndex(x = mmg$classification, #nos regroupements\n                     y = kmoy[[3]]$cluster)\n\n\nARI \n  1"
  },
  {
    "objectID": "exercices/06-solution.html#exercice-6.1",
    "href": "exercices/06-solution.html#exercice-6.1",
    "title": "Analyse de regroupements",
    "section": "",
    "text": "Les données fictives regroupement1 sont inspirées de Hsu & Lee (2002). Ces données contiennent des échelles pour certains éléments d’un questionnaire. Ce dernier a été élaboré afin d’évaluer l’importance de 55 caractéristiques des opérateurs de voyages organisés en autobus et des voyages eux-mêmes à l’aide d’une échelle de Likert à cinq points, allant de extrêmement important (5) à pas du tout important (1).\nLes variables représentent les activités sociales, les politiques de l’opérateur et références, la flexibilité des horaires, la santé et sécurité, le matériel publicitaire et la réputation.\n\nDoit-on standardiser les données avant d’effectuer l’analyse?\n\n\n\nCode\ndata(regroupements1, package = \"hecmulti\")\nstr(regroupements1)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   150 obs. of  6 variables:\n $ x1: num  3.9 2.8 3.6 2.8 2.5 2.5 2.5 2.2 3.1 3.1 ...\n $ x2: num  4.1 4.6 5 5 3.9 3.3 4.3 5 3.6 5 ...\n $ x3: num  3.6 5 4.3 4.4 3.6 3.6 2.6 2.5 4.5 3.7 ...\n $ x4: num  4.4 4.6 3.6 3.7 4.2 2.6 3.9 3.9 3.9 2.6 ...\n $ x5: num  4 4.5 3.5 3.2 3.2 3.9 4.2 5 4.9 5 ...\n $ x6: num  5 4 4.4 3.5 4 4.5 5 4.9 5 3.9 ...\n\n\nCode\nsummary(regroupements1)\n\n\n       x1              x2              x3              x4       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.100   1st Qu.:2.400   1st Qu.:1.925   1st Qu.:2.525  \n Median :2.800   Median :3.200   Median :3.000   Median :3.150  \n Mean   :2.933   Mean   :3.218   Mean   :2.957   Mean   :3.197  \n 3rd Qu.:3.775   3rd Qu.:4.100   3rd Qu.:3.900   3rd Qu.:3.900  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n       x5              x6       \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.400   1st Qu.:1.925  \n Median :3.450   Median :3.400  \n Mean   :3.321   Mean   :3.170  \n 3rd Qu.:4.200   3rd Qu.:4.400  \n Max.   :5.000   Max.   :5.000  \n\n\nLes données sont toutes sur la même échelle de Likert donc la standardisation n’est pas nécessaire apriori.\n\nFaites une analyse en composantes principales et projetez les observations sur un nuage de points avec les deux premières composantes principales.\n\n\n\nCode\nacp &lt;- princomp(regroupements1, cor = FALSE)\nhecmulti::eboulis(acp)\n\n\n\n\n\nCode\n# 2 à 3 composantes principales suffisantes\nlibrary(ggplot2)\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2)) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\") +\n  theme_classic()\n\n\n\n\n\nCode\n# Chargements\nacp$loadings\n\n\n\nLoadings:\n   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6\nx1  0.247  0.918  0.202                0.229\nx2  0.410        -0.498  0.758              \nx3  0.459         0.187         0.616 -0.609\nx4  0.343  0.140 -0.505 -0.568 -0.440 -0.304\nx5  0.410 -0.213  0.648  0.119 -0.588       \nx6  0.525 -0.300        -0.292  0.266  0.690\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.167  0.167  0.167  0.167  0.167  0.167\nCumulative Var  0.167  0.333  0.500  0.667  0.833  1.000\n\n\nOn voit plus ou moins clairement trois regroupements. Les chargements révèlent que la première composante principale est grosso modo une moyenne globale des scores, mais que le score pour les activités sociales ressort davantage dans la deuxième composante, comparativement au matériel publicitaire et à la réputation.\n\nUtilisez l’algorithme des \\(K\\)-moyennes en faisant varier le nombre de groupes de 1 à 10. Utilisez une dizaine d’initialisations aléatoires.\n\nSélectionnez un nombre de regroupement adéquat\nRetournez le nombre d’observation par groupe pour la valeur de \\(K\\) choisie.\nRapportez les statistiques descriptives (moyennes, etc.) de chaque segment\nInterprétez les profils obtenus.\n\n\n\n\nCode\nset.seed(60602)\nKmax &lt;- 10L # nombre maximum de groupes\nkmoy &lt;- list() # liste pour stocker les résultats des segmentations\nfor(k in seq_len(Kmax)){\n kmoy[[k]] &lt;- kmeans(x = regroupements1, \n                     centers = k,  # nombre de regroupements initiaux\n                     nstart = 10L) # nombre d'initialisation aléatoires\n}\n\n\nOn peut extraire le critère de la fonction objective (SCD): l’homogénéité est calculée comme la somme des carrés des distances intra-groupes. Cela nous permet d’utiliser les diagnostics pour la sélection avec le \\(R^2\\) en cherchant le coude à partir duquel aucun changement n’est visible.\n\n\nCode\nscd_intra &lt;- sapply(kmoy, function(x){x$tot.withinss})\n# Diagnostics pour sélection - clairement trois groupes\nhecmulti::homogeneite(scd = scd_intra)\n\n\n\n\n\nLes graphiques de \\(R^2\\) et le \\(R^2\\) semi-partiel indiquent tous deux clairement que trois regroupements est le choix le plus logique. On peut étiqueter les observations et visualiser les regroupements obtenus en projetant sur les deux premières composantes principales. Les groupes sont bien démarqués et il semble que l’initialisation aléatoire ait été adéquate.\n\n\nCode\n# Extraire les identifiants des regroupements\netiquettes_kmoy3 &lt;- kmoy[[3]]$cluster\n# Faire un graphique avec les composantes principales\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2, \n                     col = factor(etiquettes_kmoy3))) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\",\n       col = \"regroupement\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 1: Représentation des regroupements des \\(K\\)-moyennes projetés sur les deux premières composantes principales des données.\n\n\n\n\nLe nombre peut être extrait de la liste avec $size. On a 45 observations dans le groupe 1, 28 dans le groupe 2 et 77 dans le groupe 3. Au vu de la taille de la base de donnée, ces nombres sont relativement équilibrés.\nOn peut maintenant extraire les moyennes des trois regroupements. Le Tableau 1 montre le résultat arrondi à un chiffre après la virgule.\n\n\nCode\nknitr::kable(kmoy[[3]]$centers, digits = 1)\n\n\n\n\nTableau 1: Moyennes des trois regroupements obtenus avec les \\(K\\)-moyennes.\n\n\nx1\nx2\nx3\nx4\nx5\nx6\n\n\n\n\n1.7\n2.0\n1.6\n2.1\n2.1\n1.6\n\n\n4.5\n2.9\n2.6\n3.1\n3.0\n2.6\n\n\n3.0\n4.0\n3.9\n3.9\n4.2\n4.3\n\n\n\n\n\n\nLe groupe 1 est le groupe où les sujets ont les valeurs, en moyenne, les plus faibles pour les six variables. Le groupe 2 est celui où les sujets ont les valeurs, en moyenne, les plus élevées pour les 6 variables sauf pour la variable X1 (activité sociale). Le groupe 3 est celui où les sujets ont, en moyenne, la valeur la plus élevée de la variable X1 et des valeurs moyennes inférieures au groupe 3 mais supérieures au groupe 2 pour les cinq autres variables.\nDans l’article, les auteurs ont baptisé les sujets du groupe 1, les « indépendants », ceux du groupe 2, les « dépendants » et ceux du groupe 3, les « sociables ».\n\nFaites une segmentation avec d’autres méthodes. Est-ce que la segmentation est plus satisfaisante? Justifiez votre raisonnement.\n\nLa segmentation obtenue est très satisfaisante et interprétable, mais nous essayerons d’autres méthodes pour démontrer leur utilisation.\nLes méthodes basées sur la densité ne fonctionnent pas bien parce que les scores sont relativement rapprochés.\n\n\nCode\n# Regarder les plus proches voisins\ndbscan::kNNdistplot(regroupements1, \n                    minPts = 5)\nabline(h = 1.7, col = \"red\")\n\n\n\n\n\nCode\ndbscan_regroup &lt;- dbscan::dbscan(\n  x = regroupements1,\n  minPts = 5,\n  eps = 1.7)\ndbscan_regroup\n\n\nDBSCAN clustering for 150 objects.\nParameters: eps = 1.7, minPts = 5\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 2 cluster(s) and 6 noise points.\n\n 0  1  2 \n 6 99 45 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\n\nLa solution retournée fusionne deux regroupements et plusieurs valeurs sont traitées comme des points isolés.\n\n\nCode\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2, \n                     col = factor(dbscan_regroup$cluster + 1))) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\",\n       col = \"regroupement\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2: Représentation des regroupements DBSCAN projetés sur les deux premières composantes principales des données.\n\n\n\n\nAvec une petite taille d’échantillons, les méthodes de regroupements hiérarchiques sont compétitives et on a le luxe de pouvoir en essayer plusieurs.\n\n\nCode\ndist_euc &lt;- dist(regroupements1, method = \"euclidean\")\nrh_simple &lt;- fastcluster::hclust(dist_euc, method = \"single\")\nrh_complet &lt;- fastcluster::hclust(dist_euc, method = \"complete\")\nrh_ward &lt;- fastcluster::hclust(dist_euc, method = \"ward.D2\")\nrh_bary &lt;- fastcluster::hclust(dist_euc, method = \"centroid\")\nrh_moy &lt;- fastcluster::hclust(dist_euc, method = \"average\")\nrh_med &lt;- fastcluster::hclust(dist_euc, method = \"median\")\nrh_genie &lt;- genieclust::gclust(d = regroupements1)\nrh_energie &lt;- energy::energy.hclust(d = dist_euc)\n\n\nPour visualiser le dendrogramme, la méthode plot permet de tracer l’arborescence. Il suffit ensuite d’encadrer avec rect.hclust() en spécifiant le nombre de groupes. Par exemple,\n\n\nCode\nplot(rh_ward, labels = FALSE)\nrect.hclust(rh_ward, k = 3)\n\n\nOn peut tracer les dendrogrammes (Figure 3) pour essayer de voir combien de groupes sont raisonnables. Les plus proches voisins (liaison simple) et le barycentre retournent deux groupes, les autres critères trois et en l’apparence tous de taille similaire (à l’exclusion de la distance médiane qui donne une segmentation inacceptable avec plusieurs observations isolées).\n\n\n\n\n\nFigure 3: Dendrogrammes des regroupements hiérarchiques selon les mesures de distance et de liaison entre groupes (plus proches voisins, voisins les plus éloignées, distances moyenne et médiane, distance entre barycentres, critère de Ward, distance d’énergie et GENIE).\n\n\n\n\nUne fois qu’on a sélectionné le nombre de groupes, on utilise cutree pour élaguer l’arbre et obtenir la segmentation. La procédure retourne le vecteur avec les étiquettes.\nPuisque les étiquettes peuvent être permutées, on peut calculer l’indice de Rand pour voir la concordance entre les regroupements en fournissant à la fonction flexclust::randIndex() les deux vecteurs d’étiquettes.\nLes regroupements obtenus avec les \\(K\\)-moyennes coïncident à 95% pour la distance énergie, 93% pour la méthode de Ward et 93% pour GENIE. Le résultat est cohérent.\n\n\nCode\nmoy_ward &lt;- cbind(regroupements1, \n                  etiquette = cutree(rh_ward, k = 3)) |&gt;\n  dplyr::group_by(etiquette) |&gt;\n  dplyr::summarize_all(.funs = mean)\n# Initialisation avec regroupements pour K-moyennes\n# kmeans(x = regroupements1, centers = moy_ward[,-1])\n\n\nAvec la méthode de Ward, les fonctions objectives sont similaires et il serait envisageable, seulement si la taille de l’échantillon est petite, d’utiliser la solution de Ward pour initialiser les \\(K\\)-moyennes. Ces dernières permettent une réassignation et ne peuvent qu’améliorer le critère objectif.\nOn pourrait aussi envisager d’autres mesures de dissemblance, ici avec la distance de Manhattan, ce qui donne les \\(K\\)-médianes.\n\n\nCode\nlibrary(flexclust, quietly = TRUE)\nkmedianes &lt;- kcca(x = regroupements1,\n                  family = kccaFamily(\"kmedians\"),\n                  control = list(initcent = \"kmeanspp\"),\n                  k = 3)\nkmedianes@clusinfo # décompte par groupe\n\n\n  size  av_dist max_dist separation\n1   31 3.416129     5.60        4.5\n2   73 3.293151     5.10        4.8\n3   46 3.223913     5.95        4.4\n\n\nCode\nkmedianes@centers  # médianes des groupes\n\n\n      x1  x2  x3  x4   x5  x6\n[1,] 4.6 3.0 2.9 3.1 3.20 2.6\n[2,] 3.1 4.1 3.9 3.9 4.20 4.4\n[3,] 1.7 2.0 1.5 2.2 1.95 1.4\n\n\nEncore une fois, les regroupements sont à 91% les mêmes que pour les \\(K\\)-moyennes et l’interprétation est la même.\n\n\nCode\nggplot(data = data.frame(acp$scores[,1:2]),\n       mapping = aes(x = Comp.1,\n                     y = Comp.2, \n                     col = factor(kmedianes@cluster))) +\n  geom_point() +\n  labs(x = \"composante principale 1\",\n       y = \"composante principale 2\",\n       col = \"regroupement\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4: Représentation des regroupements des \\(K\\)-médianes projetés sur les deux premières composantes principales des données.\n\n\n\n\nOn pourrait aussi utiliser la méthode des \\(K\\)-médoïdes.\n\n\nCode\nlibrary(cluster)\nkmedoides &lt;- pam(regroupements1, k = 3)\n# plot(kmedoides)\nsilhouette &lt;- silhouette(x = kmedoides, dist = dist_euc)\nplot(silhouette)\n\n\n\n\n\nCode\nkmedoides$medoids\n\n\n     x1  x2  x3  x4  x5  x6\n36  2.9 3.7 3.9 3.6 4.0 4.5\n87  4.2 2.9 3.0 3.1 2.5 2.4\n142 1.7 1.9 1.6 2.3 1.3 1.1\n\n\nL’algorithme de partition autour des médoïdes (PAM) retourne un résultat similaire aux \\(K\\)-moyennes si on fixe le nombre de rgroupements à trois. La méthode plot par défaut retourne une projection sur les deux composantes principales. Les silhouettes montrent des profils globalement homogènes, sauf pour deux observations du groupe 2.\nLa plupart des méthodes employées retournent, peu ou prou, le même résultat que les \\(K\\)-moyennes. Ces données simulées sont un cas d’école; en pratique, il faudra davantage se fier à son jugement pratique pour voir si certains regroupements sont fortement débalancés avec plusieurs données isolées, difficilement interprétables ou voire trop similaires.\nSi la solutions des \\(K\\)-moyennes et cie est satisfaisante, l’assignation probabiliste plutôt que rigide pourrait être souhaitable. Les mélanges de modèles Gaussiens offrent cette option. Ici, on ajuste tous les modèles avec \\(K=1, \\ldots, 10\\) regroupements pour toutes les structures de covariance et on choisit le modèle avec le plus petit BIC.\n\n\nCode\nlibrary(mclust)\n\n\nPackage 'mclust' version 6.0.0\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\nCode\nmmg &lt;- Mclust(regroupements1, G = 1:10)\nsummary(mmg)\n\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust EII (spherical, equal volume) model with 3 components: \n\n log-likelihood   n df       BIC       ICL\n      -1092.714 150 21 -2290.651 -2293.147\n\nClustering table:\n 1  2  3 \n77 28 45 \n\n\nCode\n# Critère BIC selon le nombre de groupes \n# et la structure de covariance\nplot(mmg, what = \"BIC\")\n\n\n\n\n\nOn peut clairement voir que trois regroupements est le nombre idéal. Les modèles plus complexes à coefficients variables sont pénalisés davantage et difficilement estimables vu la taille du jeu de données.\nOn voit clairement les points en bordure de l’espace qui pourraient réalistement être assignés à l’un ou l’autre des regroupements.\n\n\nCode\n## Nuages de points 2x2\nplot(mmg, what = \"uncertainty\")\n\n\n\n\n\nLes points plus larges sont ceux qui sont plus incertains. Ce degré d’incertitude vaut un moins la probabilité d’appartenir à la classe assignée. On peut extraire de la sortie les probabilités de chaque classe en bonus.\n\n\nCode\nmmg$uncertainty # très faible ici, regroupements bien déterminés\n# Probabilité de chaque observation par classe\nmmg$z\n\n\nLes regroupements sont exactement les mêmes que ceux obtenus avec les \\(K\\)-moyennes.\n\n\nCode\n# Indice de Rand - adéquation entre partitions\nflexclust::randIndex(x = mmg$classification, #nos regroupements\n                     y = kmoy[[3]]$cluster)\n\n\nARI \n  1"
  },
  {
    "objectID": "exercices/installation.html",
    "href": "exercices/installation.html",
    "title": "Installer R et RStudio",
    "section": "",
    "text": "Nous utiliserons le langage de programmation libre-accès R et l’environnement de développement intégré RStudio comme porte d’accès à R.\n\nInstaller R\nPour commencer, on installe la dernière version de R (la machine qui fait les calculs), actuellement 4.3.2 (Eye Holes).\n\nAllez sur le site du Comprehensive R Archive Network (CRAN): https://cran.r-project.org/\nCliquez sur “Download R for XXX”, où XXX est Mac ou Windows:\n\n\n\n\n\n\nSi vous utilisez macOS, faites défiler le menu jusqu’au premier fichier .pkg de la liste et téléchargez-le.\n\n\n\n\n\nSi vous utilisez Windows, choisissez “base” (ou cliquez sur l’hyperlien en gras “install R for the first time”) et téléchargez le programme.\n\n\n\n\n\n\nDouble cliquez sur le fichier de téléchargement. Approuvez toutes les requêtes, comme pour n’importe quel autre programme.\nSi vous utilisez macOS, téléchargez et installez XQuartz. Cette étape n’est pas nécessaire sur Windows.\n\n\n\nInstaller RStudio\nUne fois R installée, nous installerons une interface utilisateur graphique, RStudio, pour interagir avec R.\n\nNaviguez jusqu’à la section de téléchargements gratuits du site de RStudio: https://www.rstudio.com/products/rstudio/download/#download\nLe site internet devrait automatiquement détecter votre système d’exploitation (Linux, macOS ou Windows). Cliquez sur le bouton pour lancer le téléchargement:\n\n\n\n\n\n\nSinon, naviguez vers le tableau et choisissez la version de RStudion adéquate.\n\n\n\n\n\n\nDouble-cliquez sur le fichier de téléchargement (potentiellement caché dans votre dossier de Téléchargements). Installez comme n’importe lequel autre logiciel.\n\nDouble-cliquez sur l’icône RStudio pour lancer l’application.\n\n\nInstaller tidyverse\nLes paquets R sont faciles à installer avec l’interface graphique. Sélectionnez le panneau “packages”, cliquez sur “Install,” tapez le nom du paquet que vous voulez installer et appuyer sur la touche Retour.\n\n\n\n\n\nLe paquet tidyverse est une collection d’une douzaine de paquets (incluant ggplot2 et dplyr) qui fonctionnent ensemble selon une syntaxe commune. L’installer (ou charger le paquet) inclura automatiquement toutes les dépendances associées\n\n\n\n\n\nNotez que RStudio génère le code R pour l’installation: install.packages(\"tidyverse\"). Vous pourriez utiliser directement cette fonction et faire copier-coller dans la console pour installer les paquets du tidyverse.\n\n\nInstaller tinytex\nQuand vous créez un fichier Rmarkdown (.Rmd) ou Quarto (.qmd) pour créer un document reproductible qui inclut calculs et texte et que vous tricotez ce dernier en PDF, R utilise le programme de typographie scientifique LaTeX. L’installation la plus aisée pour ce dernier, si votre machine n’a pas déjà de suite LaTeX, est d’installer le paquet tinytex pour obtenir une version minimale qui prend moins d’espace.\nPour installer tinytex:\n\nUtilisez l’onglet Packages dans le panneau RStudio pour installer tinytex, comme n’importe quel autre paquet. Sinon, tapez install.packages(\"tinytex\") dans la console.\nExécutez tinytex::install_tinytex() dans la console.\nAttendez que le téléchargement soit complété. Vous devriez ensuite être en mesure de tricoter des documents PDF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse multidimensionnelle appliquée",
    "section": "",
    "text": "Analyse multidimensionnelle appliquée\n        \n        \n            Formation de base en traitement de données multidimensionnelles. Compréhension intuitive, interprétation et utilisation de plusieurs techniques statistiques à l’aide de logiciels appropriés.\n        \n        \n            MATH 60602, automne 2023HEC Montréal\n        \n    \n\n\n\n\n\nEnseignant\n\n   Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\nDétails du cours\n\n   automne 2023\n   jeudi (J01) et mercredi (S01)\n   12h à 15h (J01) et 18h45 à 21h45 (S01)\n   Decelles, Victoriaville (S01) et Forestville (J01)\n\n\n\nContact\nMerci de publier toute question relative au contenu du cours sur le forum Piazza. Pour toute autre question, je suis plus facilement joignable par courriel."
  }
]