---
title: "Régression logistique"
execute:
  echo: true
  eval: true
  message: false
  warning: false
  error: false
---


## Exercice 4.1

*Les données `logistclient` contiennent des données simulées pour un cas fictif de promotion pour des clients.*

1. *Estimez le modèle logistique pour la probabilité que `promo=1` avec les variables explicatives `nachats`, `sexe` et `tclient`.*

```{r}
#| eval: true
#| echo: true
library(hecmulti)
data(logistclient, package = "hecmulti")
# Modèle logistique
mod <- glm(promo ~ nachats + sexe + tclient, 
           family = binomial(link = "logit"),
           data = logistclient)
# Coefficients du modèle et statistique de Wald
summary(mod)
```



2. *Interprétez les coefficients du modèle à l'échelle de la cote en terme de pourcentage d'augmentation ou de diminution.*

```{r}
exp(coef(mod)) # Rapport de cote
```

- La cote pour l'offre promotionnelle (oui versus non) des hommes est 23.5% plus faible que celle des femmes, *ceteris paribus*
- Le rapport de cotes pour `tclient` fréquent/occasionnel est 0.934: les clients fréquents ont une cote 6.6% inférieure à celle des clients occasionnels, toute chose étant égale par ailleurs. De manière équivalente, la cote des clients occasionnels est 7.11% supérieure à celle des clients fréquents, *ceteris paribus*.
- La cote de `nachats` augmente de 23.1% pour chaque augmentation du nombre d'achats dans le dernier mois,  *ceteris paribus*
 

3. *Testez si l'effet de `nachats` est statistiquement significatif à niveau $\alpha = 0.05$.*

- L'intervalle de confiance à 95% pour le rapport de cote de `nachats`, basé sur la vraisemblance profilée, est de $[1.15, 1.32]$; comme 1 est exclu, cette différence est statistiquement significative. 

On obtiendrait la même conclusion avec la statistique du test de rapport de vraisemblance, ici $37.237$ pour 1 degré de liberté. La probabilité, si $\beta_{\text{nachats}}=0$, d'obtenir une telle différence d'ajustement est inférieure à $10^{-4}$, bien en deça du seuil de significativité. On rejette l'hypothèse nulle et on conclut que le nombre d'achat est important pour expliquer si une personne s'est prévalue de l'offre promotionnelle.

```{r}
exp(confint(mod)) #IC pour rapport de cote
car::Anova(mod, type = 3) # tests de rapport de vraisemblance
```


4. *Choisissez un point de coupure pour la classification pour maximiser le taux de bonne classification.* 
   i. *Pour le point de coupure choisi, construisez une matrice de confusion.*
   ii. *Calculez la sensibilité, la spécificité et le taux de bonne classification manuellement. Vérifiez vos réponses avec la sortie du tableau.*
   
```{r}
#| eval: true
#| echo: true
#| cache: true
#| message: false
#| warning: false
set.seed(60602)
pred <- hecmulti::predvc(mod)
resp <- logistclient$promo
```


On prend le modèle ajusté avec `glm` et on calcule la prédiction à l'aide de la validation croisée à 10 groupes, répétée 10 fois. La fonction `predvc` retourne la moyenne des prédictions (ici, des probabilités) pour chacune des 1000 observations.

```{r}
#| eval: true
#| echo: true

library(ggplot2)
tableau <- perfo_logistique(prob = pred, 
                             resp = resp)
# Graphique du taux de bonne classification
# selon le point de coupure
ggplot(data = tableau, 
       aes(x = coupe, y = pcorrect)) + 
  geom_line() + 
  theme_classic() + 
  scale_y_continuous(limits = c(0, 100),
                     expand = c(0,0)) + 
  labs(x = "point de coupure",
       y = "",
       subtitle = "Taux de bonne classification")
opt <- which.max(tableau$pcorrect)
knitr::kable(tableau[opt,])
```


Ensuite, il suffit de passer les valeurs de la variable réponse et nos probabilités de succès prédites aux différentes fonctions.
   
Si on considère des points de coupure de 0.01 à 0.99 en incréments de 0.01, on obtient un point de coupure optimal à
`r tableau[opt,"coupe"]`. On note que le taux de bonne classification change assez peu au final.

```{r}
#| label: tbl-confusion
#| eval: true
#| echo: false
#| tbl-cap: "Matrice de confusion avec point de coupure optimal"
confumat <- with(
  tableau[opt,],
  matrix(c(VP, FN, FP, VN), nrow = 2, ncol = 2))
rownames(confumat) <- c("$\\widehat{Y}=1$","$\\widehat{Y}=0$")
colnames(confumat) <- c("$Y=1$","$Y=0$")
knitr::kable(x = confumat,
                align = "r",
                escape = FALSE,
                booktabs = TRUE)
```
Ainsi, si on fait les calculs à la main, on estime 

- la spécificité $\mathsf{VN}/(\mathsf{VN} + \mathsf{FN})$, soit `r tableau[opt,"VN"]` / (`r tableau[opt,"VN"]` + `r tableau[opt,"FN"]`) ou `r round(tableau[opt,"VN"] / (tableau[opt,"VN"] + tableau[opt,"FN"]), 3)`.
- la sensibilité $\mathsf{VP}/(\mathsf{VP} + \mathsf{FP})$, soit `r tableau[opt,"VP"]` / (`r tableau[opt,"VP"]` + `r tableau[opt,"FP"]`) ou `r round(tableau[opt,"VP"] / (tableau[opt,"VP"] + tableau[opt,"FP"]), 3)`.
- le taux de bonne classification $\mathsf{VN} + \mathsf{VP}/(\mathsf{VN} + \mathsf{VP} + \mathsf{FN} + \mathsf{FP})$, soit (`r tableau[opt,"VP"]` + `r tableau[opt,"VN"]`) / 1000 ou `r (tableau[opt,"VP"] + tableau[opt,"VN"])/1000`.

Ces valeurs coincident, à arrondi près, avec ce qui est reporté dans le tableau.


5. *Produisez un graphique de la fonction d’efficacité du récepteur (courbe ROC) et rapportez l'aire sous la
courbe estimée à l’aide de la validation croisée.*

```{r}
roc <- courbe_roc(prob = pred, resp = resp)
```

On obtient une estimation de l'aire sous la courbe de `r round(roc$aire, 3)`.

6. *Calculez la statistique de Spiegelhalter (1986) pour la calibration du modèle. Y a-t-il des preuves de surajustement?*

```{r}
calibration(prob = pred, resp = resp)
```

L'hypothèse nulle est que le modèle est calibré; ici, la valeur-$p$ est près de 0.5, donc on ne rejette pas l'hypothèse nulle et on conclut qu'il n'y a pas de preuve de surajustement.

