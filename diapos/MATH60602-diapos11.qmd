---
title: "Analyse de regroupements"
subtitle: "Analyse multidimensionnelle appliquée"
date: ""
author: "Léo Belzile"
institute: "HEC Montréal"
format: beamer
navigation: empty
colortheme: Flip
innertheme: Flip
outertheme: Flip
themeoptions: "bullet=circle, topline=true, shadow=false"
beamerarticle: false
pdf-engine: lualatex
code-line-numbers: true
fig-align: 'center'
mainfont: "VisbyCF-Medium"
mathfont: 'Latin Modern Math'
sansfont: 'Latin Modern Sans'
keep-tex: true
include-in-header: 
      text: |
        \usepackage{tabu}
        \usepackage{mathtools}
        \usepackage{mathrsfs}
---

# Analyse de regroupements


```{r}
#| label: setup
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
library(knitr)
library(kableExtra)
set.seed(1014)
library(hecmulti)
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  out.width = "80%",
  fig.align = 'center',
  fig.width = 8.5,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.table.format = function() {
  if (knitr::is_latex_output()) 
    "latex" else "html"
})

options(dplyr.print_min = 6, dplyr.print_max = 6)
options(knitr.graphics.auto_pdf = TRUE)
options(scipen = 1, digits = 3)
library(viridis)
library(ggplot2, warn.conflicts = FALSE, quietly = TRUE)
library(poorman, quietly = TRUE, warn.conflicts = FALSE)
library(patchwork)

safe_colorblind_palette <- MetBrewer::met.brewer("Hiroshige",10)

options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill = "viridis")
scale_colour_discrete <- scale_color_manual(MetBrewer::met.brewer("Hiroshige",10))
scale_fill_discrete <- scale_fill_manual(MetBrewer::met.brewer("Hiroshige",10))
theme_set(theme_classic())
```

**Objectif**: regrouper des **observations** de telle sorte que 

- les observations d'un même groupe soient le plus semblables possible,
- les groupes soient le plus différent possible les uns des autres.

Chaque observation se voit assigner une étiquette de groupe.

On procède ensuite à une analyse **descriptive**, segment par segment.


# Analogie avec analyse factorielle

En analyse factorielle, on combine des variables similaires.

Pour l'analyse de regroupements, on regroupe des observations.

```{r}
include_graphics(path = "figures/tidy-data-sub.png")
```

Ce sont des méthodes dites d'**apprentissage non-supervisé**: l'objectif est de déduire les regroupements sans étiquette préalable (contrairement à la classification).


# Étapes d'une analyse de regroupements

1. Choisir les variables pertinentes à l'analyse. Cette étape peut nécessiter de créer, transformer de nouvelles variables ou d'aggréger les données.
2. Choisir un algorithme et ses hyperparamètres (dissemblance, nombre de regroupements, etc.)
3. Regrouper et étiqueter les observations.
4. Valider la qualité de la segmentation.
5. Obtenir le prototype de chaque groupe et interpréter les regroupements à l'aide de ces derniers.

# Exemples d'analyses de regroupements

- Programmes de fidélisation et résolution d'entités
- Segmentation de la clientèle de transport en commun et élaboration de forfaits 
- Démarchage d'organismes de charité
- Segmentation de [quartiers de Los Angeles](https://fivethirtyeight.com/features/the-6-political-neighborhoods-of-los-angeles/) et de [New York](https://fivethirtyeight.com/features/the-5-political-boroughs-of-new-york-city/) selon leur vote
- [Profils des électeurs albertains](https://www.cbc.ca/news/canada/calgary/danielle-smith-alberta-moderate-middle-ucp-ndp-poll-1.6651460)
- [Positionnement de joueurs lors de match de la NBA](https://github.com/nedwardsthro/Thesis_Work)


# Illustration


```{r}
#| label: fig-regroupements-bidons
#| fig-cap: "Données simulées avec deux regroupements hypothétiques."
#| echo: false
#| cache: false
set.seed(1234)
dat <- rbind(
  mvtnorm::rmvnorm(n = 50, 
                 mean = c(-10,0), 
                 sigma = rWishart(n = 1, 
                                  df = 5, 
                                  Sigma = diag(0.25,2,2))[,,1]),
  mvtnorm::rmvt(n = 100, 
                sigma = cbind(c(2,-1), c(-1,1.2)),
                df = 3, 
                delta = c(6,8)))
dat <- data.frame(dat)
colnames(dat) <- c("x1", "x2")
ggplot(data = dat, aes(x = x1, y = x2)) + 
  geom_point() + 
  labs(x = "variable 1",
       y = "variable 2") + 
  theme_minimal()
```



# Structure de la base de données

Quelles variables $\mathrm{X}_1, \ldots, \mathrm{X}_p$ sont d'intérêt?

- Choisir des variables pertinentes pour faire ressortir les différences
- Créer de nouvelles variables explicatives


Pour les données longitudinales, on va typiquement aggréger les bases de données marketing par identifiant client.

# Exemple avec transport en commun

La carte Opus enregistre 

- les temps de passage
- le type de déplacement (REM, métro, bus)
- le nombre de passages
- les abonnements
- le profil client (études, rabais pour personnes âgées)

# Quelles variables créer ou conserver?

- Nombre de passages mensuels
- Abonnement mensuel ou annuel (oui/non)
- Type de déplacement (soir, jour)
- Nombre d'allers-retours hebdomadaires en heure de pointe
- Variabilité de la fréquentation


#  À votre tour

Vous avez toutes les données transactionnelles associées à des comptes d'épicerie avec un compte de fidélisation. 


Quelles variables pourriez-vous créer à partir des données aggrégées pour créer des segments?


# Diviser pour régner

Souvent, il existe une division naturelle des données.

Les jeunes avec des abonnements de transport publics l'utilisent principalement pour aller à l'école.

On peut faire la segmentation **séparément** pour ces sous-groupes.

# Choix des variables 

Recommandations: choisir les variables pertinentes qui font ressortir les effets voulus.

- inclure de nombreuses variables similaires dilue les différences.
- transformez les variables pour diminuer la corrélation.

Typiquement, ne pas utiliser les variables sociodémographiques (âge, revenu, sexe, etc.)

- on compare plutôt leur répartition au sein des regroupements.


# Exemple - dons à un organisme de charité

La base de données `dons` contient `r nrow(hecmulti::dons)` observations pour `r ncol(hecmulti::dons)` variables.

Nous avons plusieurs variables:

- la valeur des dons (total, min, max)
- la valeur des promesses, 
- le délai entre les dons (moyen, min, max)
- le nombre de dons, 
- la fréquence, 
- l'ancienneté, etc. 


Une poignée de dons sont très élevés, mais la plupart des montants tourne autour de 5\$, 10\$, 20\$, etc.


# Modification des données



```{r}
#| eval: true
#| echo: true
donsmult <- hecmulti::dons |>
  filter(ndons > 1L) |> #seulement dons multiples
  mutate(mtdons = vdons/ndons, # montant moyen
         # nombre de dons selon l'anciennete
         snrefus = nrefus/anciennete*mean(anciennete),
         mpromesse = case_when(
           npromesse > 0 ~ vpromesse/npromesse, #montant moyen
           TRUE ~ 0)) |> # zero si aucune promesse
  select(!c(
    vradiations, # trop de valeurs manquantes dans vradations
    nindecis, vdons, ddonsmax,
    ddonsmin, vdonsmin, npromesse,
    vpromesse, nrefus, nradiations)) |>
  relocate(mtdons) # mettre mtdons dans la 1e colonne
```

# Corrélation des nouvelles données

```{r}
#| eval: true
#| echo: false
#| out-width: '80%'
corrplot::corrplot(cor(donsmult))
```


# Mesures de dissimilarité et de similarité

Comment mesurer si deux observations appartiennent à un même regroupement et sont similaires?

Une mesure de dissimilarité sert à quantifier la proximité de deux objets à partir de leurs coordoonnées. 

Plus la dissimilarité est élevée, moins les observations sont semblables (plus éloignées).



# Mesures de dissimilarité

Quelques propriétés des mesures de dissimilarité:

1. positivité: la distance entre deux observations est nulle si et seulement si on a les mêmes caractéristiques pour toutes les variables explicatives et strictement positive sinon.
2. la dissimilarité est la même peu importe l'ordre des observations (symmétrie)

Toute distance est une mesure de dissimilarité.

# Distance euclidienne

La mesure de dissimilarité la plus utilisée en pratique est la distance euclidienne.


La distance entre les vecteurs ligne $\mathbf{X}_i$ et $\mathbf{X}_j$ (deux lignes de la base de données) est
\begin{align*}
d(\mathbf{X}_i, \mathbf{X}_j; l_2) = \left\{(X_{i1}-X_{j1})^2 + \cdots + (X_{ip}-X_{jp})^2\right\}^{1/2}.
\end{align*}
C'est tout simplement la longueur du segment qui relie deux points dans l'espace $p$ dimensionnel. 

# Autres mesures de dissimilarité


```{r}
#| out-width: '70%'
knitr::include_graphics("figures/distance.pdf")
```

- La distance de Manhattan est la somme des valeurs absolues entre chaque composante, $|X_{i1}-X_{j1}| + \cdots + |X_{ip}-X_{jp}|$.
- La distance $l_\infty$, soit le maximum des différences entre les coordonnées des vecteurs d'observations $i$ et $j$, $\max_{k=1}^p |X_{ik}-X_{jk}|$



# Autres mesures de dissimilarité

Pour les données catégorielles nominales, on peut assigner une dissimilarité de 0 si les variables ont la même modalité et 1 sinon. 

Pour le cas de variables mixtes, la distance de Gower permet de traiter
les valeurs manquantes et standardise automatiquement.

# dissimilarités dans **R**

Avec notre base de données `donsmult`, le stockage des distances prend environ 750MB!

```{r}
#| eval: false
#| echo: true
# Distance euclidienne, de Manhattan, de Gower
d1 <- dist(donsmult, method = "euclidean")
d2 <- dist(donsmult, method = "minkowski", p = 1)
d3 <- cluster::daisy(donsmult, metric = "gower")
# Voir aussi ?flexclust::dist2
```

\footnotesize 

Les objets de class `dist` ne stockent que la matrice triangulaire inférieure (puisque la distance est symmétrique).

# Au plus fort la poche

Le poids accordé à une variable explicative dépend de son étendu et de sa variabilité.

- Plus la variable est grande, plus elle aura un impact dans le calcul des distances
- Problème de standardisation (résultats différents selon les unités de mesure)

# Standardisation

Généralement, on standardise les données avant l'analyse de regroupements.

- Soustraire la moyenne et diviser par l'écart-type empiriques (fonction `scale` dans **R**)
- ou utiliser des mesures robustes: soustraire la médiane et diviser par l'écart absolu à la médiane (*mad*)


Notez qu'il est illogique de standardiser les variables catégorielles (déclarer obligatoirement les variables binaires en facteurs et traiter à part!)

# Standardisation avec **R**

```{r}
#| eval: true
#| echo: true
# Standardisation usuelle
# (soustraire la moyenne, diviser par écart-type)
donsmult_std <- scale(donsmult)
# Extraire moyenne et écart-type
dm_moy <- attr(donsmult_std, "scaled:center")
dm_std <- attr(donsmult_std, "scaled:scale")
# Standardisation robuste
donsmult_std_rob <- apply(
  donsmult,
  MARGIN = 2,
  FUN = function(x){(x - median(x))/mad(x)})
```


# Être aux abonnés absents

Attention aux valeurs manquantes, rarement supportées par les algorithmes d'analyse de regroupements.

Quelques solutions

- ignorer les variables explicatives avec beaucoup de valeurs manquantes.
- faire une segmentation manuelle si les valeurs manquantes déterminent des regroupements (ex: temps entre dons valide uniquement pour dons multiples).
- imputer les données manquantes (voir chapitre sur les données manquantes)
 
 
# Exemple: typologie des votants en France

```{r}
knitr::include_graphics("figures/typologie-vote-france.jpg")
```


# Algorithmes pour la segmentation

L'analyse de regroupements cherche à créer une division de $n$ observations de $p$ variables en regroupements.

1. méthodes basées sur les centroïdes et les médoïdes ($k$-moyennes, $k$-médoides)
2. mélanges de modèles
3. méthodes basées sur la connectivité (regroupements hiérarchiques agglomératifs et divisifs)
4. méthodes basées sur la densité (pas couvertes)


# Notation pour la complexité

- La notation $\mathrm{O}(\cdot)$ nous renseigne sur l'ordre du nombre d'opérations nécessaires pour le calcul. Par exemple, additionner une colonne de chiffres nécessite $\mathrm{O}(n)$ flops

- De même, le stockage d'une matrice de distance, qui contient $n(n-1)/2$ entrées distinctes, est $\mathrm{O}(n^2)$.

Plus le chiffre (ou la puissance) est élevé, plus le calcul ou le stockage est coûteux. 

# Critères pour sélection

- Complexité: plus un algorithme a une complexité élevée (coût de calcul et quantité de stockage), moins il sera susceptible d'être applicable à des mégadonnées.
- Choix des hyperparamètres: plusieurs paramètres (nombre de groupes, rayon, choix de la dissimilarité, etc.) à spécifier selon les méthodes.


# Méthodes basées sur les centroïdes et les médoïdes 


On assigne chaque observation à un de $K$ regroupements, représentés par un prototype, disons $\boldsymbol{\mu}_k$ pour le regroupement $k$. 


- le nombre $K$ est fixé apriori

On cherche à assigner les observations aux groupes de manière à minimiser la distance entre les observations et les prototypes.

# $K$ moyennes

Probablement la méthode de regroupement la plus populaire en raison de son faible coût (linéaire en $n$ et $p$).

La fonction objective considère la distance totale entre les observations et les prototypes. 

$$
\min_{\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_K}\underset{\text{distance entre obs. $i$ et son prototype} \mu_j}{\sum_{i=1}^n \min_{c_i \in \{1, \ldots, K\}} d(\mathbf{X}_i,  \boldsymbol{\mu}_{c_i})}
$$ {#eq-fobjKmoy}


L'allocation optimale de $n$ observations à $K$ groupes est un problème NP complet: on cherchera plutôt une solution approximative au problème d'optimisation.

# Initialisation des $K$-moyennes

**Initialisation**: on sélectionne préalablement 

- un nombre $K$ de regroupements et 
- les coordonnées de départ pour les prototypes. 


# Algorithme EM

L'algorithme de type EM itère entre deux étapes:

1. **Assignation** (étape E): calculer la distance entre chaque observation et les prototypes; assigner chaque observation au prototype le plus près.
2. **Mise à jour** (étape M): calculer les coordonnées optimales des prototypes de chaque groupe
  (avec la distance Euclidienne, c'est le barycentre des observations du groupe).



L'algorithme termine après un nombre prédéfini d'itérations ou lorsque l'assignation ne change plus (solution locale).

Visionner l'animation [en ligne](https://lbelzile.github.io/math60602/03-regroupements_files/figure-html/fig-kmoy-animation-.gif) ou [ce site](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/).

```{r}
#| eval: false
#| echo: false
#| label: fig-kmoyanimation
#| fig-cap: "Animation de l'algorithme des $K$-moyennes avec $K=3$ regroupements."
# knitr::include_graphics("figures/fig-kmoy-animation.gif")
```


# Forces et faiblesses

Quelques forces ($+$) et faiblesses ($-$) de l'algorithme des $K$ moyennes

- ($+$) Complexité **linéaire** dans la dimension et dans le nombre de variables.
- ($+$) L'algorithme converge rapidement vers une solution locale (garantie théorique).
- ($-$) Regroupements globulaires d'apparence sphérique (distance Euclidienne). 
- ($+$) Pour les prédictions, on peut assigner les nouvelles observations au barycentre le plus près.

# Performance

- ($-$) Chaque observation est assignée à un seul des $K$ regroupements (partition rigide). 
- ($-$) Valeurs aberrantes pas étiquetées à part (manque de robustesse pour moyenne).
- ($-$) Sensible aux valeurs initiales des prototypes.
- ($-$) Les prototypes ne correspondent pas à des observations du groupe.

# Performance des $K$-moyennes

Illustration de segmentations problématiques avec $K$-moyennes


```{r}
#| label: kmoyperfo
#| echo: false
#| cache: true
#| message: false
#| fig-width: 8
#| fig-height: 8
#| out-width: '80%'
set.seed(1234)
#https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html
# Figure 9.1 in Bishop
library(poorman)
library(ggplot2)
x1 <- rbind(mvtnorm::rmvnorm(n=200, mean = c(-10,-5), sigma = diag(2)),
            mvtnorm::rmvnorm(n=200, mean = c(-2.5,0), sigma = diag(2)),
            mvtnorm::rmvnorm(n=200, mean = c(2.5,0), sigma = diag(2)))
x2 <- rbind(mvtnorm::rmvnorm(n=200, mean = c(-3,3), sigma = cbind(c(1,1.1), c(1.1,1.5))),
            mvtnorm::rmvnorm(n=200, mean = c(-2,0.5), sigma = cbind(c(1,1.1), c(1.1,1.5))),
            mvtnorm::rmvnorm(n=200, mean = c(1,1), sigma = cbind(c(1,1.1), c(1.1,1.5))))
set.seed(1234)
x3 <- rbind(mvtnorm::rmvnorm(n=200, mean = c(-3,2.5), sigma = diag(rep(3,2L))),
            mvtnorm::rmvnorm(n=200, mean = c(-3,-1.5), sigma = diag(rep(0.25,2L))),
            mvtnorm::rmvnorm(n=200, mean = c(1.5,4), sigma = diag(rep(0.5,2L))))
df1 <- data.frame(x = x1[,1], 
                  y = x1[,2], 
                  class = rep(1:3, each = 200), 
                  cluster = kmeans(x = x1, nstart = 10L, centers = 2L)$cluster)
g1 <- ggplot(data = df1, aes(x = x, y = y, col = factor(cluster))) +
  geom_point(show.legend = FALSE) + 
  scale_color_manual(values=viridis::viridis(3)) + 
  theme_minimal() + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank(),
)
df2 <- data.frame(x = x2[,1], 
                  y = x2[,2], 
                  class = rep(1:3, each = 200), 
                  cluster = kmeans(x = x2[,1:2], nstart = 10L, centers = 3L)$cluster)
g2 <- ggplot(data = df2, aes(x=x,y=y, col = factor(cluster))) +
  geom_point(show.legend = FALSE) + 
  scale_color_manual(values=viridis::viridis(3)) + 
  theme_minimal() + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank(),
)
df3 <- data.frame(x = x3[,1], 
                  y = x3[,2],
                  cluster = kmeans(x = x3, nstart = 10L, centers = 3L)$cluster)
g3 <- ggplot(data = df3, aes(x = x, y = y, col = factor(cluster))) +
  geom_point(show.legend = FALSE) + 
  scale_color_manual(values=viridis::viridis(3)) + 
  theme_minimal() + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank(),
)
x4 <- rbind(cbind(rnorm(125) + 2.5, rep(0, 125)),
            cbind(rnorm(200) + 2.5, 1+ rexp(200, rate = 0.25) ))#mlbench::mlbench.spirals(200,1,0.025)$x
#https://smorbieu.gitlab.io/k-means-is-not-all-about-sunshines-and-rainbows/
# df4 <- {
#     generateUniformData <- function(cluster, minX, maxX) {
#         x = runif(500, min = minX, max = maxX)
#         y = runif(500, min = -4, max = 9)
#         data.frame(x, y) %>% mutate(class=cluster)
#     }
# 
# generateUniformData(1, -4, 1) %>% bind_rows(generateUniformData(2, 3, 9)) %>%
#   mutate(class = factor(class))
# }
df4 <- data.frame(x = x4[,2], 
                  y = x4[,1], 
                  cluster = kmeans(x = x4, nstart = 10L, centers = 2)$cluster)
g4 <- ggplot(data = df4, aes(x = x, y = y, col = factor(cluster))) +
  geom_point(show.legend = FALSE) + 
  scale_color_manual(values=viridis::viridis(2)) + 
  theme_minimal() + 
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank(),
)
library(patchwork)
(g1 + g4) / (g2 + g3)
```


# Séparation linéaire de l'espace

Avec la distance euclidienne, la partition de l'espace est linéaire.

```{r}
#| eval: true
#| echo: false
#| label: fig-voronoy
#| fig-cap: "Partitions de Voronoï pour les regroupements avec séparateur linéaire."
knitr::include_graphics("figures/fig-voronoikmoy.png")
```




# Hyperparamètres avec $K$-moyennes

1. le choix de la mesure de distance
2. les valeurs initiales des prototypes
3. le nombre de groupes $K$ 


# Distance

Avec la distance euclidienne $l_2$, les prototypes correspondent avec le barycentre (moyenne variable par variable) des observations du regroupement.

Avec la distance de Manhattan $l_1$, les prototypes correspondent avec la médiane variable par variable ($K$-médianes).

Autrement, optimisation nécessaire dans l'étape $M$ de l'algorithme.

# Initialisation

\footnotesize 

- Choisir aléatoirement $K$ observations dans la base de données. 
- Répéter plusieurs fois
- Prendre la meilleure segmentation du lot (celle avec la valeur optimale de la fonction objective).



```{r}
#| label: fig-kmoyenne-mauvais
#| echo: false
#| eval: true
#| fig.cap: "Regroupements pour $K=3$ groupes avec une mauvaise initialisation principale (gauche) et une bonne initialisation (droite)."
#| out-width: '65%'
set.seed(1234)
set1 <- mvtnorm::rmvnorm(n = 300, c(14,12), matrix(c(4,0.5,0.5,5),2))
set2 <- mvtnorm::rmvnorm(n = 300, c(5.5,-1), matrix(c(1,0.5,0.5,6),2))
set3 <- mvtnorm::rmvnorm(n = 300, c(0,0), matrix(c(2,0,0,2),2))
db <- rbind(set1, set2, set3)
colnames(db) <- c("X1", "X2")
DF <- tibble::as_tibble(db)

kmoy1 <- kmeans(x = DF, 
       centers = rbind(c(5, 0), c(0,0), c(15,10)), 
       iter.max = 25L, 
       algorithm = "Lloyd")
kmoy2 <- kmeans(x = DF, 
       centers = rbind(c(0,2.5), c(12,12), c(20,20)), 
       iter.max = 25L, 
       algorithm = "Lloyd")

DF$cluster1 <- factor(kmoy1$cluster)
DF$cluster2 <- factor(kmoy2$cluster)
g1 <- ggplot(data = DF,
       mapping = aes(x = X1, 
                     y = X2, 
                     color = cluster1)) +
  geom_point() + 
  theme_minimal() + 
  theme(legend.position = 'none') +
  labs(x = "",
       y = "")
g2 <- ggplot(data = DF,
       mapping = aes(x = X1, 
                     y = X2, 
                     color = cluster2)) +
  geom_point() + theme_minimal() +
  theme(legend.position = 'none') +
  labs(x = "",
       y = "")
g2 + g1
```

# K-moyennes dans **R**

```{r}
#| eval: false
#| echo: true
set.seed(60602)
kmoy5 <- kmeans( # distance euclidienne 
   x = donsmult_std, # données
   centers = 5L, # nb groupes
   nstart = 10, # nb initialisation aléatoire
   iter.max = 25) # nb étapes maximum dans optimisation 

kmoy5$cluster # étiquettes
kmoy5$size # répartition
kmoy5$tot.withinss # fonction objective minimal
kmoy5$centers # barycentres (données standardisées)
```

# Algorithme des $K$-moyennes${}^{++}$

Choisir des observations comme valeurs initiales, mais avec échantillonnage préférentiel (points éloignés les uns des autres).

0. sélectionner une observation au hasard pour $\boldsymbol{\mu}_1$

Pour $k=2, \ldots, K$

1. calcul de la distance carrée minimale entre l'observation $\mathbf{X}_i$ et les prototypes précédemment choisis,
\begin{align*}
p_i = \min \{d(\mathbf{X}_i, \boldsymbol{\mu}_1; l_2)^2, \ldots, d(\mathbf{X}_i, \boldsymbol{\mu}_{k-1}; l_2)^2)\}
\end{align*}
2. Choisir le prototype initial $\boldsymbol{\mu}_k$ au hasard parmi les observations avec une probabilité de $p_i/\sum_{j} p_j$ pour l'observation $\mathbf{X}_i$.

#  $K$-moyennes${}^{++}$ dans **R**

Utiliser le paquet `flexclust` 
```{r}
#| eval: false
#| echo: true
set.seed(60602)
kmoypp5 <- flexclust::kcca(
  x = donsmult_std,
  k = 5, # nb groupes
  family = flexclust::kccaFamily("kmeans"),
  control = list(initcent = "kmeanspp"))
# Vérifier répartition
kmed5@clusinfo
# Coordonnées des prototypes standardisés
kmed5@centers
# Étiquettes
kmed5@cluster
```

# Choix du nombre de regroupements

Plusieurs critères généralement applicables

- silhouettes (`cluster::silhouette`)
- statistique d'écart (`cluster::clusGap`)

Critères plus spécifiques aux $K$-moyennes rattachés à la fonction objective

- graphique du $R^2$
- critère d'information bayésien "BIC"

# Somme du carré des distances intra-groupes

La fonction objective de l'@eq-fobjKmoy avec la distance euclidienne représente la somme du carré des distances (SCD) 
\begin{align*}
\mathsf{SCD}_K &= \mathsf{SCD}_{1,K} + \cdots + \mathsf{SCD}_{K,K};
\intertext{où}
\mathsf{SCD}_{k,K} &= \sum_{i \in G_k}\|\mathbf{X}_i -  \boldsymbol{\mu}_{k}\|^2_2,
\end{align*}
est la somme des distances euclidiennes au carré entre les observation du groupe $G_k$ et leur barycentre $\boldsymbol{\mu}_k$.

Avec un seul groupe, la distance par rapport à la moyenne est $\mathsf{SCT} = \mathsf{SCD}_{1}$, la somme totale du carré de toutes les distances par rapport au barycentre global.



# Somme du carré des distances

La valeur optimale de la somme du carré des distances mesure va mécaniquement${}^{*}$ diminuer  quand $K$ augmente $\mathsf{SCD}_1 > \mathsf{SCD}_2 \cdots$.

En pratique, cela peut ne pas être le cas si le minimum local est sous-optimal. 

Si la réduction de la somme du carré des distances est négligeable, on pourrait penser que l'ajout d'un groupe supplémentaire.

# Critères $R^2$ et "BIC"

On peut mesurer le pourcentage de variance expliquée, $$R^2_K = 1-\frac{\mathsf{SCD}_K}{\mathsf{SCT}}.$$ 

On cherche un point d'inflexion (coude) à partir duquel l'amélioration est négligeable.


Puisque la somme du carré des distances diminue avec $K$, on peut considérer l'ajout d'une pénalité pour le nombre de paramètres estimés

$$\textsf{BIC}=\mathsf{SCD}_K + \ln(n)Kp$$

La plus petite valeur du "$\textsf{BIC}$" est préférable.

# Graphique de coefficient de détermination $R^2$

Le pourcentage de variance expliqué augmente de manière plus ou moins constante jusqu'à 8 ou 9 composantes.

```{r}
#| eval: true
#| echo: false
#| cache: true
library(hecmulti)
set.seed(60602)
kmoy <- list()
ngmax <- 15L
for(i in seq_len(ngmax)){
 kmoy[[i]] <- kmeans(donsmult_std,
                     centers = i,
                     nstart = 20,
                     iter.max = 50)
}

# Déterminer le nombre de groupes avec critères

# Somme carré intra-groupes et somme carré totale
scd <- sapply(kmoy, function(x){x$tot.withinss})

# Homogénéité et pourcentage de variance expliquée
# Graphique du R-carré et du R-carré semi-partiel
homogene <- homogeneite(scd, which = 1)
```

# Critère BIC

Le critère suggère aussi un nombre élevé de regroupements, ici `r which.min(sapply(kmoy, BIC))` (nombre maximum de `r length(kmoy)`).

```{r}
#| eval: true
#| echo: false
#| cache: true
bic_kmoy <- sapply(kmoy, BIC)
ggplot(data = data.frame(bic = bic_kmoy,
                         ng = seq_along(kmoy)),
       mapping = aes(x = ng, y = bic)) +
  geom_line() +
  scale_x_continuous(breaks = seq_along(kmoy)) +
  labs(x = "nombre de regroupements",
       y = "",
       subtitle = "Critère BIC (somme du carré des distances)") +
  theme_classic()

```

# Silhouettes

Pour chaque observation $\mathbf{X}_i$, on calcule
 

- $a_i$, la moyenne des dissimilarités entre $\mathbf{X}_i$ et les observations de son regroupement
- $b_i$, le minimum parmi les $K-1$ dissimilarités  moyennes entre $\mathbf{X}_i$ et les observations de chaque autre regroupement. 

On calcule la silhouette
$$s_i=\frac{b_i-a_i}{\max\{a_i, b_i\}}$$


Il est possible que la silhouette $s_i$ soit négative: cela indique généralement des observations mal regroupées.

De bons regroupements seront obtenus si la silhouette moyenne est élevée.

# Graphique des silhouettes {.smaller}

\footnotesize
Coûteux en calcul (nécessite matrice de dissimilarité), possible de faire avec un sous-échantillon aléatoire.

La segmentation de droite de la @fig-silhouette est supérieure parce que les regroupements sont plus homogènes et mieux équilibrés.

```{r}
#| eval: true
#| echo: false
#| label: fig-silhouette
#| fig-cap: "Profil des silhouettes pour deux regroupements d'un jeu de données."
#| out-width: '70%'
par(mfrow=c(1,2))
library(cluster)
cols <- MetBrewer::MetPalettes$Hiroshige[[1]][c(10,5,1)]
#Même résultat avec hclust
m1 <- cluster::agnes(x=iris[,1:4],
                     metric = "canberra", 
                     method = "single")
s1 <- silhouette(cutree(m1, k = 3), dist(iris[,1:4]))
plot(s1, 
     main="", 
     sub = paste0("moyenne des silhouettes: ", round(mean(s1[,3]),2)), 
     xlab = "silhouette", 
     col = cols) 
abline(v=mean(s1[,3]))
kpam <- pam(iris[,1:4], k=3)
s.pam <- silhouette(kpam)
plot(s.pam, 
     main="", 
     sub = paste0("moyenne des silhouettes: ", round(mean(s.pam[,3]),2)), 
     xlab = "silhouette", 
     col = cols)
abline(v = mean(s.pam[,3]))
```

# Silhouettes avec segmentation des $K$-moyennes

```{r}
#| eval: true
#| echo: false
#| cache: true
#| out-width: '80%'
set.seed(60602)
kmoy5 <- kmeans(
   x = donsmult_std, # données
   centers = 5L, # nb groupes
   nstart = 25, # nb initialisation aléatoire
   iter.max = 25)
sub <- sample.int(
  n = nrow(donsmult),
  size = 2500)
dist_sub <- cluster::daisy(
  x = donsmult_std[sub,],
  metric = "euclidean")
scale_colour_discrete <- 
  silhouettes_kmoy <-     
    cluster::silhouette(x = kmoy5$cluster[sub],
               dist = dist_sub)
g1 <- factoextra::fviz_silhouette(silhouettes_kmoy,
                            print.summary = FALSE)
g1 +
  labs(title = "",
       y = "",
       subtitle = "silhouette",
       caption = "silhouette moyenne: 0.32") +
  scale_color_manual(values  =  MetBrewer::met.brewer("Hiroshige",5)[1:5]) + 
  theme(legend.position = "none")
```

# Règle importante pour le choix de $K$

Utilisez votre jugement (et le gros bon sens). 

Les segments doivent être interprétables.


Vérifiez que la taille des segments n'est pas fortement débalancée.

# Statistiques descriptives par segment

\footnotesize

```{r}
#| eval: true
#| echo: false
#| label: tbl-kmoy5resume
#| tbl-cap: "Moyenne des variables explicatives par segment (segmentation avec $K$-moyennes et cinq regroupements)."
set.seed(60602)
kmoy5 <- kmeans(
   x = donsmult_std, # données
   centers = 5L, # nb groupes
   nstart = 50, # nb initialisation aléatoire
   iter.max = 25)
kmoy5_tab <- donsmult |>
  dplyr::group_by(groupe = kmoy5$cluster) |>
  dplyr::summarise_all(mean) |> 
  t() |>
  as.data.frame()
kmoy5_tab[] = lapply(kmoy5_tab, 
                     function(x){sprintf("%.2f", x)})
rownames(kmoy5_tab)[1] <- "décompte"
kmoy5_tab[1,] <- as.character(table(kmoy5$cluster))
knitr::kable(kmoy5_tab, 
             align = "r", 
             booktabs = TRUE,
             row.names = TRUE, 
             linesep = "",
             col.names = 1:5) |>
      kableExtra::kable_styling()
```

# Interprétation des segments

Les regroupements obtenus sont interprétables:

- Groupe 1: Petits donateurs, faible nombre de dons. N'ont pas donné depuis longtemps. Refus fréquents et délai entre dons élevés
- Groupe 2: Grands donateurs fidèles: plus petit groupe. Ces personnes ont fait plusieurs dons, leur valeur maximale est élevée. N'ont pas donné récemment.
- Groupe 3: Petits fidèles. Dons plus élevés que la moyenne, nombre de dons élevés et récents.
- Groupe 4: Petits nouveaux. Moins d'ancienneté, dons récents et refus fréquents relativement à l'ancienneté.
- Groupe 5: Donateurs inactifs. Faible montant de dons, plutôt anciens, plusieurs refus.


# Récapitulatif

- L'analyse de regroupement (*clustering*) est une méthode d'apprentissage non-supervisée
- Plusieurs choix de l'analyste (mesure de dissimilarité, algorithme, choix des hyperparamètres) impactent la segmentation et peuvent mener à des résultats très différents avec les mêmes données.
- L'algorithme des $K$-moyennes est le plus employé et son faible coût permet son utilisation avec des mégadonnées.


