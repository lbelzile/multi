---
title: "Régression logistique"
subtitle: "Analyse multidimensionnelle appliquée"
date: ""
author: "Léo Belzile"
institute: "HEC Montréal"
format: beamer
navigation: empty
colortheme: Flip
innertheme: Flip
outertheme: Flip
themeoptions: "bullet=circle, topline=true, shadow=false"
beamerarticle: false
pdf-engine: lualatex
cache: true
code-line-numbers: true
fig-align: 'center'
mainfont: "VisbyCF-Medium"
mathfont: 'Latin Modern Math'
sansfont: 'Latin Modern Sans'
keep-tex: true
include-in-header: 
      text: |
        \usepackage{tabu}
        \usepackage{mathtools}
        \usepackage{mathrsfs}
---


# _Professional Rodeo Cowboys Association_ {#cowboy}

L'exemple suivant est inspiré de l'article

> Daneshvary, R. et Schwer, R. K. (2000) The Association Endorsement and Consumers' Intention to Purchase. _Journal of Consumer Marketing_ **17**, 203-213.

**Objectif**: Les auteurs cherchent à voir si le fait qu'un produit soit recommandé par le _Professional Rodeo Cowboys Association_ (PRCA) a un effet sur les intentions d'achats. 



# Données du PRCA

On dispose de 500 observations sur les variables suivantes dans la base de données `logit1`:
\footnotesize 

- $Y$: seriez-vous intéressé à acheter un produit recommandé par le PRCA
    - $\texttt{0}$: non
    - $\texttt{1}$: oui
- $\mathrm{X}_1$: quel genre d'emploi occupez-vous?
    - $\texttt{1}$: à la maison
    - $\texttt{2}$: employé
    - $\texttt{3}$: ventes/services
    - $\texttt{4}$: professionnel
    - $\texttt{5}$: agriculture/ferme
- $\mathrm{X}_2$: revenu familial annuel
    - $\texttt{1}$: moins de 25 000
    - $\texttt{2}$: 25 000 à 39 999
    - $\texttt{3}$: 40 000 à 59 999
    - $\texttt{4}$: 60 000 à 79 999
    - $\texttt{5}$: 80 000 et plus

\normalsize

# Données du PRCA    
    
\footnotesize

- $\mathrm{X}_3$: sexe
    - $\texttt{0}$: homme
    - $\texttt{1}$: femme
- $\mathrm{X}_4$: avez-vous déjà fréquenté une université?
    - $\texttt{0}$: non
    - $\texttt{1}$: oui
- $\mathrm{X}_5$: âge (en années)
- $\mathrm{X}_6$: combien de fois avez-vous assisté à un rodéo au cours de la dernière année?
    - $\texttt{1}$: 10 fois ou plus
    - $\texttt{2}$: entre six et neuf fois
    - $\texttt{3}$: cinq fois ou moins

\normalsize


# Régression logistique

Expliquer le comportement de la **moyenne** d'une variable binaire $Y\in\{0,1\}$ en utilisant un modèle de régression avec $p$ variables explicatives $\mathrm{X}_1, \ldots, \mathrm{X}_p$.

$$\underset{\text{moyenne théorique}}{\mathsf{E}(Y=1 \mid \mathbf{X})} = \underset{\text{probabilité de succès}}{\Pr(Y=1 \mid \mathbf{X})}=p$$

# Objectif de la régression

1) **Inférence** : comprendre comment et dans quelles mesures les variables $\mathbf{X}$ influencent la probabilité que $Y=1$.
2) **Prédiction** : développer un modèle pour prévoir des valeurs de $Y$ ou la probabilité de succès à partir des $\mathbf{X}$.

# Exemples

- Est-ce qu'un client potentiel va répondre favorablement à une offre promotionnelle?
- Est-ce qu'un client est satisfait du service après-vente?
- Est-ce qu'un client va faire faillite ou non au cours des trois prochaines années.

# Inférence et interprétation

Ce cours est consacré à l'estimation et l'interprétation des paramètres du modèle dans le cas binaire.



Par convention, on désigne le résultat « $1$ » par un succès et « $0$ » par un échec.

# Modéliser une probabilité avec une régression linéaire?

Mauvaise idée!

- Sans contrainte, on peut obtenir des probabilités négatives ou supérieures à 1!
- les données binaires ne respectent pas le postulat d'égalité des variances
    - invalide les résultats des tests d'hypothèse pour les coefficients.
 
# Illustration: linéaire vs logistique



```{r}
#| label: fig-demandecredit
#| eval: true
#| echo: false
#| cache: true
#| fig-cap: "Données de la réserve de Boston sur l'approbation de prêts hypothécaires (1990); données tirées de Stock et Watson (2007)."
#| message: false
#| warning: false
#| out.width: '80%'
#| fig.width: 7
#| fig.height: 4
library(ggplot2)
data(HMDA, package = "AER")
ggplot(data = HMDA[which(HMDA$pirat < 1 & HMDA$afam == "yes"),],
       aes(y = ifelse(deny == "yes", 1, 0), 
           x = pirat)) +
  geom_hline(yintercept = c(0,1), 
             alpha = 0.1, 
             linetype = 2) + 
  geom_point() +
  stat_smooth(method = "lm", 
              se = FALSE, 
              fullrange = TRUE,
              col = 4) +
  stat_smooth(method = "glm", 
              se = FALSE,
              fullrange = TRUE,
              method.args = list(family = "binomial"), col = 2) +
  labs(subtitle = "demande d'approbation pour crédit hypothécaire", 
       y = "",
       x = "ratio paiements sur revenus") +
  scale_y_continuous(breaks=c(0L,1L), 
                     limits = c(0,1)) + 
  scale_x_continuous(breaks = seq(0,1, by = 0.25), 
                     limits = c(0,1),
                     expand = c(0,0),
                     labels = c("0","0.25","0.5","0.75","1")) + 
  geom_text(data = tibble::tibble(
    x = c(1,0.05), 
    y = c(0.1,0.9), 
    label = c("refusée", "acceptée")), 
   aes(x = x, y = y, label = label),
   hjust = "inward") +
    theme_classic()
```

# Fonction de liaison

Idée: appliquer une transformation au **prédicteur linéaire**
$$\eta = \beta_0 + \beta_1 \mathrm{X}_1 + \cdots + \beta_p \mathrm{X}_p$$
pour que la prédiction soit entre zéro et un.

On considère
\begin{align*}
 p &= \textrm{expit}(\eta) = \frac{\exp(\eta)}{1+\exp(\eta)}
= \frac{1}{1+\exp(-\eta)}.
\end{align*}

# Courbe sigmoïde



```{r}
#| label: fig-logitplot
#| echo: false
#| out-width: '90%'
#| fig.height: 6
#| fig.width: 10
#| fig.cap: "Valeurs ajustées du modèle de régression logistique en fonction du prédicteur linéaire $\\eta$."
#| eval: true
#| fig.align: 'center'
logit <- function(x){log(x/(1-x))}
expit <- function(x){1/(1+exp(-x))}
par(mar = c(4,4,1,0.1), bty = "l")
curve(expit, 
    ylim = c(0,1),
    yaxs="i",
    from = -3.5, 
    to = 3.5, 
    xlab = expression(eta), 
    ylab = expression(p))

```

# Ajustement du modèle

La fonction `glm` dans **R** ajuste un modèle linéaire généralisé (par défaut, Gaussien pour régression linéaire).

- L'argument `family=binomial(link="logit")` permet de spécifier que l'on ajuste un modèle logistique.


```{r}
#| label: logistique-init
#| eval: false
#| echo: true
#| message: false
data(logit1, package = "hecmulti")
# Ajustement du modèle avec toutes
# les variables explicatives
modele1 <- glm(formula = y ~ .,
            family = binomial(link = "logit"),
            data = logit1)
```

# Sortie

Tableau résumé avec les coefficients (`summary`)

```{r}
#| eval: false
#| echo: true
summary(modele1)
```

# Interprétation

Par défaut, pour des variables $0/1$, le modèle décrit la probabilité de succès.


```{r}
#| label: logitplot2
#| echo: false
#| eval: true
#| out.width: '70%'
#| fig.align: 'center'
#| fig.width: 7
#| fig.height: 3 
par(mar = c(4,4,1,0.1), bty = "l")
curve(expit(-3.05 + 0.0749 * x), 
    ylim = c(0,1),
    yaxs = "i",
    from = 18, 
    to = 59, 
    xlab = "âge (en années)", 
    ylab = "p")

```

Si le coefficient $\beta_j$ de la variable $\mathrm{X}_j$ est positif, alors plus la variable augmente, plus $\Pr(Y=1)$ augmente.


# Rappel exponentiels et logarithme

Quelques propriétés de la fonction exponentielle:

- $\exp(0) = 1$
- $\exp(a + b) = \exp(a)\exp(b)$
- $\exp(ab)= \exp(a)^b$

Quelques propriétés de la fonction logarithmique

- $\ln(1)=0$,
- $\ln(\exp(x))=x$ (fonction inverse)
- $\ln(ab) = \ln(a) + \ln(b)$


# Cote

Si on applique la transformation inverse, on obtient

$$\ln\left(\frac{p}{1-p} \right) = \eta = \beta_0 + \beta_1 \mathrm{X}_1 + \cdots + \beta_p \mathrm{X}_p.$$

ou, en prenant l'exponentielle de chaque côté,

$$ 
\mathsf{cote} = \frac{p}{1-p} = \exp(\beta_0)\cdots\exp(\beta_p \mathrm{X}_p)
$$

**Modèle multiplicatif** pour la cote.

# Cote et probabilité

La cote est utilisée dans les paris sportifs
\begin{align*}
 \mathsf{cote}(p) = \frac{p}{1-p} = \frac{\Pr(Y=1 \mid \mathbf{X})}{\Pr(Y=0 \mid \mathbf{X})}.
\end{align*}


```{r}
#| label: tbl-cotes
#| eval: true
#| echo: false
#| tbl-cap: "Cote et probabilité de succès"
datf <- matrix("", nrow = 2, ncol = 10)
datf[1,] <- c("\\(p\\)", sprintf(seq(0.1,0.9, by = 0.1),fmt = "%.1f"))
datf[2,] <- c("cote", paste0("\\(",c("\\frac{1}{9}","\\frac{1}{4}","\\frac{3}{7}","\\frac{2}{3}","1","\\frac{3}{2}","\\frac{7}{3}","4","9"),"\\)"))
knitr::kable(datf[-1, , drop = FALSE], 
             col.names = datf[1, , drop = FALSE],
             row.names = FALSE,
             booktabs = TRUE,
             longtable = FALSE,
             align =  paste0(c("l",rep("c", 9)),collapse = ""),
             escape = FALSE,
             format = "latex") 
```


# Interprétation avec données du PRCA

Le modèle ajusté en termes de cote est
\begin{align*}
 \frac{\Pr(Y=1 \mid \mathrm{X}_5=x_5)}{\Pr(Y=0 \mid \mathrm{X}_5=x_5)} = \exp(-3.05)\exp(0.0749x_5).
\end{align*}

\small

- Lorsque $\mathrm{X}_5$ augmente d'une année, la cote est multipliée par $\exp(0.0749) = 1.078$ peut importe la valeur de $x_5$. 
- Pour deux personnes dont la différence d'âge
    - est d'un an, la cote de la personne plus âgée est 7.8\% plus élevée
    - est de 10 ans, la cote de la personne plus âgée est 112\% plus élevée (cote multipliée par $\exp(10 \times 0.0749)=1.078^{10} = 2.12$)

\normalsize

# Modèle complet

On considère le modèle avec toutes les variables explicatives:

```{r}
#| eval: false
#| echo: true
modele2 <- glm(
  formula = y ~ .,
  data = logit1,
  family = binomial)
exp(coef(modele2))
```

```{r}
#| eval: true
#| echo: false
modele2 <- glm(
  formula = y ~ .,
  data = hecmulti::logit1,
  family = binomial)
round(exp(coef(modele2)), 3)
```
# Interprétation des coefficients

Si on a plusieurs variables explicatives, les coefficients sont interprétés en modifiant une variable à la fois.

On compare deux profils identiques, sauf pour la variable en question 

- toute chose étant égale par ailleurs
- *ceteris paribus*

# Interprétation des coefficients

**Variable continue**:

- La cote de la personne plus âgée d'un an est 1.116 fois celle de la personne plus jeune, *ceteris paribus*, une augmentation de 11,6%

# Variable binaire

Le rapport de cote pour les femmes (`x3=1`) versus les hommes (`x3=0`) est de $\exp(\widehat{\beta}_{\mathrm{X}_3}) = 3.854$:

- les femmes sont plus susceptibles de suivre les recommendations d'achat toute chose étant égale par ailleurs, 
- Inversement, le rapport de cote homme/femme est de 1/3.854=0.259, 

On peut donc conclure que:

- la cote des femmes est 285.4% plus élevée que celle des hommes. 
- la cote des hommes est 74.1% inférieure à celle des femmes.

# Variable catégorielle

Toutes les comparaisons sont effectuées avec la catégorie de référence.

Pour `x1`, c'est à la maison. Le rapport de cote est
$$\frac{\mathsf{cote}\{Y \mid \mathrm{X}_{1}=2 (\text{employé}), \ldots\}}{\mathsf{cote}\{Y \mid \mathrm{X}_{1}=1 (\text{maison}), \ldots\}} = \exp(\widehat{\beta}_{\mathrm{X}_{1}=2}) = 0.438$$

Le coefficient pour $\mathrm{X}_{1}=1$ est zéro, d'où $\exp(\widehat{\beta}_{\mathrm{X}_{1}=1})=1$ (absent du tableau).

On peut ordonner les type d'emploi selon la probabilité de succès à l'aide des coefficients: *ceteris paribus* on obtient le classement

- employé < professionnel < ventes/service < agriculture < maison.

# Invariance

Si on voulait le rapport de cote professionnel vs employé, inutile de réajuster le modèle.

On peut calculer 

$$ 
\dfrac{\dfrac{\mathsf{cote}(Y \mid \mathrm{X}_{1}=4, \ldots)}{\mathsf{cote}(Y \mid \mathrm{X}_{1}=1, \ldots)}}{\dfrac{\mathsf{cote}(Y \mid \mathrm{X}_{1}=2, \ldots)}{\mathsf{cote}(Y \mid \mathrm{X}_{1}=1, \ldots)}} = \frac{\exp(\widehat{\beta}_{\mathrm{X}_{1}=4})}{\exp(\widehat{\beta}_{\mathrm{X}_{1}=2})} =1.162746
$$

plutôt que de changer la catégorie de référence via

```{r}
#| eval: false
#| echo: true
logit2 <- logit1 |> 
   mutate(x1 = relevel(x1, ref = 2))
```

# Vraisemblance et estimation du modèle

Pour un modèle probabiliste donné, on peut calculer la « probabilité » d'avoir obtenu les données de l'échantillon.

Si on traite cette « probabilité » comme une fonction des paramètres, on l'appelle **vraisemblance**.


**Maximum de vraisemblance**: valeurs des paramètres qui maximisent la fonction de vraisemblance.

- on cherche les valeurs des paramètres qui rendent les données les plus plausibles
   

# Vraisemblance d'une observation

La vraisemblance d'une observation $Y_i \in \{0,1\}$ (loi Bernoulli/binomiale) est 

\begin{align*}
L(\boldsymbol{\beta}; y_i) = p_i^{y_i}(1-p_i)^{1-y_i} = \begin{cases} 
p_i & y_i = 1 (\text{succès})\\
1-p_i & y_i = 0 (\text{échec}) 
\end{cases}
\end{align*}
et où $$p_i = \mathrm{expit}(\eta_i) = \frac{\mathrm{exp}(\beta_0 + \beta_1 \mathrm{X}_{i1} + \cdots + \beta_p\mathrm{X}_{ip})}{1+\mathrm{exp}(\beta_0 + \beta_1 \mathrm{X}_{i1} + \cdots + \beta_p\mathrm{X}_{ip})}.$$


# Probabilité conjointe d'événements binaires

- Si les observations sont indépendantes, la probabilité conjointe d'avoir un résultat donné est le produit des probabilités pour chaque observation. 
- Il n'y a pas de solution explicite pour $\widehat{\beta}_0, \ldots, \widehat{\beta}_p$ dans le cas de la régression logistique: il faut maximiser la vraisemblance.
   
# Log vraisemblance


- Pour des raisons de stabilité numérique, on maximise le logarithme naturel $\ell(\boldsymbol{\beta}) = \ln L(\boldsymbol{\beta})$ de la log vraisemblance conjointe de l'échantillon (transformation monotone croissante).
- La log vraisemblance est simplement la somme des contributions individuelles.
- On utilise la log vraisemblance $\ell$ comme mesure d'ajustement et pour construire des tests d'hypothèse.

# Prédiction des probabilités de succès

Des estimés des coefficients $\widehat{\beta}$ découlent une estimation de $\Pr(Y=1)$ pour les valeurs $\mathrm{X}_1=x_1, \ldots, \mathrm{X}_p=x_p$ d'un individu donné,
\begin{align*}
 \widehat{p} = \textrm{expit}(\widehat{\beta}_0 + \cdots + \widehat{\beta}_px_p).
\end{align*}



# Modèle nul

Un modèle avec uniquement l'ordonnée à l'origine retournera $\widehat{p}$, la proportion empirique de succès.

Comme pour la régression linéaire, c'est la moyenne des observations.



# Test du rapport de vraisemblance {.fragile}

:::: {.columns}

::: {.column width="60%"}

Pour les modèles ajustés par maximum de vraisemblance.



Comparaison de modèles **emboîtés**


- Modèle complet (sous l'alternative) avec $p$ variables explicatives
- Modèle restreint (sous l'hypothèse nulle) sur lequel on impose $k\leq p$ restrictions.


:::

::: {.column width="40%"}

```{r}
#| echo: false
#| eval: true
#| out.width: '100%'
knitr::include_graphics("figures/poupeesrusses.jpg")
```

:::

::::

# Exemple

Comparons un modèle avec et sans $X_6$.

Variable catégorielle à trois niveaux (deux coefficients associés à $\mathrm{I}(\mathrm{X}_{6}=2)$ et $\mathrm{I}(\mathrm{X}_{6}=3)$.

```{r}
#| eval: true
#| echo: true
modele2 <-  glm(y ~ x1 + x2 + x3 + x4 + x5 + x6,
                 data = hecmulti::logit1,
                 family = binomial(link = "logit"))
modele3 <-  glm(y ~ x1 + x2 + x3 + x4 + x5,
                 data = hecmulti::logit1,
                 family = binomial(link = "logit")) 
```

# Test d'hypothèse

On teste l'hypothèse nulle  $\mathscr{H}_0: \beta_{\mathrm{X}_6=2} = \beta_{\mathrm{X}_6=3} = 0$ (soit $k=2$ restrictions).

L'hypothèse alternative est qu'au moins un des coefficients est non-nul.

Si la valeur $p$ est inférieure au seuil de signification, typiquement $\alpha = 0.05$, on rejette l'hypothèse nulle.

 - on conclut que la variable explicative $\mathrm{X}_6$ améliore significativement l'ajustement du modèle.



# Rapport de vraisemblance

Le test est basé sur la statistique
\begin{align*}
 D = -2\{\ell(\widehat{\boldsymbol{\beta}}_0)-\ell(\widehat{\boldsymbol{\beta}})\}.
\end{align*}

Cette différence $D$, lorsque l'hypothèse $\mathscr{H}_0$ est vraie, suit approximativement une loi khi-deux $\chi^2_k$.

# Exemple de test

\footnotesize

```{r}
#| eval: true
#| echo: true
# modèle 2 (alternative), modèle 3 (nulle)
anova(modele3, modele2, test = "LR")
## Deviance = -2*log vraisemblance
rvrais <- modele3$deviance - modele2$deviance
pchisq(rvrais, df = 2, lower.tail = FALSE) # valeur-p
```

\normalsize

# Tester la significativité des variables

Si un paramètre n'est pas significativement différent de 0, cela veut dire qu'il n'y a pas de lien significatif entre la variable et la réponse *une fois que les autres variables* sont dans le modèle.

\footnotesize

```{r}
#| eval: true
#| echo: true
car::Anova(modele2, type = "3")
```

\normalsize


# Intervalles de confiance pour coefficients

On peut aussi considérer des intervalles de confiance pour les coefficients individuels.

Ceux obtenus par défaut dans **R** sont appelés *intervalles de confiance de vraisemblance profilée*.

```{r}
#| eval: false
#| echo: true
confint(modele2)      # IC pour beta
exp(confint(modele2)) # IC pour exp(beta)
```

Ces intervalles sont invariants aux reparamétrisation: si $[b_i, b_s]$ est l'intervalle de vraisemblance profilée pour $\beta$, l'intervalle pour $\exp(\beta)$ est simplement $[\exp(b_i), \exp(b_s)]$.

# Intervalles de confiance

```{r}
#| label: fig-confint-modele2-logist
#| echo: false
#| eval: true
#| out-width: '80%'
#| fig-width: 10
#| fit-height: 4
#| fig-cap: "Intervalles de confiance profilés de niveau 95\\% pour les coefficients du modèle logistique (échelle exponentielle)."
data(logit1, package = "hecmulti")
modele2 <- glm(
  y ~ x1 + x2 + x3 + x4 + x5 + x6,
  data = logit1,
  family = binomial(link = "logit")
)
modelsummary::modelplot(modele2, 
                        exponentiate = TRUE,
       coef_omit = 'Interc') + 
  ggplot2::labs(x = paste0("exp(", expression(beta),")")) +
  ggplot2::geom_vline(xintercept = 1)


```


# Tests et intervalles de confiances

Comme $\exp(\cdot)$ est  une transformation monotone croissante, 
$$\beta>0 \quad \iff \quad \exp(\beta)>1.$$

Si la valeur postulée, par exemple $\mathscr{H}_0: \beta_j=0$ ou $\exp(\beta_j)=1$, est dans l'intervalle de confiance de niveau $1-\alpha$, on ne rejette pas l'hypothèse nulle.

# Coefficients pour données complètes

\footnotesize

```{r}
#| label: tbl-logit1-complet
#| eval: true
#| echo: false
#| cache: true
#| tbl-cap-location: bottom
#| tbl-cap: "Modèle logistique avec toutes les variables catégorielles."
data(logit1, package = "hecmulti")
logit1 <- logit1 |> 
  dplyr::mutate(y = factor(y),
               x3 = factor(x3),
               x4 = factor(x4))
m1 <- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6, 
          family = binomial(link = "logit"),
          data = logit1)
tbl <- m1 |>
  gtsummary::tbl_regression(
  exponentiate = TRUE,
  intercept = FALSE) |>
  gtsummary::add_global_p() |>
  gtsummary::bold_labels()
tbl$table_styling$header$label[c(12,20,26,27)] <- c("variables","cote", "IC 95%","valeur-p")
tbl$table_body <- tbl$table_body[1:15,]
# tbl$table_body$label[1] <- "cst"
tbl$table_styling$footnote_abbrev$footnote <- 
  c("cote = rapport de cote",
    "IC = intervalle de confiance",
    "ET = erreur-type")
tbl |> 
   gtsummary::as_gt()  |>
   gt::tab_options(table.width = gt::pct(100))
```

\normalsize

# Coefficients pour données complètes

\footnotesize

```{r}
#| label: tbl-logit1-complet2
#| eval: true
#| echo: false
#| cache: true
#| tbl-cap-location: bottom
#| tbl-cap: "Modèle logistique avec toutes les variables catégorielles."
tbl <- m1 |>
  gtsummary::tbl_regression(
  exponentiate = TRUE,
  intercept = FALSE) |>
  gtsummary::add_global_p() |>
  gtsummary::bold_labels()
tbl$table_styling$header$label[c(12,20,26,27)] <- c("variables","cote", "IC 95%","valeur-p")
tbl$table_body <- tbl$table_body[-(1:15),]
# tbl$table_body$label[1] <- "cst"
tbl$table_styling$footnote_abbrev$footnote <- 
  c("cote = rapport de cote",
    "IC = intervalle de confiance",
    "ET = erreur-type")
tbl |> 
   gtsummary::as_gt()  |>
   gt::tab_options(table.width = gt::pct(100))
```

# Multicolinéarité

Il est difficile de départager l'effet individuel d'une variable explicative lorsqu'elle est fortement corrélée avec d'autres.


La multicollinéarité ne dépend pas de la variable réponse $Y$, mais de la matrice $\mathbf{X}$ du modèle.

# Multicolinéarité pour PRCA

Mêmes diagnostics qu'en régression linéaire: considérer les facteurs d'inflation de la variance (`car::vif`).

```{r}
#| eval: true
#| echo: true
car::vif(modele2)
```

Pas d'inquiétude ici, coefficients faibles (inférieurs à 5)

# Dichotomiser des variables continues

Si $Y$ est continue et qu'on cherche à estimer $\Pr(Y> c \mid \mathbf{X})$ pour une valeur $c$ donnée, il n'est **pas** recommandé de dichotomiser $Y$ via

\begin{align*}
Y^{*} = \begin{cases}
1, & Y > c; \\
0, & Y \leq c.
\end{cases}
\end{align*}

et d'ajuster une régression logistique.

Pourquoi? **On perd de l'information**.

# Probabilité de dépassement

On peut estimer plutôt une régression linéaire et prendre 
$$\Pr(Y > c \mid \mathbf{X}) = \Phi\left(\frac{\widehat{\mu}-c}{\widehat{\sigma}}\right),$$

où 

- $\widehat{\mu}=\widehat{\beta}_0 +  \cdots + \beta_p\mathrm{X}_p$ est la moyenne prédite pour le profil donné, 
- $\widehat{\sigma}$ est l'estimation de l'écart-type
- $\Phi(\cdot)$ est la fonction de répartition d'une loi normale standard (`pnorm` dans **R**)

# Modèle linéaire et probabilité d'excès

```{r}
#| eval: true
#| echo: false
#| label: fig-density-normalcurves
#| out-width: '80%'
#| fig-cap: "Régression linéaire simple et densité normale à différentes valeurs de $x$."
## Sample data
set.seed(0)
dat <- data.frame(x=(x=runif(100, 0, 50)),
                  y=rnorm(100, 10*x, 100))

## breaks: where you want to compute densities
breaks <- seq(0, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))
ypos <- predict(lm(y~ x, data = dat), newdata = data.frame(x=breaks[-1]))
stdev <- sd(dat$res)
## Compute densities for each section, and flip the axes, and add means of sections
## Note: the densities need to be scaled in relation to the section size (2000 here)
xs <- seq(min(dat$y)-ypos[1]-100, max(dat$y)-ypos[4]+100, length.out = 200)
xs4 <- length(xs)*4L
dens <- data.frame(y = rep(xs, length.out = xs4) +
                       rep(ypos, each = length(xs)),
                   x = rep(breaks[-1], each = length(xs)) - 
                     rep(2000*dnorm(xs, 0, stdev), length.out = xs4),
                   section = factor(rep(levels(dat$section), each = length(xs))))

## Plot both empirical and theoretical
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_smooth(method="lm", fill=NA, lwd=2) +
  geom_path(data=dens, 
            aes(x, y, group=interaction(section)), lwd=1.1) +
  geom_vline(xintercept=breaks, lty=2) +
  labs(x = "", y = "") +
  theme_bw()
  
```


# Récapitulatif

- Une régression logistique sert à modéliser la moyenne de **variables catégorielles**, typiquement binaires.
- C'est un cas particulier d'un modèle de régression linéaire généralisée (GLM)

# Récapitulatif

Le modèle est interprétable à l'échelle de la cote

- La cote donne le rapport probabilité de réussite (1) sur probabilité d'échec (0)
- Interprétation en terme de 
   - pourcentage d'augmentation si $\exp(\widehat{\beta}) > 1$, avec $\exp(\widehat{\beta})-1$.
   - pourcentage de diminution si $\exp(\widehat{\beta}) < 1$, avec $1-\exp(\widehat{\beta})$

# Récapitulatif

- Estimation par maximum de vraisemblance
- Tests d'hypothèse comparent modèles emboîtés 
   - loi nulle asymptotique $\chi^2$
   - degrés de liberté égal au nombre de restrictions
- Intervalles de confiance de vraisemblance profilée
   - invariants aux reparamétrisations

