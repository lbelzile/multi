---
title: "Données manquantes"
subtitle: "Analyse multidimensionnelle appliquée"
date: "automne 2022"
author: "Léo Belzile"
institute: "HEC Montréal"
format: beamer
navigation: empty
colortheme: Flip
innertheme: Flip
outertheme: Flip
themeoptions: "bullet=circle, topline=true, shadow=false"
beamerarticle: false
pdf-engine: lualatex
code-line-numbers: true
fig-align: 'center'
mainfont: "D-DIN"
mathfont: 'Latin Modern Math'
sansfont: 'Latin Modern Sans'
keep-tex: true
include-in-header: 
      text: |
        \usepackage{tabu}
        \usepackage{mathtools}
        \usepackage{mathrsfs}
---

# Données manquantes

Plusieurs champs d'une base de donnée peuvent être manquants 

- non-réponse
- valeurs erronées (erreur d'encodage)
- perte de suivi et censure
- plusieurs versions de formulaires (question optionnelles)

# Pourquoi s'en préoccuper?

La plupart des procédures ne gèrent que les cas complets (toute observation avec des valeurs manquantes est éliminée).

Les données manquantes réduisent l'information disponible.

Sans traitement adéquat, les estimations seront **biaisées**.

# Références

- van Buuren, S. (2018). [*Flexible imputation of missing data*](https://stefvanbuuren.name/fimd/), CRC Press, 2e édition.
- Little, R. et D. Rubin (2019). *Statistical Analysis with Missing Data*, Wiley, 3e édition
- Schafer, J. L. (1997). *Analysis of Incomplete Multivariate Data*. Chapman & Hall / CRC.

Les valeurs manquantes dans un contexte de prédictions sont couvertes dans le cours MATH 60600.

# Classification des données manquantes (MCAR)

Cas 1: Données manquantes de façon complètement aléatoire (*missing completely at random*)

La probabilité que la valeur soit manquante ne dépend ni de la valeur, ni de celles des autres variables.

Exemple: questionnaire trop long, la personne ne répond pas à tout (sans lien avec les questions posées).

Hypothèse souvent irréaliste en pratique.

# Classification des données manquantes (MAR)

Cas 2: données manquantes de façon aléatoire (*missing at random*): la probabilité que la valeur soit manquante ne dépend pas de la valeur *une fois qu'on a contrôlé pour les autres variables*.

Exemple: les hommes sont plus susceptibles dans l'ensemble de divulguer leur âge que les femmes.

# Classification des données manquantes (MNAR)

Cas 3: données manquantes de façon non-aléatoire (*missing not at random*): la probabilité que la mesure soit manquante dépend de la valeur elle-même, pas déterminable avec d'autres variables

Exemple: une personne transgenre ne répond pas à la question genre (si seulement deux choix, homme/femme) et aucune autre question ne se rattache au genre ou à l'identité sexuelle. 

# Comment déterminer le type de données manquantes

Une personne ne divulgue pas son salaire? Données manquante de manière aléatoire ou non aléatoire?

Hypothèse pas testable, dépend du contexte et des variables auxiliaires disponibles.

# Valeur logique

Les données manquantes ont souvent une valeur logique:

- un client qui n'a pas de carte de crédit a un solde de 0!

D'où l'importance des validations d'usage et du nettoyage préliminaire de la base de données.


# Types de schémas de données manquantes

Matrice $n \times p$  (observations en lignes, variables en colonnes).

```{r}
#| eval: true
#| echo: false
#| out-width: '100%'
#| fig-height: 5
#| fig-width : 10
#| cache: true
library(patchwork)
library(ggplot2)
set.seed(1234)
tile <- expand.grid(1:8, 1:3)
colnames(tile) <- c("x", "y")
g1 <- ggplot(data = data.frame(tile, 
                               count = c(rep(0, 16), 
                                         rep(1, 3), 
                                         rep(0,5))),
             mapping = aes(y = factor(x), 
                           x = factor(y), 
                           fill = factor(count))) +
  geom_tile(colour = "white", linewidth = 2) +
  labs(x = "",
       y = "",
       fill = "",
       subtitle = "unidimensionnel") +
  scale_fill_discrete(palette = mice::mdc) + 
  scale_x_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  scale_y_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  theme_classic() +
  theme(legend.position = "none",
        axis.line = element_blank())

g2 <- ggplot(data = data.frame(tile, 
                               count = c(rep(0, 8), 
                                         rep(1, 2), 
                                         rep(0, 6), 
                                         rep(1, 4),
                                         rep(0, 4))),
             mapping = aes(y = factor(x), 
                           x = factor(y), 
                           fill = factor(count))) +
  geom_tile(colour = "white", linewidth = 2) +
  labs(x = "",
       y = "",
       fill = "",
       subtitle = "monotone") +
  scale_fill_discrete(palette = mice::mdc) + 
  scale_x_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  scale_y_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  theme_classic() +
    theme(legend.position = "none",
        axis.line = element_blank())
g3 <- ggplot(data = data.frame(tile, 
                               count = c(rep(0, 8), 
                                         rep(1, 5), 
                                         rep(0, 3), 
                                         rep(0, 5),
                                         rep(1, 3))),
             mapping = aes(y = factor(x), 
                           x = factor(y), 
                           fill = factor(count))) +
  geom_tile(colour = "white", linewidth = 2) +
  labs(x = "",
       y = "",
       fill = "",
       subtitle = "appariement") +
  scale_fill_discrete(palette = mice::mdc) + 
  scale_x_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  scale_y_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  theme_classic() +
    theme(legend.position = "none",
        axis.line = element_blank())
g4 <- ggplot(data = data.frame(tile, 
                               count = sample(c(0,1), 
                                              size = 24, 
                                              replace = TRUE, 
                                              prob = c(0.8, 0.2))),
             mapping = aes(y = factor(x), 
                           x = factor(y), 
                           fill = factor(count))) +
  geom_tile(colour = "white", linewidth = 2) +
  labs(x = "",
       y = "",
       fill = "",
       subtitle = "général") +
  scale_fill_discrete(palette = mice::mdc) + 
  scale_x_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  scale_y_discrete(breaks = NULL, expand = c(0,0), labels = NULL) +
  theme_classic() +
    theme(legend.position = "none",
        axis.line = element_blank())

g1 + g2 + g3 + g4 + plot_layout(nrow = 1, byrow = FALSE)
```

\footnotesize

Les cases grises représentent des valeurs manquantes. Illustration adapté de la Figure 4.1 de van Buuren (2022)

# Analyse des cas complet

Retirer les observations avec données manquantes pour conserver les cas complets.

- Valide uniquement pour complètement aléatoire.
- On perd de la précision en utilisant moins d'observations.

Méthode par défaut dans les logiciels.

# Méthodes d'imputation: imputation simple


Imputation: emplacer les valeurs manquantes par une valeur judicieuse pour *combler les trous*.

> Le concept d'imputation est à la fois séduisant et dangereux (Dempster et Rubin, 1983)

 Par exemple, on remplacer la valeur manquante par la moyenne (variables continues) ou le mode (variables catégorielles).

Approche pas recommandée (pourquoi?)


# Faut-il toujours imputer?

Il faut utiliser son jugement. 


Une observation imputée ne remplacera jamais une vraie observation.

- Si la proportion d'observations manquantes est petite (moins de 5\%), on pourrait faire une analyse avec les cas complets (et valider au besoin en utilisant l'imputation multiple).
- Si la proportion de  valeurs manquantes est 30\%  et que cette proportion baisse à 3\% lorsque vous éliminez quelques variables peu importantes pour votre étude, alors procédez à leur élimination.



# Imputation par la moyenne

Dilution de la relation (corrélation) entre variables explicatives. Réduction de la variabilité.

```{r}
#| echo: false
#| eval: true
#| out-width: '90%'
#| fig-width: 8
#| fig-align: 'center'
#| fig-height: 4
library(ggplot2)
set.seed(1234)
nobs <- 400L
ventes <- runif(n = nobs, min = 400, max = 800) + 
  rlnorm(n = nobs, meanlog = 4, sdlog = 1) -100 + 
  TruncatedNormal::rtnorm(n = 1, 
              mu = seq(100, to = 300, length.out = nobs), 
              sd = seq(0, 100, length.out = nobs),
              lb = 0, ub = 1000)
nbvisiteurs <- sort(rpois(n = nobs, lambda = 40))
full <- data.frame(ventes = ventes, 
                   nbclients = nbvisiteurs)
manquants <- as.matrix(full)
amputer <- mice::ampute(data = manquants, prop = 0.25)
# manquants[sample(x = c(TRUE, FALSE),
#                          size = 2*nobs, 
#                          replace = TRUE,
#                          prob = c(0.15,0.85))] <- NA
manquants1 <- amputer$amp |> 
  tibble::as_tibble() |>
  dplyr::mutate(
    manquant = dplyr::case_when(
      is.na(nbclients) | is.na(ventes) ~ 1,
      TRUE ~ 2),
    manqventes = dplyr::case_when(
      is.na(ventes) ~ 1,
      TRUE ~ 2), 
    manqnbclients = dplyr::case_when(
      is.na(nbclients) ~ 1,
      TRUE ~ 2),
    ventes = dplyr::case_when(
      is.na(ventes) ~ mean(ventes, na.rm = TRUE),
      TRUE ~ ventes),
    nbclients = dplyr::case_when(
      is.na(nbclients) ~ mean(nbclients, na.rm = TRUE),
      TRUE ~ nbclients))
g1 <- ggplot(data = manquants1,
       aes(x = nbclients, 
           y = ventes, 
           color = factor(1-manquant),
           shape = factor(manquant))) + 
  geom_point(size = 2) + 
    scale_shape_manual(values = c(1,20)) +
  scale_colour_discrete(palette = mice::mdc) +
  labs(y = "ventes",
       x = "nombre de clients") +
  theme_classic() +
  theme(legend.position = "none")

g2 <- ggplot(data = manquants1,
       aes(x = ventes,
           #y = after_stat(density),
           fill = factor(1-manqventes),
           group = factor(1-manqventes))) + 
  geom_histogram(alpha = 0.8) +
  scale_y_continuous(expand = c(0,0)) + 
  scale_fill_discrete(palette = mice::mdc) +
  labs(y = "") +
  theme_classic() +
  theme(legend.position = "none")
library(patchwork)
g2 + g1 
```

# Modèles prédictifs

L'imputation par régression (gauche) mène à une sous-estimation de l'incertitude en raison de l'augmentation de la corrélation, contrairement à l'imputation aléatoire (droite).


```{r}
#| echo: false
#| eval: true
#| out-width: '90%'
#| fig-width: 8
#| fig-align: 'center'
#| fig-height: 4
#| cache: true
manquants2 <- manquants1 |>
  dplyr::mutate(
   ventes = dplyr::case_when(
      manqventes == 1 & manqnbclients == 2 ~
        predict(lm(ventes ~ nbclients, 
                   data = manquants1)),
      manqventes == 1 & manqnbclients == 1 ~ mean(ventes, na.rm = TRUE),
      TRUE ~ ventes
    ),
   nbclients = dplyr::case_when(
      manqventes == 2 & manqnbclients == 1 ~
        predict(lm(nbclients ~ ventes,
                   data = manquants1)),
      manqventes == 1 & manqnbclients == 1 ~ mean(nbclients, na.rm = TRUE),
      TRUE ~ nbclients
    )
  )
g1 <- ggplot(data = manquants2,
       aes(x = nbclients, 
           y = ventes, 
           color = factor(1-manquant),
           shape = factor(manquant))) + 
  geom_point(size = 2) + 
    scale_shape_manual(values = c(1,20)) +
  scale_colour_discrete(palette = mice::mdc) +
  labs(y = "ventes",
       x = "nombre de clients") +
  theme_classic() +
  theme(legend.position = "none")
# 
# g2 <- ggplot(data = manquants2,
#        aes(x = ventes,
#            #y = after_stat(density),
#            fill = factor(manqventes),
#            group = factor(manqventes))) + 
#   geom_histogram(alpha = 0.5,) +
#   scale_y_continuous(expand = c(0,0)) + 
#   labs(y = "") +
#   theme_classic() +
#   theme(legend.position = "none")
g3 <- ggplot(data = data.frame(cbind(
  mice::complete(
    mice::mice(manquants[,1:2], 
               m = 1, 
               printFlag = FALSE), 1),
  manquant = manquants1$manquant)),
      mapping = aes(x = nbclients, 
           y = ventes, 
           color = factor(1-manquant),
           shape = factor(manquant))) + 
  geom_point(size = 2) + 
  scale_colour_discrete(palette = mice::mdc) +
    scale_shape_manual(values = c(1,20)) +
  labs(y = "ventes",
       x = "nombre de clients") +
  theme_classic() +
  theme(legend.position = "none")

g1 + g3
```


# Imputation aléatoire

Considérons le cas d'une régression logistique pour une variable explicative binaire.

Plutôt que d'assigner à la classe la plus probable, une prédiction aléatoire simule une variable 0/1 avec probabilité $(1-\widehat{p}_i, \widehat{p}_i)$.

```{r}
#| eval: true
#| echo: true
pred <- 0.3 #probabilité de succès
rbinom(n = 15, size = 1, prob = pred)
```

# Problèmes de l'imputation simple

On ne tient pas compte du fait que des valeurs ont été remplacées (on fait comme si c'était de vraies observations). 

On sous-évalue encore une fois la **variabilité** des données

- les écarts-type des estimations sont trop petits.

# Inspection des valeurs manquantes

Il est donc nécessaire d'examiner la configuration des valeurs manquantes avant de faire quoi que ce soit. 

```{r}
#| label: manquantes-summary-uni
#| echo: true
#| eval: false
data(manquantes, package = 'hecmulti')
summary(manquantes)
# Pourcentage de valeurs manquantes
apply(manquantes, 2, function(x){mean(is.na(x))})
# Voir les configurations de valeurs manquantes
md.pattern(manquantes) # graphique diapo suivante
```

```{r}
#| label: tbl-manquantes-univ
#| cache: true
#| eval: true
#| echo: false
#| tbl-cap: "Nombre et pourcentage de valeurs manquantes par variable."
data(manquantes, package = 'hecmulti')
manq <- formatC(apply(manquantes, 2, 
                      function(x){c(sum(is.na(x)), 100*mean(is.na(x)))}))
rownames(manq) <- c("nombre", "pourcentage")
knitr::kable(manq,
             booktabs = TRUE) |>
  kableExtra::kable_styling()
```

# Configuration des valeurs manquantes

```{r}
#| label: fig-manquantes2
#| echo: false
#| eval: true
#| out-width: '50%'
#| fig-cap: "Configurations des valeurs manquantes pour  `manquantes`."
puzle <- mice::md.pattern(hecmulti::manquantes[,1:6], 
                          plot = TRUE)
```





# Imputation multiple

Valides pour les données manquantes de manière (complètement) aléatoires.

1. Procéder à plusieurs imputations **aléatoires** pour obtenir un échantillon complet (`mice`)
2. Ajuster le modèle d'intérêt avec chaque échantillon (`with`). 3. Combiner les résultats obtenus (`pool` et `summary`)

```{r}
#| eval: true
#| echo: false
#| out-width: '70%'
#| fig-align: 'center'
knitr::include_graphics("figures/donnees_manquantes_workflow.pdf")
```

# Combinaison des résultats

Considérons un seul paramètre $\theta$ (ex: coefficient d'une régression) et supposons qu'on procède à $K$ imputations.

On estime les paramètres du modèle séparément pour chacun des $K$ ensembles de données imputés, disons 

- $\widehat{\theta}_k$ pour l'estimation du paramètre $\theta$ dans l'échantillon $k$ et
- $\widehat{\sigma}_k^2=\mathsf{Va}(\widehat{\theta}_k)$ pour l'estimation de la variance de $\widehat{\theta}_k$. 

# Estimation du paramètre moyen

L'estimation finale de $\theta$, dénotée $\widehat{\theta}$, est obtenue tout simplement en faisant la moyenne des estimations de tous les modèles, c'est-à-dire,
\begin{align*}
\widehat{\theta} = \frac{\widehat{\theta}_1 + \cdots + \widehat{\theta}_K}{K}.
\end{align*}

# Estimation des erreurs-types

Une estimation ajustée de la variance de $\widehat{\theta}$ est 
\begin{align*}
\mathsf{Va}(\hat{\theta}) &= W+ \frac{K+1}{K}B, 
\intertext{où}
W &= \frac{1}{K} \sum_{k=1}^K \widehat{\sigma}^2_k = \frac{\widehat{\sigma}_1^2 + \cdots + \widehat{\sigma}_K^2}{K},\\
B &= \frac{1}{K-1} \sum_{k=1}^K (\widehat{\theta}_k - \widehat{\theta})^2.
\end{align*}

- $W$ est la moyenne des variances (variance intra-groupe) et
- $B$ la variance des moyennes (variance inter-groupe).

# Imputation multiple

Des formules analogues existent pour les degrés de liberté, les valeurs-$p$, etc. ainsi que pour la cas multidimensionnel (plusieurs paramètres).

Si on procédait à une seule imputation (même en ajoutant une part d'aléatoire pour essayer de reproduire la variabilité des données), on ne serait pas en mesure d'estimer la variance inter-groupe de l'estimateur.


On peut estimer la fraction de l'information manquante sur $\theta$ avec $(1+1/K)B/\mathsf{Va}(\hat{\theta})$.


# Imputation multiple par équations chaînées (MICE)

Avec $p$ variables $X_1, \ldots, X_p$, spécifier un ensemble de modèles **conditionnels** pour chaque variable $X_j$ en fonction de

- toutes les autres variables, $\boldsymbol{X}_{-j}$
- les valeurs observées pour cette variable, $X_{j, \text{obs}}$

1. Initialisation: remplir les trous avec des données au hasard parmi $X_{j, \text{obs}}$ pour $X_{j, \text{man}}$
2. À l'itération $t$, pour chaque variable $j=1, \ldots, p$, à tour de rôle:
   a) tirage aléatoire des paramètres $\phi_j^{(t)}$ du modèle pour $X_{j,\text{man}}$ conditionnel à $\boldsymbol{X}_{-j}^{(t-1)}$ et  $X_{j, \text{obs}}$
   b) échantillonnage de nouvelles observations $X^{(t)}_{j,\text{man}}$ du modèle avec paramètres $\phi_j^{(t)}$ conditionnel à $\boldsymbol{X}_{-j}^{(t-1)}$ et  $X_{j, \text{obs}}$
3. Répéter le cycle



# Imputation multiple avec `mice`



```{r}
#| label: manquante2
#| eval: false
#| echo: true
library(mice)
# Intensif en calcul, réduire "m" si nécessaire
impdata <- mice(
   data = manquantes,
   # argument "method" pour le modèle
   # dépend du type des variables, par ex.
   # régression logistique pour données binaires
   m = 50, # nombre d'imputations
   seed = 60602, # germe aléatoire
   printFlag = FALSE)
# Extraite une copie (m=1,..., 50) imputée 
complete(data = impdata, 
         action = 1) #no de la copie
```

# Estimation et combinaison avec `mice`

```{r}
#| eval: false
#| echo: true
# ajuste le modèle avec les données imputées
adj_im <- with(
  data = impdata,
  expr = glm(y ~ x1 + x2 + x3 + x4 + x5 + x6,
             family = binomial))
# combinaison des résultats 
fit <- pool(adj_im)
summary(fit)
```


# Résultats

```{r}
#| label: manquantsres
#| eval: true
#| echo: false
#| cache: true
library(mice)
# Intensif en calcul, réduire "m" si nécessaire
impdata <- mice(data = hecmulti::manquantes,
                # argument method pour le type de modèles
                # selon les variables
                m = 50, # nombre d'imputations
                seed = 60602, # germe aléatoire
                printFlag = FALSE)

# ajuste le modèle avec les données imputées
adj_im <- with(
  data = impdata,
  expr = glm(y ~ x1 + x2 + x3 + x4 + x5 + x6,
             family = binomial))
# combinaison des résultats 
fit <- pool(adj_im)
fit_df <- tibble::as_tibble(summary(fit))
fit_df$p.value <- format.pval(fit_df$p.value,
                              digits = 1, 
                              eps = 1e-4)
knitr::kable(fit_df, 
             digits = 2, 
             booktabs = TRUE,
             linesep = "",
             col.names = c("terme","estimation", "erreur-type", "stat", "ddl", "valeur-p")) |>
  kableExtra::kable_styling()
```

# Récapitulatif

- Les données manquantes réduisent la quantité d'information disponible et augmentent l'incertitude.
- On ne peut **pas** les ignorer (étude des cas complets) sans biaiser les interprétations et réduire la quantité d'information disponible.
- Pour bien capturer l'**incertitude** et ne pas modifier les relations entre variables, il faut utiliser une méthode **aléatoire**.
- Avec l'algorithme MICE, on utilise un modèle conditionnel pour chaque variable à tour de rôle

# Récapitulatif

L'imputation multiple est préférée à l'imputation simple car elle permet d'estimer l'incertitude sous-jacente en raison des données manquantes.

- On procède à l'imputation plusieurs fois (avec un modèle conditionel, prédictions différentes chaque fois)
- on crée plusieurs copies
- ajuste le modèle sur chacune et 
- combine les résultats

Traitement spécial pour erreurs-type, degrés de liberté, valeurs-$p$ et intervalles de confiance.
