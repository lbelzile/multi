% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{D-DIN}
  \setsansfont[]{Latin Modern Sans}
  \setmathfont[]{Latin Modern Math}
\fi
\usecolortheme{Flip}
\usefonttheme{serif} % use mainfont rather than sansfont for slide text
\useinnertheme{Flip}
\useoutertheme{Flip}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{tabu}
\usepackage{mathtools}
\usepackage{mathrsfs}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Analyse de regroupements},
  pdfauthor={Léo Belzile},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Analyse de regroupements}
\subtitle{Analyse multidimensionnelle appliquée}
\author{Léo Belzile}
\date{automne 2022}
\institute{HEC Montréal}

\begin{document}
\frame{\titlepage}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, boxrule=0pt, breakable, enhanced, borderline west={3pt}{0pt}{shadecolor}, interior hidden, sharp corners]}{\end{tcolorbox}}\fi

\begin{frame}{Algorithmes pour l'analyse de regroupements}
\protect\hypertarget{algorithmes-pour-lanalyse-de-regroupements}{}
L'analyse de regroupements cherche à créer une division de \(n\)
observations de \(p\) variables en regroupements.

\begin{enumerate}
\tightlist
\item
  méthodes basées sur la connectivité (regroupements hiérarchiques,
  AGNES et DIANA)
\item
  méthodes basées sur les centroïdes et les médoïdes (\(k\)-moyennes,
  \(k\)-médoides PAM, CLARA)
\item
  mélanges de modèles (mélanges Gaussiens, etc.)
\item
  méthodes basées sur la densité (DBScan)
\item
  méthodes spectrales
\end{enumerate}
\end{frame}

\begin{frame}{Algorithme de partition autour des médoïdes (PAM)}
\protect\hypertarget{algorithme-de-partition-autour-des-muxe9douxefdes-pam}{}
\begin{enumerate}
\tightlist
\item
  Initialisation: sélectionner \(K\) des \(n\) observations comme
  médoïdes initiaux.
\item
  Assigner chaque observation au médoïde le plus près.
\item
  Calculer la dissimilarité totale entre chaque médoïde et les
  observations de son groupe.
\item
  Pour chaque médoïde \((k=1, \ldots, K\)):

  \begin{itemize}
  \tightlist
  \item
    considérer tous les \(n-K\) observations à tour de rôle et permuter
    le médoïde avec l'observation.\\
  \item
    calculer la distance totale et sélectionner l'observation qui
    diminue le plus la distance totale.
  \end{itemize}
\item
  Répéter les étapes 2 à 4 jusqu'à ce que les médoïdes ne changent plus.
\end{enumerate}
\end{frame}

\begin{frame}{Algorithme CLARA (1/2)}
\protect\hypertarget{algorithme-clara-12}{}
L'algorithme CLARA, décrit dans Kaufman \& Rousseeuw (1990), réduit le
coût de calcul et de stockage.

\begin{itemize}
\tightlist
\item
  Diviser l'échantillon en \(S\) sous-échantillons de taille
  approximativement égale de taille \(n_S\) (typiquement
  \(K \ll n_S < 1000\))
\item
  Utiliser l'algorithme PAM sur chaque sous-échantillon.
\end{itemize}

Une fois les médoïdes obtenus, le reste de toutes les observations de
l'échantillon sont assignées au regroupement du médoïde le plus près.
\end{frame}

\begin{frame}{Algorithme CLARA (2/2)}
\protect\hypertarget{algorithme-clara-22}{}
La qualité de la segmentation pour chacune des \(S\) segmentations est
calculée en obtenant la distance moyenne entre les médoïdes et les
observations.

On retourne la meilleure segmentation parmi les \(S\) (celle qui a la
plus petite distance moyenne).
\end{frame}

\begin{frame}[fragile]{PAM et CLARA dans \textbf{R}}
\protect\hypertarget{pam-et-clara-dans-r}{}
Disponible depuis le paquet \texttt{cluster}.

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{60602}\NormalTok{)}
\NormalTok{kmedoide5 }\OtherTok{\textless{}{-}}\NormalTok{ cluster}\SpecialCharTok{::}\FunctionTok{clara}\NormalTok{(}
   \AttributeTok{x =}\NormalTok{ donsmult\_std,}
   \AttributeTok{k =}\NormalTok{ 5L, }\CommentTok{\# nombre de groupes}
   \AttributeTok{sampsize =} \DecValTok{500}\NormalTok{, }\CommentTok{\#taille échantillon pour PAM}
   \AttributeTok{metric =} \StringTok{"euclidean"}\NormalTok{, }\CommentTok{\# distance l2}
   \CommentTok{\#cluster.only = TRUE, \# ne conserver que étiquettes}
   \AttributeTok{rngR =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# germe aléatoire depuis R}
   \AttributeTok{pamLike =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# même algorithme que PAM}
   \AttributeTok{samples =} \DecValTok{10}\NormalTok{) }\CommentTok{\#nombre de répétitions aléatoires}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Valeurs initiales et paramètres}
\protect\hypertarget{valeurs-initiales-et-paramuxe8tres}{}
Même hyperparamètres que \(K\)-moyennes (dissemblance, nombre de
regroupements, initialisation et séparation).

Comme les \(K\)-moyennes, on fera plusieurs essais pour trouver de
bonnes valeurs de départ. On peut tracer le profil des silhouettes.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-clarasilhouette-1.pdf}

}

\caption{\label{fig-clarasilhouette}Silhouettes pour les données de dons
multiples avec l'algorithme CLARA pour \(K=5\) regroupements.}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Prototypes}
\protect\hypertarget{prototypes}{}
Puisque les prototypes (médoïdes) sont des observations, on peut
simplement extraire leur identifiant.

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{medoides\_orig }\OtherTok{\textless{}{-}}\NormalTok{ donsmult[kmedoide5}\SpecialCharTok{$}\NormalTok{i.med,]}
\CommentTok{\# Taille des regroupements}
\NormalTok{kmedoide5}\SpecialCharTok{$}\NormalTok{clusinfo}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Avantages et inconvénients des \(K\)-médoïdes}
\protect\hypertarget{avantages-et-inconvuxe9nients-des-k-muxe9douxefdes}{}
\begin{itemize}
\tightlist
\item
  (\(-\)) solution approximative pour grand échantillons
\item
  (\(+\)) les prototypes sont des observations de l'échantillon.
\item
  (\(+\)) la fonction objective est moins impactée par les extrêmes.
\item
  (\(-\)) le coût de calcul est prohibitif avec des mégadonnées
  (problème combinatoire) avec complexité \(\mathrm{O}(n^2)\). PAM
  fonctionne avec maximum 1000 observations.
\end{itemize}
\end{frame}

\begin{frame}{Mélange de modèles}
\protect\hypertarget{muxe9lange-de-moduxe8les}{}
On suppose qu'on a \(K\) groupes, chacun caractérisé par une densité de
dimension \(p\), soit \(f_k(\boldsymbol{X}_i;\boldsymbol{\theta}_k)\) si
\(\boldsymbol{X}_i\) provient du groupe \(k=1, \ldots, K\).

Généralement, on choisit une loi normale multidimensionnelle pour le
\(k\)e groupe \(G\), \begin{align*}
\boldsymbol{X} \mid G=k \sim \mathsf{No}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{align*}

La probabilité qu'une observation soit tirée du groupe \(G=k\) est
\(\pi_k\).
\end{frame}

\begin{frame}{Estimation du mélange de modèle}
\protect\hypertarget{estimation-du-muxe9lange-de-moduxe8le}{}
La vraisemblance est une fonction des paramètres \(\boldsymbol{\mu}_k\),
\(\boldsymbol{\Sigma}_k\) et de la probabilité \(\pi_k\) qu'une
observation \(\mathbf{X}_i\) tombe dans le groupe \(k\), \begin{align*}
 L_i(\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \boldsymbol{\pi}_k\}_{k-1}^K; \mathbf{X}_i)= \sum_{k=1}^K\pi_k
f_{k}(\boldsymbol{X}_i \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
\end{align*}

Le maximum de vraisemblance est obtenu à l'aide de l'algorithme
d'espérance-maximisation en augmentant les observations avec un
indicateur de groupe.

\begin{itemize}
\tightlist
\item
  Étape E: assignation aux groupes (multinomiale).
\item
  Étape \(M\): estimation des probabilités, des moyennes et variances.
\end{itemize}

Le mélange de modèle nous donne accès à la probabilité \(\pi_k\) qu'une
observation appartiennent au groupe \(G_k\) (\textbf{assignation
probabiliste}).
\end{frame}

\begin{frame}{Fléau de la dimension}
\protect\hypertarget{fluxe9au-de-la-dimension}{}
Chacune des \(K\) matrice de covariance contient \(p(p+1)/2\)
paramètres!

En paramétrisant ces dernière, on peut réduire le nombre de paramètres à
estimer.

\begin{itemize}
\tightlist
\item
  compromis entre simplicité (d'estimation) et nombre de paramètres
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Paramétrisation des matrices de covariance}
\protect\hypertarget{paramuxe9trisation-des-matrices-de-covariance}{}
La matrice de covariance dans \texttt{mclust} est paramétrisée en
fonction de

\begin{itemize}
\tightlist
\item
  \(\lambda\), qui contrôle le volume,
\item
  une matrice diagonale \(\mathbf{A}\) qui contrôle les variances de
  chaque observation et
\item
  \(\mathbf{D}\) une matrice orthogonale qui permet de créer de la
  corrélation entre observations.
\end{itemize}

Un index \(k\) spécifie que cette composante varie d'un regroupement à
l'autre.
\end{frame}

\begin{frame}[fragile]{Paramétrisation des variables}
\protect\hypertarget{paramuxe9trisation-des-variables}{}
Voir \texttt{mclust.options("emModelNames")} et la documentation dans le
Tableau 3 de
\href{https://journal.r-project.org/archive/2016/RJ-2016-021/RJ-2016-021.pdf}{l'article
sur \texttt{mclust}}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{figures/mclust5-parametrization.png}

}

\caption{\label{fig-modeles}Forme des ellipsoïdes pour le mélange de
modèle selon la forme de la structure de covariance. Tirée de
\href{(https://journal.r-project.org/archive/2016/RJ-2016-021/RJ-2016-021.pdf)}{\texttt{mclust5}},
licence \href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0}.}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Paquet \texttt{mclust}}
\protect\hypertarget{paquet-mclust}{}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\DocumentationTok{\#\# Mélanges de modèles gaussiens}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{60602}\NormalTok{)}
\FunctionTok{library}\NormalTok{(mclust)}
\NormalTok{mmg }\OtherTok{\textless{}{-}} \FunctionTok{Mclust}\NormalTok{(}\AttributeTok{data =}\NormalTok{ donsmult\_std,}
       \AttributeTok{G =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
       \CommentTok{\# Ajouter composante uniforme}
       \CommentTok{\#  pour bruit (aberrances)}
       \AttributeTok{initialization =} \FunctionTok{list}\NormalTok{(}\AttributeTok{noise =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\# Résumé de la segmentation}
\FunctionTok{summary}\NormalTok{(mmg)}
\end{Highlighting}
\end{Shaded}

On peut obtenir les étiquettes (avec \texttt{0} pour le bruit) avec
\texttt{mmg\$classification}.
\end{frame}

\begin{frame}{Hyperparamètres}
\protect\hypertarget{hyperparamuxe8tres}{}
\begin{itemize}
\tightlist
\item
  le nombre de regroupements \(K\)
\item
  la forme des ellipsoïdes (structure de covariance)
\item
  les valeurs pour l'initialisation.
\end{itemize}

Les mêmes considérations pratiques qu'avec les \(K\)-moyennes
s'appliquent.

En pratique, on ajuste le modèle avec différent nombre de regroupements
et différentes structures de covariance et on prend le modèle avec le
plus petit BIC.
\end{frame}

\begin{frame}[fragile]{Sélection des hyperparamètres}
\protect\hypertarget{suxe9lection-des-hyperparamuxe8tres}{}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{plot}\NormalTok{(mmg, }\AttributeTok{what =} \StringTok{"BIC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\footnotesize

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-mclustbic-1.pdf}

}

\caption{\label{fig-mclustbic}Valeur du négatif du BIC pour les mélanges
de modèles gaussiens selon le nombre de regroupements et la structure de
covariance.}

\end{figure}

Le logiciel n'arrive pas à estimer certains modèles complexes quand le
nombre de groupes est trop élevé.
\end{frame}

\begin{frame}{Représentation graphique des regroupements}
\protect\hypertarget{repruxe9sentation-graphique-des-regroupements}{}
\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-classifreducmclust-1.pdf}

}

\caption{\label{fig-classifreducmclust}Projection des observations,
colorées par regroupement (gauche) et structure des regroupements avec
ellipsoides de confiance (droite).}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Avantages et inconvénients des mélanges de
modèles}
\protect\hypertarget{avantages-et-inconvuxe9nients-des-muxe9langes-de-moduxe8les}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) approche est plus flexible que les \(K\)-moyennes.
\item
  (\(+\)) l'ajout d'une composante uniforme permet de gérer les
  aberrances (supporté par \texttt{mclust}).
\item
  (\(+\)) l'algorithme EM garantie la convergence à un minimum local
  (comme pour les \(K\)-moyennes)
\item
  (\(+\)) on obtient une assignation probabiliste plutôt que rigide
\item
  (\(-\)) le coût de calcul est plus élevé que les \(K\)-moyennes
\item
  (\(-\)) le nombre de paramètre des matrices de covariance augmente
  rapidement avec la dimension \(p\).
\end{itemize}
\end{frame}

\begin{frame}{Méthodes basées sur la densité}
\protect\hypertarget{muxe9thodes-basuxe9es-sur-la-densituxe9}{}
L'algorithme DBSCAN (\emph{density-based spatial clustering of
applications with noise}) est une méthode de partitionnement basée sur
la densité des points.

L'idée de base de l'algorithme est de tracer une boule de rayon
\(\epsilon\) autour de chaque observation et de voir si elle inclut
d'autres observations.
\end{frame}

\begin{frame}{Mécanisme d'assignation}
\protect\hypertarget{muxe9canisme-dassignation}{}
L'algorithme classe les observations en trois catégories:

\begin{itemize}
\tightlist
\item
  Un point central est une observation qui possède \(M-1\) voisins à
  distance \(\epsilon\).
\item
  Un point frontière est un point qui est distant de moins de
  \(\epsilon\) d'un point central, sans en être un.
\item
  Un point isolé est une observation qui n'est pas rattachée à aucun
  regroupement.
\end{itemize}
\end{frame}

\begin{frame}{Algorithme DBSCAN}
\protect\hypertarget{algorithme-dbscan}{}
L'algorithme répète les étapes suivantes jusqu'à ce que chaque
observation ait été visitée.

\begin{enumerate}
\tightlist
\item
  Choisir un point aléatoirement parmi ceux qui n'ont pas été visités.
\item
  Si le point n'est pas étiqueté, calculer le nombre de points voisins
  qui se trouvent dans un rayon \(\epsilon\): s'il y a moins de \(M\)
  observations, provisoirement étiqueter l'observation comme point
  isolé, sinon comme point central.
\item
  Si l'observation est un point central avec \(M-1\) voisins ou plus,
  créer un regroupement.
\item
  Étiqueter chaque point à distance \(\epsilon\) créé et l'ajouter au
  regroupement, ainsi que tout point à distance \(\epsilon\) de ces
  voisins.
\end{enumerate}

\href{https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/}{Ce
site web} offre une visualisation interactive des différentes étapes de
l'algorithme et de comparer la performance de DBSCAN selon le type de
regroupements.
\end{frame}

\begin{frame}{Hyperparamètres}
\protect\hypertarget{hyperparamuxe8tres-1}{}
\begin{itemize}
\tightlist
\item
  le rayon \(\epsilon\) et
\item
  le nombre minimal de points pour former un regroupement, \(M\)
\end{itemize}

Hyperparamètres \(M\) et \(\epsilon\) corrélés: si on augmente le nombre
minimal de point \(M\) par regroupement, il faudra également augmenter
le rayon \(\epsilon\) pour éviter d'avoir un nombre trop élevé de points
isolés et d'aberrances.
\end{frame}

\begin{frame}{Classification avec DBSCAN}
\protect\hypertarget{classification-avec-dbscan}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-dbscan1-1.pdf}

}

\caption{\label{fig-dbscan1}Illustration de la classification des points
avec DBSCAN: toutes les observations sont assignées à un regroupement,
moins une aberrance.}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Choix des hyperparamètres}
\protect\hypertarget{choix-des-hyperparamuxe8tres}{}
Avec \(p\) variables explicatives, on recommande \(M > p+1\).

Pour \(\varepsilon\), considérer les \(M\) plus proches voisins.

La fonction \texttt{kNNdistplot} du paquet \texttt{dbscan} permet de
tracer un graphique de la distance moyenne des \(k\) plus proches
voisins pour chaque observation:

\begin{itemize}
\tightlist
\item
  en prenant \(k=M-1\), calculer la distance entre le \(k\) plus proche
  voisin de chaque observation.
\item
  ordonner ces distances.
\item
  choisir \(\epsilon\) selon coude
\end{itemize}
\end{frame}

\begin{frame}{Critère du coude}
\protect\hypertarget{crituxe8re-du-coude}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-dbscan2-1.pdf}

}

\caption{\label{fig-dbscan2}Graphique des distances entre chaque
observation et son troisième plus proche voisin (gauche), en fonction du
pourcentage d'observations à moins de cette distance et regroupements
obtenus avec DBSCAN avec \(M=10\) et \(\epsilon=1.1\) (droite).}

\end{figure}
\end{frame}

\begin{frame}{Avantages et inconvénients de DBSCAN (1/2)}
\protect\hypertarget{avantages-et-inconvuxe9nients-de-dbscan-12}{}
\begin{itemize}
\tightlist
\item
  (\(-\)) le traitement des aberrances est automatique et l'algorithme
  est robuste.
\item
  (\(+\)) le nombre de regroupements n'a pas à être spécifié apriori.
\item
  (\(+\)) la forme des regroupements est arbitraire, peut être non
  convexe et de taille différente.
\item
  (+/-) la complexité de l'algorithme est d'au mieux
  \(\mathrm{O}(n\ln n)\) (calcul) et \(\mathrm{O}(n)\) pour le stockage
  puisque chaque point est visité à tour de rôle et comparé aux autres
  pour trouver les plus proches voisins.
\end{itemize}
\end{frame}

\begin{frame}{Avantages et inconvénients de DBSCAN (2/2)}
\protect\hypertarget{avantages-et-inconvuxe9nients-de-dbscan-22}{}
\begin{itemize}
\tightlist
\item
  (\(-\)) les hyperparamètres ont une interprétation physique
\item
  (\(-\)) leur choix n'est pas aisé
\item
  (\(-\)) DBSCAN ne permet pas de traiter le cas où la densité des
  regroupements change et risque de fusionner des regroupements s'il y a
  une série d'observations qui permet de relier deux regroupements.
\item
  (\(-\)) comme la plupart des algorithmes, le voisinage des points
  devient épars quand \(p\) augmente en raison du fléau de la dimension.
\end{itemize}
\end{frame}

\begin{frame}{Regroupements hiérarchiques}
\protect\hypertarget{regroupements-hiuxe9rarchiques}{}
Méthode déterministe de regroupement à partir d'une matrice de
dissimilarité.

\begin{enumerate}
\tightlist
\item
  Initialisation: chaque observation est assignée à son propre groupe.
\item
  les deux groupes les plus rapprochés sont fusionnés; la distance entre
  le nouveau groupe et les autres regroupements est recalculée.
\item
  on répète l'étape 2 jusqu'à obtenir un seul regroupement.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]{Fonction de liaison}
\protect\hypertarget{fonction-de-liaison}{}
Il y a plusieurs façons de calculer la distance entre deux groupes
d'observations de plusieurs observations, notamment

\begin{itemize}
\tightlist
\item
  liaison simple (\texttt{method\ =\ single}): plus proches voisins
\item
  liaison complète (\texttt{method\ =\ complete}): voisins les plus
  éloignés
\item
  liaison moyenne (\texttt{method\ =\ average}): utilise la moyenne des
  distances entre toutes les paires de sujets (un pour chaque groupe)
  provenant des deux groupes.
\item
  méthode de Ward (\texttt{method\ =\ ward.D2}): calcul de l'homogénéité
  globale
\end{itemize}
\end{frame}

\begin{frame}{Illustration des mesures de liaison}
\protect\hypertarget{illustration-des-mesures-de-liaison}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-distances-1.pdf}

}

\caption{\label{fig-distances}Distances entre regroupements selon la
liaison (simple, complète, barycentre, homogenéité de Ward).}

\end{figure}
\end{frame}

\begin{frame}{Méthode de Ward}
\protect\hypertarget{muxe9thode-de-ward}{}
La méthode de Ward utilise l'homogénéité comme critère.

Pour chaque groupe, on calcule la somme des carrés des distances par
rapport à la moyenne du groupe, disons \(\mathsf{SCD}_k\)
(\(k=1, \ldots, M\)).

On calcule ensuite l'homogénéité globale en faisant la somme,
\(\mathrm{H}^{(M)} = \mathsf{SCD}_1 + \cdots + \mathsf{SCD}_M\).

La méthode de Ward va fusionner les deux groupes qui feront augmenter le
moins possible l'homogénéité.
\end{frame}

\begin{frame}[fragile]{Méthodes hiérarchiques et coût de calcul}
\protect\hypertarget{muxe9thodes-hiuxe9rarchiques-et-couxfbt-de-calcul}{}
Les algorithmes de regroupement hiérarchiques stockent une matrice de
dissemblance \(n \times n\): coût de stockage quadratique
\(\mathrm{O}(n^2)\).

Généralement, le coût de calcul est au mieux \(\Omega(n^2)\) et au pire
\(\mathrm{O}(n^3)\).

Pour la méthode de liaison simple, un algorithme permet d'obtenir un
coût de calcul quadratique de \(\mathrm{O}(n^2)\) sans stocker la
matrice de dissemblance, d'où un coût de stockage linéaire de
\(\mathrm{O}(n)\).

\texttt{stat::hclust} permet de faire des regroupements agglomératifs,
mais le paquet \texttt{fastcluster} propose une version avec une
empreinte mémoire inférieure (*plus rapide!)
\end{frame}

\begin{frame}[fragile]{Genie}
\protect\hypertarget{genie}{}
Alternative de Gagolewski (2016) qui modifie la fonction de liaison
simple en retenant son efficacité de calcul.

Plutôt que de simplement trouver la paire de regroupements à distance
minimale, cette fusion n'est appliquée que si une mesure d'inéquité est
inférieur à un seuil spécifié par l'utilisateur.

Si les regroupements sont fortement inéquitables, la fusion survient
entre les regroupements dont un de la taille minimale courante.

L'implémentation \textbf{R} dans le paquet \texttt{genieclust} est
nettement plus rapide que les autres alternative.
\end{frame}

\begin{frame}{Performance relative}
\protect\hypertarget{performance-relative}{}
\begin{itemize}
\item
  \textbf{Liaison simple}: fonctionne bien si l'écart entre deux
  regroupements est suffisamment grand. S'il y a du bruit entre deux
  regroupements, la qualité des regroupements en sera affectée. Souvent
  quelques valeurs isolées et un seul grand regroupement
\item
  \textbf{Liaison complète}: moins sensible au bruit et aux faibles
  écarts entre regroupements, mais a tendance à casser les regroupements
  globulaires.
\item
  \textbf{Homogénéité de Ward}: le critère ressemble à celui des
  \(K\)-moyennes.
\end{itemize}

Voir la page
\href{https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html}{scikit-learn}
pour une illustration.
\end{frame}

\begin{frame}{Hyperparamètres des méthodes hiérarchiques}
\protect\hypertarget{hyperparamuxe8tres-des-muxe9thodes-hiuxe9rarchiques}{}
\begin{enumerate}
\tightlist
\item
  choix de la fonction de liaison (et hyperparamètres associés)
\item
  mesure de dissemblance
\item
  nombre de regroupements
\end{enumerate}

On peut représenter le modèle à l'aide d'un arbre, où les feuilles
indiquent les regroupements à chaque étape jusqu'à la racine à la
dernière étape (\textbf{dendrogramme}).

La distance entre chaque embranchement est déterminée par notre critère:
cela nous permet de sélectionner un nombre de regroupements \(K\) après
inspection visuelle du dendrogramme.

On élague l'arbre à la hauteur voulue.
\end{frame}

\begin{frame}{Dendrogramme}
\protect\hypertarget{dendrogramme}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-dendrogramme-1.pdf}

}

\caption{\label{fig-dendrogramme}Dendrogramme pour l'exemple de
regroupement hiérarchique avec la méthode de Ward et 20 observations.}

\end{figure}
\end{frame}

\begin{frame}{Critères pour Ward}
\protect\hypertarget{crituxe8res-pour-ward}{}
On peut choisir \(K\) à partir du pourcentage de variance expliquée,
\(R^2\) en calculant
\[R^2_{(M)} = 1-\mathrm{H}_{(M)}/\mathrm{H}_{(1)},\] où
\(\mathrm{H}_{(1)}\) est l'homogénéité globale avec un seul groupe.

Le R-carré semi-partiel mesure la perte d'homogénéité d'une étape à
l'autre, renormalisée par
\[R^2_{\text{sp}(M)} =\frac{\mathrm{H}_{(M)} - \mathrm{H}_{(M-1)}}{\mathrm{H}_{(1)}},\]

mesure la perte d'homogénéité (relative) en combinant ces deux groupes.

On cherche un point d'inflection (un coude).
\end{frame}

\begin{frame}{Critères d'homogénéité (Ward)}
\protect\hypertarget{crituxe8res-dhomoguxe9nuxe9ituxe9-ward}{}
\end{frame}

\begin{frame}{Avantages et inconvénient, regroupements hiérarchiques}
\protect\hypertarget{avantages-et-inconvuxe9nient-regroupements-hiuxe9rarchiques}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) la solution du regroupement hiérarchique est toujours la même
  (déterministe)
\item
  (\(-\)) l'assignation d'une observation à un regroupement est finale
\item
  (\(-\)) les aberrances ne sont pas traitées et sont souvent assignées
  dans des regroupements à part
\item
  (\(+\)) les méthodes d'arborescence sont faciles à expliquer
\item
  (\(-\)) le nombre de groupes n'a pas à être spécifié apriori (une
  seule estimation)
\item
  (\(-\)) le coût de calcul est prohibitif, avec une complexité
  quadratique de \(\mathrm{O}(n^2)\) pour la méthode de liaison simple
  et autrement \(\mathrm{O}(n^3)\) pour la plupart des autres fonctions
  de liaison.
\end{itemize}
\end{frame}



\end{document}
