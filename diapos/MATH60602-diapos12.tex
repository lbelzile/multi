% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{VisbyCF-Medium}
  \setsansfont[]{Latin Modern Sans}
  \setmathfont[]{Latin Modern Math}
\fi
\usecolortheme{Flip}
\usefonttheme{serif} % use mainfont rather than sansfont for slide text
\useinnertheme{Flip}
\useoutertheme{Flip}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{tabu}
\usepackage{mathtools}
\usepackage{mathrsfs}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Analyse de regroupements},
  pdfauthor={Léo Belzile},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Analyse de regroupements}
\subtitle{Analyse multidimensionnelle appliquée}
\author{Léo Belzile}
\date{}
\institute{HEC Montréal}

\begin{document}
\frame{\titlepage}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, frame hidden, enhanced, breakable, interior hidden, sharp corners, borderline west={3pt}{0pt}{shadecolor}]}{\end{tcolorbox}}\fi

\begin{frame}{Algorithmes pour l'analyse de regroupements}
\protect\hypertarget{algorithmes-pour-lanalyse-de-regroupements}{}
L'analyse de regroupements cherche à créer une division de \(n\)
observations de \(p\) variables en regroupements.

\begin{enumerate}
\tightlist
\item
  méthodes basées sur les centroïdes et les médoïdes (\(k\)-moyennes,
  \(k\)-médoides)
\item
  mélanges de modèles
\item
  méthodes basées sur la connectivité (regroupements hiérarchiques
  agglomératifs et divisifs)
\item
  méthodes basées sur la densité
\end{enumerate}
\end{frame}

\begin{frame}{Algorithme de partition autour des médoïdes (PAM)}
\protect\hypertarget{algorithme-de-partition-autour-des-muxe9douxefdes-pam}{}
\begin{enumerate}
\tightlist
\item
  Initialisation: sélectionner \(K\) des \(n\) observations comme
  médoïdes initiaux.
\item
  Assigner chaque observation au médoïde le plus près.
\item
  Calculer la dissimilarité totale entre chaque médoïde et les
  observations de son groupe.
\item
  Pour chaque médoïde \((k=1, \ldots, K\)):

  \begin{itemize}
  \tightlist
  \item
    considérer tous les \(n-K\) observations à tour de rôle et permuter
    le médoïde avec l'observation.\\
  \item
    calculer la distance totale et sélectionner l'observation qui
    diminue le plus la distance totale.
  \end{itemize}
\item
  Répéter les étapes 2 à 4 jusqu'à ce que les médoïdes ne changent plus.
\end{enumerate}
\end{frame}

\begin{frame}{Algorithme CLARA (1/2)}
\protect\hypertarget{algorithme-clara-12}{}
L'algorithme CLARA, décrit dans Kaufman \& Rousseeuw (1990), réduit le
coût de calcul et de stockage.

\begin{itemize}
\tightlist
\item
  Diviser l'échantillon en \(S\) sous-échantillons de taille
  approximativement égale de taille \(n_S\)

  \begin{itemize}
  \tightlist
  \item
    typiquement \(K \ll n_S < 1000\)
  \end{itemize}
\item
  Utiliser l'algorithme PAM sur chaque sous-échantillon.
\end{itemize}

Une fois les médoïdes obtenus, le reste de toutes les observations de
l'échantillon sont assignées au regroupement du médoïde le plus près.

Pour chacune des \(S\) segmentations, on calcule la distance moyenne
entre les médoïdes et les observations.

La meilleure segmentation est retournée: c'est celle qui a la plus
petite distance moyenne parmi les \(S\).
\end{frame}

\begin{frame}[fragile]{PAM et CLARA dans \textbf{R}}
\protect\hypertarget{pam-et-clara-dans-r}{}
Disponible depuis le paquet \texttt{cluster}.

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{60602}\NormalTok{)}
\NormalTok{kmedoide5 }\OtherTok{\textless{}{-}}\NormalTok{ cluster}\SpecialCharTok{::}\FunctionTok{clara}\NormalTok{(}
   \AttributeTok{x =}\NormalTok{ donsmult\_std,}
   \AttributeTok{k =}\NormalTok{ 5L, }\CommentTok{\# nombre de groupes}
   \AttributeTok{sampsize =} \DecValTok{500}\NormalTok{, }\CommentTok{\#taille échantillon pour PAM}
   \AttributeTok{metric =} \StringTok{"euclidean"}\NormalTok{, }\CommentTok{\# distance l2}
   \CommentTok{\#cluster.only = TRUE, \# ne conserver que étiquettes}
   \AttributeTok{rngR =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# germe aléatoire depuis R}
   \AttributeTok{pamLike =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# même algorithme que PAM}
   \AttributeTok{samples =} \DecValTok{10}\NormalTok{) }\CommentTok{\#nombre de répétitions aléatoires}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Valeurs initiales et paramètres}
\protect\hypertarget{valeurs-initiales-et-paramuxe8tres}{}
\footnotesize

Même hyperparamètres que \(K\)-moyennes (dissemblance, nombre de
regroupements, initialisation et séparation).

Comme les \(K\)-moyennes, on fera plusieurs essais pour trouver de
bonnes valeurs de départ. On peut tracer le profil des silhouettes.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-clarasilhouette-1.pdf}

}

\caption{\label{fig-clarasilhouette}Silhouettes pour les données de dons
multiples avec l'algorithme CLARA pour \(K=5\) regroupements.}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Prototypes}
\protect\hypertarget{prototypes}{}
Puisque les prototypes (médoïdes) sont des observations, on peut
simplement extraire leur identifiant.

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{medoides\_orig }\OtherTok{\textless{}{-}}\NormalTok{ donsmult[kmedoide5}\SpecialCharTok{$}\NormalTok{i.med,]}
\CommentTok{\# Taille des regroupements}
\NormalTok{kmedoide5}\SpecialCharTok{$}\NormalTok{clusinfo}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Avantages et inconvénients des \(K\)-médoïdes}
\protect\hypertarget{avantages-et-inconvuxe9nients-des-k-muxe9douxefdes}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) les prototypes sont des observations de l'échantillon.
\item
  (\(+\)) la fonction objective est moins impactée par les extrêmes.
\item
  (\(-\)) le coût de calcul est prohibitif avec des mégadonnées
  (problème combinatoire). PAM fonctionne avec maximum 1000
  observations.
\item
  (\(-\)) solution approximative pour grand échantillons avec CLARA
\end{itemize}
\end{frame}

\begin{frame}{Mélange de modèles}
\protect\hypertarget{muxe9lange-de-moduxe8les}{}
On suppose qu'on a \(K\) groupes, chacun caractérisé par une densité de
dimension \(p\), soit \(f_k(\boldsymbol{X}_i;\boldsymbol{\theta}_k)\) si
\(\boldsymbol{X}_i\) provient du groupe \(k=1, \ldots, K\).

Généralement, on choisit une loi normale multidimensionnelle pour le
\(k\)e groupe \(G\), \begin{align*}
\boldsymbol{X} \mid G=k \sim \mathsf{No}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{align*}

La probabilité qu'une observation soit tirée du groupe \(G=k\) est
\(\pi_k\).
\end{frame}

\begin{frame}{Estimation du mélange de modèle}
\protect\hypertarget{estimation-du-muxe9lange-de-moduxe8le}{}
La vraisemblance est une fonction des paramètres \(\boldsymbol{\mu}_k\),
\(\boldsymbol{\Sigma}_k\) et de la probabilité \(\pi_k\) qu'une
observation \(\mathbf{X}_i\) tombe dans le groupe \(k\), \begin{align*}
 L_i(\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \boldsymbol{\pi}_k\}_{k-1}^K; \mathbf{X}_i)= \sum_{k=1}^K\pi_k
f_{k}(\boldsymbol{X}_i \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
\end{align*}

Le maximum de vraisemblance est obtenu à l'aide de l'algorithme
d'espérance-maximisation en augmentant les observations avec un
indicateur de groupe.

\begin{itemize}
\tightlist
\item
  Étape E: assignation aux groupes (multinomiale).
\item
  Étape M: estimation des probabilités, des moyennes et variances.
\end{itemize}

Le mélange de modèle nous donne accès à la probabilité \(\pi_k\) qu'une
observation appartiennent au groupe \(G_k\) (\textbf{assignation
probabiliste}).
\end{frame}

\begin{frame}{Fléau de la dimension}
\protect\hypertarget{fluxe9au-de-la-dimension}{}
Chacune des \(K\) matrice de covariance contient \(p(p+1)/2\)
paramètres!

En paramétrisant ces dernière, on peut réduire le nombre de paramètres à
estimer.

\begin{itemize}
\tightlist
\item
  compromis entre simplicité (d'estimation) et nombre de paramètres
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Paramétrisation des matrices de covariance}
\protect\hypertarget{paramuxe9trisation-des-matrices-de-covariance}{}
La matrice de covariance dans \texttt{mclust} est paramétrisée en
fonction de

\begin{itemize}
\tightlist
\item
  \(\lambda\), qui contrôle le volume,
\item
  une matrice diagonale \(\mathbf{A}\) qui contrôle les variances de
  chaque observation et
\item
  \(\mathbf{D}\) une matrice orthogonale qui permet de créer de la
  corrélation entre observations.
\end{itemize}

Un index \(k\) spécifie que cette composante varie d'un regroupement à
l'autre.
\end{frame}

\begin{frame}[fragile]{Paramétrisation des variables}
\protect\hypertarget{paramuxe9trisation-des-variables}{}
Voir \texttt{mclust.options("emModelNames")} et la documentation dans le
Tableau 3 de
\href{https://journal.r-project.org/archive/2016/RJ-2016-021/RJ-2016-021.pdf}{l'article
sur \texttt{mclust}}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{figures/mclust5-parametrization.png}

}

\caption{\label{fig-modeles}Forme des ellipsoïdes pour le mélange de
modèle selon la forme de la structure de covariance. Tirée de
\href{(https://journal.r-project.org/archive/2016/RJ-2016-021/RJ-2016-021.pdf)}{\texttt{mclust5}},
licence \href{https://creativecommons.org/licenses/by/4.0/}{CC BY 4.0}.}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Paquet \texttt{mclust}}
\protect\hypertarget{paquet-mclust}{}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\DocumentationTok{\#\# Mélanges de modèles gaussiens}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{60602}\NormalTok{)}
\FunctionTok{library}\NormalTok{(mclust)}
\NormalTok{mmg }\OtherTok{\textless{}{-}} \FunctionTok{Mclust}\NormalTok{(}\AttributeTok{data =}\NormalTok{ donsmult\_std,}
       \AttributeTok{G =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
       \CommentTok{\# Matrice de covariance (par défaut, tous choix)}
       \CommentTok{\# modelNames = mclust.options("emModelNames"), }
       \DocumentationTok{\#\# Ajouter composante uniforme}
       \DocumentationTok{\#\#  pour bruit (aberrances)}
       \AttributeTok{initialization =} \FunctionTok{list}\NormalTok{(}\AttributeTok{noise =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\# Résumé de la segmentation}
\FunctionTok{summary}\NormalTok{(mmg)}
\end{Highlighting}
\end{Shaded}

On peut obtenir les étiquettes (avec \texttt{0} pour le bruit) avec
\texttt{mmg\$classification}.
\end{frame}

\begin{frame}{Hyperparamètres}
\protect\hypertarget{hyperparamuxe8tres}{}
\begin{itemize}
\tightlist
\item
  le nombre de regroupements \(K\)
\item
  la forme des ellipsoïdes (structure de covariance)
\item
  les valeurs pour l'initialisation.
\end{itemize}

Les mêmes considérations pratiques qu'avec les \(K\)-moyennes
s'appliquent.

En pratique, on ajuste le modèle avec différent nombre de regroupements
et différentes structures de covariance et on prend le modèle avec le
plus petit BIC.
\end{frame}

\begin{frame}[fragile]{Sélection des hyperparamètres}
\protect\hypertarget{suxe9lection-des-hyperparamuxe8tres}{}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\FunctionTok{plot}\NormalTok{(mmg, }\AttributeTok{what =} \StringTok{"BIC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\footnotesize

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-mclustbic-1.pdf}

}

\caption{\label{fig-mclustbic}Valeur du négatif du BIC pour les mélanges
de modèles gaussiens selon le nombre de regroupements et la structure de
covariance.}

\end{figure}

Le logiciel n'arrive pas à estimer certains modèles complexes quand le
nombre de groupes est trop élevé.
\end{frame}

\begin{frame}{Représentation graphique des regroupements}
\protect\hypertarget{repruxe9sentation-graphique-des-regroupements}{}
\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-classifreducmclust-1.pdf}

}

\caption{\label{fig-classifreducmclust}Projection des observations,
colorées par regroupement (gauche) et structure des regroupements avec
ellipsoides de confiance (droite).}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Avantages et inconvénients des mélanges de
modèles}
\protect\hypertarget{avantages-et-inconvuxe9nients-des-muxe9langes-de-moduxe8les}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) approche est plus flexible que les \(K\)-moyennes.
\item
  (\(+\)) l'ajout d'une composante uniforme permet de gérer les
  aberrances (supporté par \texttt{mclust}).
\item
  (\(+\)) l'algorithme EM garantie la convergence à un optimum local
  (comme pour les \(K\)-moyennes)
\item
  (\(+\)) on obtient une assignation probabiliste plutôt que rigide
\item
  (\(-\)) le coût de calcul est plus élevé que les \(K\)-moyennes
\item
  (\(-\)) le nombre de paramètre des matrices de covariance augmente
  rapidement avec la dimension \(p\).
\end{itemize}
\end{frame}

\begin{frame}{Regroupements hiérarchiques}
\protect\hypertarget{regroupements-hiuxe9rarchiques}{}
Méthode déterministe de regroupement à partir d'une matrice de
dissimilarité.

\begin{enumerate}
\tightlist
\item
  Initialisation: chaque observation est assignée à son propre groupe.
\item
  les deux groupes les plus rapprochés sont fusionnés; la distance entre
  le nouveau groupe et les autres regroupements est recalculée.
\item
  on répète l'étape 2 jusqu'à obtenir un seul regroupement.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]{Fonction de liaison}
\protect\hypertarget{fonction-de-liaison}{}
Il y a plusieurs façons de calculer la distance entre deux groupes
d'observations de plusieurs observations, notamment

\begin{itemize}
\tightlist
\item
  liaison simple (\texttt{method\ =\ single}): plus proches voisins
\item
  liaison complète (\texttt{method\ =\ complete}): voisins les plus
  éloignés
\item
  liaison moyenne (\texttt{method\ =\ average}): utilise la moyenne des
  distances entre toutes les paires de sujets (un pour chaque groupe)
  provenant des deux groupes.
\item
  méthode de Ward (\texttt{method\ =\ ward.D2}): calcul de l'homogénéité
  globale
\end{itemize}
\end{frame}

\begin{frame}{Illustration des mesures de liaison}
\protect\hypertarget{illustration-des-mesures-de-liaison}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-distances-1.pdf}

}

\caption{\label{fig-distances}Distances entre regroupements selon la
liaison (simple, complète, barycentre, homogenéité de Ward).}

\end{figure}
\end{frame}

\begin{frame}{Méthode de Ward}
\protect\hypertarget{muxe9thode-de-ward}{}
La méthode de Ward utilise l'homogénéité comme critère.

Pour chaque groupe, on calcule la somme des carrés des distances par
rapport à la moyenne du groupe, disons \(\mathsf{SCD}_{k,M}\)
(\(k=1, \ldots, M\)).

On calcule ensuite la somme des distances,
\[\mathrm{SCD}_{(M)} = \mathsf{SCD}_{1,M} + \cdots + \mathsf{SCD}_{M,M}.\]

La méthode de Ward va fusionner les deux groupes qui feront augmenter le
moins possible l'homogénéité.
\end{frame}

\begin{frame}{Performance relative}
\protect\hypertarget{performance-relative}{}
\begin{itemize}
\item
  \textbf{Liaison simple}: fonctionne bien si l'écart entre deux
  regroupements est suffisamment grand. S'il y a du bruit entre deux
  regroupements, la qualité des regroupements en sera affectée. Souvent
  quelques valeurs isolées et un seul grand regroupement
\item
  \textbf{Liaison complète}: moins sensible au bruit et aux faibles
  écarts entre regroupements, mais a tendance à casser les regroupements
  globulaires.
\item
  \textbf{Homogénéité de Ward}: le critère ressemble à celui des
  \(K\)-moyennes.
\end{itemize}

Voir la page
\href{https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html}{scikit-learn}
pour une illustration.
\end{frame}

\begin{frame}[fragile]{Méthodes hiérarchiques et coût de calcul}
\protect\hypertarget{muxe9thodes-hiuxe9rarchiques-et-couxfbt-de-calcul}{}
Les algorithmes de regroupement hiérarchiques stockent une matrice de
dissemblance \(n \times n\): coût de stockage quadratique
\(\mathrm{O}(n^2)\).

Généralement, le coût de calcul est au mieux \(\Omega(n^2)\) et au pire
\(\mathrm{O}(n^3)\).

Pour la méthode de liaison simple, un algorithme permet d'obtenir un
coût de calcul quadratique de \(\mathrm{O}(n^2)\) sans stocker la
matrice de dissemblance, d'où un coût de stockage linéaire de
\(\mathrm{O}(n)\).

\texttt{stat::hclust} permet de faire des regroupements agglomératifs,
mais le paquet \texttt{fastcluster} propose une version avec une
empreinte mémoire inférieure (\emph{plus rapide}!)
\end{frame}

\begin{frame}[fragile]{Genie}
\protect\hypertarget{genie}{}
La fonction de liaison simple permet des calculs rapides, mais le
résultat est rarement satisfaisant.

Une proposition de Gagolewski (2016), implémentée dans le paquet
\textbf{R} \texttt{genieclust}, modifie la fonction de liaison simple en
retenant son efficacité de calcul.

Plutôt que de simplement trouver la paire de regroupements à distance
minimale, cette fusion n'est appliquée que si une mesure d'inéquité, le
coefficient de Gini, est inférieur à un seuil spécifié par
l'utilisateur.

Si les regroupements sont fortement inéquitables, la fusion survient
entre les regroupements dont un de la taille minimale courante.
\end{frame}

\begin{frame}{Hyperparamètres des méthodes hiérarchiques}
\protect\hypertarget{hyperparamuxe8tres-des-muxe9thodes-hiuxe9rarchiques}{}
\begin{enumerate}
\tightlist
\item
  choix de la fonction de liaison (et hyperparamètres associés)
\item
  mesure de dissemblance
\item
  nombre de regroupements
\end{enumerate}

On peut représenter le modèle à l'aide d'un arbre, où les feuilles
indiquent les regroupements à chaque étape jusqu'à la racine à la
dernière étape (\textbf{dendrogramme}).

La distance entre chaque embranchement est déterminée par notre critère:
cela nous permet de sélectionner un nombre de regroupements \(K\) après
inspection visuelle du dendrogramme.

On élague l'arbre à la hauteur voulue.
\end{frame}

\begin{frame}{Dendrogramme}
\protect\hypertarget{dendrogramme}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-dendrogramme-1.pdf}

}

\caption{\label{fig-dendrogramme}Dendrogramme pour l'exemple de
regroupement hiérarchique avec la méthode de Ward et 100 premières
observations.}

\end{figure}
\end{frame}

\begin{frame}{Critères pour Ward}
\protect\hypertarget{crituxe8res-pour-ward}{}
On peut choisir \(K\) à partir du pourcentage de variance expliquée,
\(R^2\) en calculant
\[R^2_{(M)} = 1-\mathrm{SCD}_{(M)}/\mathrm{SCD}_{(1)},\] où
\(\mathrm{SCD}_{(1)}\) est l'homogénéité globale avec un seul groupe.

Le R-carré semi-partiel mesure la perte d'homogénéité d'une étape à
l'autre, renormalisée par
\[R^2_{\text{sp}(M)} =\frac{\mathrm{SCD}_{(M)} - \mathrm{SCD}_{(M-1)}}{\mathrm{SCD}_{(1)}},\]
\end{frame}

\begin{frame}[fragile]{Critères d'homogénéité (Ward)}
\protect\hypertarget{crituxe8res-dhomoguxe9nuxe9ituxe9-ward}{}
\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\NormalTok{ward }\OtherTok{\textless{}{-}}\NormalTok{ fastcluster}\SpecialCharTok{::}\FunctionTok{hclust}\NormalTok{(}
  \AttributeTok{d =} \FunctionTok{dist}\NormalTok{(donsmult\_std),}
  \AttributeTok{method =} \StringTok{"ward.D2"}\NormalTok{)}
\NormalTok{hecmulti}\SpecialCharTok{::}\FunctionTok{homogeneite}\NormalTok{(}\AttributeTok{rhier =}\NormalTok{ ward, }
                      \AttributeTok{ngroupes =} \DecValTok{10}\NormalTok{,}
                      \AttributeTok{data =}\NormalTok{ donsmult\_std)}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Graphiques du \(R^2\) et du \(R^2\) semi-partiel}
\protect\hypertarget{graphiques-du-r2-et-du-r2-semi-partiel}{}
On cherche un point d'inflection (un coude).

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/unnamed-chunk-15-1.pdf}

}

\end{figure}
\end{frame}

\begin{frame}{Avantages et inconvénient, regroupements hiérarchiques}
\protect\hypertarget{avantages-et-inconvuxe9nient-regroupements-hiuxe9rarchiques}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) la solution du regroupement hiérarchique est toujours la même
  (déterministe)
\item
  (\(+\)) les méthodes d'arborescence sont faciles à expliquer
\item
  (\(-\)) l'assignation d'une observation à un regroupement est finale
\item
  (\(-\)) les aberrances ne sont pas traitées et sont souvent assignées
  dans des regroupements à part
\item
  (\(-\)) le nombre de groupes n'a pas à être spécifié apriori (une
  seule estimation)
\item
  (\(-\)) le coût de calcul est prohibitif, avec une complexité
  quadratique de \(\mathrm{O}(n^2)\) pour la méthode de liaison simple
  et autrement \(\mathrm{O}(n^3)\) pour la plupart des autres fonctions
  de liaison.
\end{itemize}
\end{frame}

\begin{frame}{Méthodes basées sur la densité}
\protect\hypertarget{muxe9thodes-basuxe9es-sur-la-densituxe9}{}
L'algorithme DBSCAN (\emph{density-based spatial clustering of
applications with noise}) est une méthode de partitionnement basée sur
la densité des points.

L'idée de base de l'algorithme est de tracer une boule de rayon
\(\epsilon\) autour de chaque observation et de voir si elle inclut
d'autres observations.
\end{frame}

\begin{frame}{Mécanisme d'assignation}
\protect\hypertarget{muxe9canisme-dassignation}{}
L'algorithme classe les observations en trois catégories:

\begin{itemize}
\tightlist
\item
  Un point central est une observation qui possède \(M-1\) voisins à
  distance \(\epsilon\).
\item
  Un point frontière est un point qui est distant de moins de
  \(\epsilon\) d'un point central, sans en être un.
\item
  Un point isolé est une observation qui n'est pas rattachée à aucun
  regroupement.
\end{itemize}
\end{frame}

\begin{frame}{Algorithme DBSCAN}
\protect\hypertarget{algorithme-dbscan}{}
L'algorithme répète les étapes suivantes jusqu'à ce que chaque
observation ait été visitée.

\begin{enumerate}
\tightlist
\item
  Choisir un point aléatoirement parmi ceux qui n'ont pas été visités.
\item
  Si le point n'est pas étiqueté, calculer le nombre de points voisins
  qui se trouvent dans un rayon \(\epsilon\): s'il y a moins de \(M\)
  observations, provisoirement étiqueter l'observation comme point
  isolé, sinon comme point central.
\item
  Si l'observation est un point central avec \(M-1\) voisins ou plus,
  créer un regroupement.
\item
  Étiqueter chaque point à distance \(\epsilon\) créé et l'ajouter au
  regroupement, ainsi que tout point à distance \(\epsilon\) de ces
  voisins.
\end{enumerate}

\href{https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/}{Ce
site web} offre une visualisation interactive des différentes étapes de
l'algorithme et de comparer la performance de DBSCAN selon le type de
regroupements.
\end{frame}

\begin{frame}{Hyperparamètres}
\protect\hypertarget{hyperparamuxe8tres-1}{}
\begin{itemize}
\tightlist
\item
  le rayon \(\epsilon\) et
\item
  le nombre minimal de points pour former un regroupement, \(M\)
\end{itemize}

Hyperparamètres \(M\) et \(\epsilon\) corrélés: si on augmente le nombre
minimal de point \(M\) par regroupement, il faudra également augmenter
le rayon \(\epsilon\) pour éviter d'avoir un nombre trop élevé de points
isolés et d'aberrances.
\end{frame}

\begin{frame}{Classification avec DBSCAN}
\protect\hypertarget{classification-avec-dbscan}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-dbscan1-1.pdf}

}

\caption{\label{fig-dbscan1}Illustration de la classification des points
avec DBSCAN: toutes les observations sont assignées à un regroupement,
moins une aberrance.}

\end{figure}
\end{frame}

\begin{frame}[fragile]{Choix des hyperparamètres}
\protect\hypertarget{choix-des-hyperparamuxe8tres}{}
Avec \(p\) variables explicatives, on recommande \(M > p+1\).

Pour \(\varepsilon\), considérer les \(M\) plus proches voisins.

La fonction \texttt{kNNdistplot} du paquet \texttt{dbscan} permet de
tracer un graphique de la distance moyenne des \(k\) plus proches
voisins pour chaque observation:

\begin{itemize}
\tightlist
\item
  en prenant \(k=M-1\), calculer la distance entre le \(k\) plus proche
  voisin de chaque observation.
\item
  ordonner ces distances.
\item
  choisir \(\epsilon\) selon coude
\end{itemize}
\end{frame}

\begin{frame}{Critère du coude}
\protect\hypertarget{crituxe8re-du-coude}{}
\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{MATH60602-diapos12_files/figure-beamer/fig-dbscan2-1.pdf}

}

\caption{\label{fig-dbscan2}Graphique des distances entre chaque
observation et son troisième plus proche voisin (gauche), en fonction du
pourcentage d'observations à moins de cette distance et regroupements
obtenus avec DBSCAN avec \(M=10\) et \(\epsilon=1.1\) (droite).}

\end{figure}
\end{frame}

\begin{frame}{Avantages et inconvénients de DBSCAN (1/2)}
\protect\hypertarget{avantages-et-inconvuxe9nients-de-dbscan-12}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) le traitement des aberrances est automatique et l'algorithme
  est robuste.
\item
  (\(+\)) le nombre de regroupements n'a pas à être spécifié apriori.
\item
  (\(+\)) la forme des regroupements est arbitraire, peut être non
  convexe et de taille différente.
\item
  (+/-) la complexité de l'algorithme est d'au mieux
  \(\mathrm{O}(n\ln n)\) (calcul) et \(\mathrm{O}(n)\) pour le stockage
  puisque chaque point est visité à tour de rôle et comparé aux autres
  pour trouver les plus proches voisins.
\end{itemize}
\end{frame}

\begin{frame}{Avantages et inconvénients de DBSCAN (2/2)}
\protect\hypertarget{avantages-et-inconvuxe9nients-de-dbscan-22}{}
\begin{itemize}
\tightlist
\item
  (\(+\)) les hyperparamètres ont une interprétation physique
\item
  (\(-\)) mais leur choix n'est pas aisé
\item
  (\(-\)) DBSCAN ne permet pas de traiter le cas où la densité des
  regroupements change et risque de fusionner des regroupements s'il y a
  une série d'observations qui permet de relier deux regroupements.
\item
  (\(-\)) comme la plupart des algorithmes, le voisinage des points
  devient épars quand \(p\) augmente en raison du fléau de la dimension.
\end{itemize}
\end{frame}

\begin{frame}{Comparaison de segmentations}
\protect\hypertarget{comparaison-de-segmentations}{}
On veut parfois comparer les regroupements de différentes méthodes.

Les étiquettes ne sont pas nécessairement identiques même si les
regroupements le sont (permutation des étiquettes).

Une mesure de similarité, l'indice de Rand, permet de comparer deux
vecteurs d'observations catégorielles (étiquettes des regroupements).

\begin{itemize}
\tightlist
\item
  même longueur (même nombre d'observations)
\item
  mais pas nécessairement le même nombre de modalités (nombre de
  regroupements potentiellement différents d'une partition à l'autre).
\end{itemize}

Idée: comparer à tour de rôle chacune des \(n(n-1)/2\) paires
d'observation.
\end{frame}

\begin{frame}{Indice de Rand}
\protect\hypertarget{indice-de-rand}{}
On indique si les paires d'observations sont dans le même groupe (1) ou
un groupe différent (0).

Exemple: deux partitions de 4 observations

\begin{itemize}
\tightlist
\item
  Étiquettes de la méthode \(A\) avec deux regroupements: (2, 2, 2, 1)
\item
  Étiquettes de la méthode \(B\) avec trois regroupements: (3, 1, 3, 2)
\end{itemize}

\begin{longtable}[]{@{}cccccc@{}}
\toprule()
\{1,2\} & \{1,3\} & \{1,4\} & \{2,3\} & \{2,4\} & \{3,4\} \\
\midrule()
\endhead
10 & 11 & 00 & 10 & 00 & 00 \\
\bottomrule()
\end{longtable}

L'indice de Rand est le taux de bonne classification du tableau de
contingence résultant. Parmi les six paires, quatre sont concordantes
(00 ou 11), d'où un indice de \(\textsf{Rand} = 0.66\)

Une valeur de 1 indique que les deux partitions sont identiques.
\end{frame}

\begin{frame}{Récapitulatif}
\protect\hypertarget{ruxe9capitulatif}{}
Les étapes d'une analyse de regroupements

\begin{enumerate}
\tightlist
\item
  Choisir les variables pertinentes à l'analyse. Cette étape peut
  nécessiter de créer, transformer de nouvelles variables ou d'aggréger
  les données.
\item
  Décider quel méthode sera utilisée pour la segmentation.
\item
  Choisir les hyperparamètres de l'algorithme (nombre de regroupements,
  rayon, etc.) et la mesure de dissemblance.
\item
  Valider la qualité de la segmentation (interprétabilité, taille des
  groupes, homogénéité des regroupements).
\item
  Avec les étiquettes, calculer un prototype de groupe.
\item
  Interpréter les regroupements obtenus à partir des prototypes.
\end{enumerate}
\end{frame}

\begin{frame}{Récapitulatif}
\protect\hypertarget{ruxe9capitulatif-1}{}
\begin{itemize}
\tightlist
\item
  L'analyste a une grande marge de manoeuvre.
\item
  Il n'y a pas de vérité: la segmentation n'est utile que si elle a une
  valeur ajoutée.
\item
  Aucun algorithme ne performe uniformément mieux, mais certains sont
  plus faciles à employer que d'autres.

  \begin{itemize}
  \tightlist
  \item
    avec des mégadonnées, la complexité est un facteur important pour
    choisir la méthode.
  \item
    la plupart du temps, le choix des hyperparamètres nécessite un peu
    d'essai-erreur.
  \item
    la segmentation peut être médiocre parce que les hyperparamètres
    sont mal choisis.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Récapitulatif}
\protect\hypertarget{ruxe9capitulatif-2}{}
Le Diable est dans les détails:

\begin{itemize}
\tightlist
\item
  variables catégorielles et binaires vs standardisation
\item
  valeurs manquantes
\item
  aberrances
\end{itemize}

Le nombre de groupes peut être guidé par le contexte: les formules et
indicateurs de qualité servent de balises.
\end{frame}



\end{document}
