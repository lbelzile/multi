---
title: "Classification"
subtitle: "Analyse multidimensionnelle appliquée"
date: ""
author: "Léo Belzile"
institute: "HEC Montréal"
format: beamer
navigation: empty
colortheme: Flip
innertheme: Flip
outertheme: Flip
themeoptions: "bullet=circle, topline=true, shadow=false"
beamerarticle: false
pdf-engine: lualatex
cache: true
code-line-numbers: true
fig-align: 'center'
mainfont: "VisbyCF-Medium"
mathfont: 'Latin Modern Math'
sansfont: 'Latin Modern Sans'
keep-tex: true
include-in-header: 
      text: |
        \usepackage{tabu}
        \usepackage{mathtools}
        \usepackage{mathrsfs}
---

# Rappel

La régression logistique spécifie un modèle pour la probabilité de succès 
$$p = \Pr(Y=1 \mid \mathbf{X}) = \frac{1}{1+\exp(-\eta)}$$
où $\eta = \beta_0 + \cdots + \beta_p \mathrm{X}_p$.

# Prédiction

En substituant l'estimation $\widehat{\beta}_0, \ldots, \widehat{\beta}_p$, on calcule 

- le prédicteur linéaire $\widehat{\eta}_i$ et
- la probabilité de succès $\widehat{p}_i$

pour chaque observation de la base de données.


# Classification de base

Choisir un point de coupure $c$:

- si $\widehat{p} < c$, on assigne $\widehat{Y}=0$.
- si $\widehat{p} \geq c$, on assigne $\widehat{Y}=1$.

Un point de coupure de $c=0.5$ revient à assigner l'observation à la classe (catégorie) la plus probable.

Qu'arrive t'il si $c=0$ ou $c=1$? **Exemple au tableau**

# Qualité de l'ajustement

Considérons une prédiction $\widehat{Y} \in \{0,1\}$.


L'erreur quadratique pour un problème de classification est
$$(Y-\widehat{Y})^2 = \begin{cases} 1, & Y \neq \widehat{Y}; \\0, & Y = \widehat{Y}.\end{cases}$$
et donc on obtient le **taux de mauvaise classification** si on calcule la moyenne
\begin{align*}
\frac{1}{n} \sum_{i=1}^n (y_i - \widehat{y}_i)^2.
\end{align*}

Plus le taux de mauvaise classification est petit, meilleure est la capacité prédictive du modèle.

# Base de données marketing

On considère un modèle pour la variable binaire `yachat`, qui vaut $1$ si une personne achète suite à l'envoi d'un catalogue et $0$ sinon.

```{r}
#| echo: true
#| eval: true
data(dbm, package = "hecmulti")
# Ne conserver que l'échantillon d'apprentissage
appr <- dbm[dbm$test == 0,]
formule <- formula("yachat ~ x1 + x2 + x3 +
                x4 + x5 + x6 + x7 + x8 + x9 + x10")
modele <- glm(formule,
              data = appr, 
              family = binomial)
```


# Estimation de la performance du modèle

Utiliser les mêmes données pour l'ajustement et l'estimation de la performance n'est (toujours) pas recommandé.

Plutôt, considérer la validation croisée ou la division de l'échantillon.

```{r}
#| eval: true
#| echo: true
set.seed(60602)
cv_prob <- hecmulti::predvc(
  modele = modele,  
  K = 10, # nb de plis pour la valid. croisée
  nrep = 10, # nb de répétitions de la valid. croisée
  data = appr)
```

\footnotesize

La fonction `predvc` ajuste le modèle sur chaque 9/10 des données et prédit le dixième restant. On répète la procédure 10 fois et on calcule la moyenne des 10 probabilités prédites pour chaque observation. 

\normalsize

# Prédictions


```{r}
#| label: figclassification0
#| echo: false
#| eval: true
#| cache: true
#| fig-width: 8
#| fig-height: 6
#| out-width: '80%'
#| fig-align: 'center'
# Transformer les variables catégorielles en facteurs
# Formule pour la moyenne du modèle logistique
form <- formula("yachat ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10")
mod <- glm(data = appr,
           formula = form, 
           family = binomial)
# Créer un conteneur pour les probabilités
set.seed(60602)
# Histogramme des probabilités prédites par validation croisée avec n groupes
library(ggplot2)
ggplot(data = data.frame(x = cv_prob),
       aes(x = x)) +
  geom_histogram(bins = 30) + 
  labs(x = "probabilité d'achat prédite", 
       y = "") +
  theme_minimal()
#  Modèle complet et valeurs prédites
appr_prob <- fitted(glm(data = appr, 
                         formula = form, 
                         family=binomial), 
                     type = "response") 

perfo <- function(prob, resp, plot = FALSE){
   # VRAI == 1, FAUX == 0
   cuts <- seq(from = 0.01, to = 0.99, by = 0.01)
   tab <- data.frame(coupe = cuts,
      t(sapply(cuts, function(cut){
   nsucces <- sum(resp == 1)
   nechec <- length(resp) - nsucces
   n <- length(resp)
   predy <- ifelse(prob >= cut, 1, 0) 
   c1 <- sum(predy & resp) # Y=1, Yhat=1
   c0 <- sum(!predy & !resp) # Y=0, Yhat=0
   i1 <- sum(!predy & resp) # Y=1, Yhat=0
   i0 <- sum(predy & !resp) # Y=0, Yhat=1
   c(c1 = c1, 
     c0 = c0, 
     i0 = i0,
     i1 = i1,      
     pcorrect = 100*(c0+c1)/n, 
     sensi = 100*c1/(c1+i1), # Y=1 & Yhat=1 / # Y=1
     speci = 100*c0/(c0+i0), # Y=0 & Yhat=0 / # Y=0
     fpos = 100*i0/(c1+i0), # Y=0 & Yhat=1 / # Yhat=1
     fneg = 100*i1/(c0+i1)) # Y=1 & Yhat=0 / # Yhat=0
   })))
   if(plot){
     plot(c(1,1-tab$speci/100,0), c(1,tab$sensi/100,0), type = "l", pty = "s", bty = "l", 
          xlab = "1-spécificité", 
          ylab = "sensibilité", 
          main ="Fonction d'efficacité du récepteur (ROC)",
          panel.first = abline(a=0,b=1), 
          ylim = c(0,1), xlim = c(0,1), yaxs = "i", xaxs = "i")
   }
   invisible(list(cut = tab[which.max(tab$pcorrect),"coupe"],
        confusion = matrix(tab[which.max(tab$pcorrect), c(2,5,4,3)], nrow = 2, ncol = 2),
        table = tab))
   
}
# Performance du modèle avec données d'apprentissage
perfo0 <- perfo(prob = appr_prob, resp = appr$yachat)
perfo1 <- perfo(prob = cv_prob, resp = appr$yachat)
```

\footnotesize 

Répartition des probabilités de succès prédites par validation croisée.

\normalsize 

# Choix d'un point de coupure.

On peut faire varier le point de coupure et choisir celui qui maximise le taux de bonne classification, $\widehat{\Pr}(Y = \widehat{Y})$.

```{r}
#| fig-width: 8
#| fig-height: 4
#| out-width: '80%'
#| fig-align: 'center'
library(ggplot2)
ggplot(data = perfo1$table,
       aes(x = coupe, 
           y = pcorrect)) +
  geom_line() +
  geom_vline(xintercept = perfo1$cut, linetype = "dashed", alpha = 0.5) + 
  labs(x = "point de coupure",
       y = "",
       subtitle = "taux de bonne classification (pourcentage)") +
  scale_x_continuous(expand = c(0.01,0.01),
                     breaks = c(0,0.25,0.5,0.75,1),
                     labels = c("0","0.25","0.5","0.75","1")) + 
  theme_classic()
```

Avec $c=`r perfo1$cut`$, on obtient un taux de mauvaise classification de $`r perfo1$table$pcorrect[which.max(perfo1$table$pcorrect)]`$\%.

# Performance et classification



```{r}
#| eval: false
#| echo: true
classif <- appr$yachat
# Tableau de la performance
hecmulti::perfo_logistique(
  prob = cv_prob,
  resp = classif)
```

# Matrice de confusion

On peut classifier les observations dans un tableau pour un point de coupure donné.

```{r}
#| label: tbl-confumat
#| eval: true
#| echo: false
#| tbl-cap: "Matrice de confusion avec point de coupure optimal. Les entrées donnent le décompte du nombre d'observation par combinaison $(Y, \\widehat{Y})$"
confumat <- perfo1$confusion
rownames(confumat) <- c("\\(\\widehat{Y}=1\\)","\\(\\widehat{Y}=0\\)")
colnames(confumat) <- c("\\(Y=1\\)","\\(Y=0\\)")
knitr::kable(x = confumat, 
                align = "r",
                escape = FALSE,
                booktabs = TRUE,
             format = "latex")
```


# Classification et mesures de performance

\footnotesize 

Les estimés empiriques sont simplement obtenus en calculant les rapports du nombre d'observations dans chaque classe.

\normalsize 

```{r}
#| label: tableconfumat2
#| eval: true
#| echo: false
confumat <- perfo1$confusion
rownames(confumat) <- c("\\(\\widehat{Y}=1\\)","\\(\\widehat{Y}=0\\)")
colnames(confumat) <- c("\\(Y=1\\)","\\(Y=0\\)")
confumat_th <- confumat
confumat_th[1:4] <- c("VP","FN","FP","VN")
tab1 <- knitr::kable(x = confumat,
                align = "r",
                escape = FALSE,
                booktabs = TRUE,
             format = "latex")
tab2 <- knitr::kable(x = confumat_th,
                align = "r",
                escape = FALSE,
                booktabs = TRUE,
             format = "latex")
knitr::kables(list(tab1, tab2))

```


- La **sensibilité** est le taux de succès correctement classés, $\Pr(Y=1, \widehat{Y}=1 \mid Y=1)$, soit $\mathsf{VP}/(\mathsf{VP}+\mathsf{FN})$.
- La **spécificité** est le taux d'échecs correctement classés, $\Pr(Y=0, \widehat{Y}=0 \mid Y=0)$, soit $\mathsf{VN}/(\mathsf{VN}+\mathsf{FP})$.
- Le taux de **faux positifs** est $\Pr(Y=0, \widehat{Y}=1 \mid \widehat{Y}=1)$.
- Le taux de **faux négatifs** est $\Pr(Y=1, \widehat{Y}=0 \mid \widehat{Y}=0)$.

# Matrice de gain

Il est également possible d'assigner un poids différent à chaque événement selon le scénario et chercher à maximiser le gain.


```{r}
#| label: tbl-tableconfumat4
#| eval: true
#| echo: false
#| tbl-cap: "Pondération des entrées de la matrice de confusion pour le calcul du gain."
confumat_tbclassif <- confumat
confumat_tbclassif[1:4] <- c("$c_{11}$","$c_{01}$","$c_{10}$","$c_{00}$")
tab1 <- knitr::kable(x = confumat_tbclassif,
                align = "r",
                escape = FALSE,
                booktabs = TRUE,
             format = "latex")
tab1

```



On calcule le gain en faisant la somme des entrées fois les poids, soit $$\textsf{gain} = c_{11}\mathsf{VP} + c_{10}\mathsf{FP} + c_{01}\mathsf{FN} + c_{00}\mathsf{VN}.$$

# Gain pour taux de bonne classification

Si on cherche à maximiser le taux de bonne classification, cela revient à assigner les poids suivants.


```{r}
#| label: tbl-tableconfumat3
#| tbl-cap: "Pondération des entrées de la matrice de confusion pour le calcul du taux de bonne classification."
#| eval: true
#| echo: false
#| fig-align: 'center'
confumat_tbclassif <- confumat
confumat_tbclassif[1:4] <- c("$1$","$0$","$0$","$1$")
tab1 <- knitr::kable(x = confumat_tbclassif,
                align = "r",
                escape = FALSE,
                booktabs = TRUE,
             format = "latex")
tab1

```

# Coûts et bénéfices du ciblage marketing



- Si on n'envoie pas de catalogue, notre gain est nul. 
- Si on envoie le catalogue 
   - à un client qui n'achète pas, on perd $10$\$ (le coût de l'envoi). 
   - à un client qui achète, notre revenu net est de $57$\$ (revenu moyen moins coût de l'envoi, arrondi à l'entier près pour simplifier).



```{r}
#| label: tbl-dbm-ymontant-apprentissage
#| echo: false
#| eval: true
#| tbl-cap: "Statistiques descriptives des montants d'achats."
data(dbm, package = "hecmulti")
dbm |>
  dplyr::filter(test == 0,
                  !is.na(ymontant))  |>
  dplyr::summarize("n" = length(ymontant), 
            "moyenne" = round(mean(ymontant),2), 
            "écart-type" =round(sd(ymontant),2), 
            "minimum" = min(ymontant), 
            "maximum" = max(ymontant)) |>
knitr::kable(booktabs = TRUE)
```

# Matrice de gain pour ciblage marketing

```{r}
#| label: tbl-tableconfumat5
#| eval: true
#| echo: false
#| tbl-cap: "Matrice de gain pour ciblage marketing."
confumat_tbclassif <- confumat
confumat_tbclassif[1:4] <- c("$57$","$0$","$-10$","$0$")
knitr::kable(x = confumat_tbclassif,
             align = "r",
             escape = FALSE,
             booktabs = TRUE,
             format = "latex")
```


# Point de coupure avec gain

```{r}
#| label: fig-coupure-pondere-fake
#| eval: false
#| echo: true
#| fig-cap: "Estimation du gain moyen en fonction du point de coupure pour l'exemple de base de données marketing."
set.seed(60602)
coupe <- hecmulti::select_pcoupe(
  modele = modele, 
  c00 = 0, 
  c01 = 0, 
  c10 = -10, 
  c11 = 57,
  plot = TRUE)
```

\footnotesize


La fonction `select_pcoupe` estime le gain pour différents points de coupures, avec probabilités estimées par validation croisée avec `ncv` groupes, répétée `nrep` fois.

\normalsize

# Calcul du gain


```{r}
#| label: fig-coupure-pondere
#| eval: true
#| echo: false
#| cache: true
#| fig-align: 'center'
#| out-width: '80%'
#| fig-width: 6
#| fig-height: 4
#| fig-cap: "Estimation du gain moyen en fonction du point de coupure pour l'exemple de base de données marketing."
set.seed(60602)
coupe <- hecmulti::select_pcoupe(
  modele = modele, 
  c00 = 0, 
  c01 = 0, 
  c10 = -10, 
  c11 = 57,
  plot = TRUE)
```



# Point de coupure optimal et explication

Dans l'exemple, le point de coupure qui maximise le gain est `r coupe$optim`. Avec ce point de coupure, on estime que 

- le taux de bonne classification est de `r round(perfo1$table[24,"pcorrect"], 2)` 
- la sensibilité est de `r round(perfo1$table[24,"sensi"], 2)`. 

Ainsi, on va détecter environ `r round(perfo1$table[24,"sensi"],2)`\% des clients qui achètent. 

Il est coûteux de rater un client potentiel, donc la stratégie optimale est d'envoyer le catalogue à plus de clients quitte à ce que plusieurs d'entre eux n'achètent rien. 


# Fonction d'efficacité du récepteur

Graphique de la sensibilité en fonction de un moins la spécificité, en faisant varier le point de coupure, souvent appelé courbe ROC (de l'anglais *receiver operating characteristic*).

La fonction `hecmulti::courbe_roc` permet de tracer la courbe et de calculer l’aire sous la courbe. 

```{r}
#| eval: false
#| echo: true
roc <- hecmulti::courbe_roc(
  resp = classif,
  prob = cv_prob,
  plot = TRUE)
print(roc)
## Pour extraire l'aire sous la courbe, roc$aire
```


# Courbe ROC

```{r}
#| eval: true
#| echo: false
#| out-width: '80%'
#| fig-align: 'center'
classif <- appr$yachat
roc <- hecmulti::courbe_roc(
  resp = classif,
  prob = cv_prob,
  plot = TRUE)
## Pour extraire l'aire sous la courbe, roc$aire
```

# Aire sous la courbe

- Plus la courbe se rapproche de ($0, 1$) (coin supérieur gauche), meilleure est la classification.
- Autrement dit, plus l'aire sous la courbe est près de $1$, mieux c’est.
- Une aire sous la courbe de $0.5$ (ligne diagonale) correspond à la performance d'une allocation aléatoire.

# Courbe lift

À quelle point notre modèle est-il meilleur qu'une assignation aléatoire?

Le nombre de vrais positifs sur le nombre de succès donne le taux de succès d'une détection aléatoire (si on sélectionnait au hasard des observations).

Pour calculer le nombre de succès prédit par le modèle, on va

- Ordonner les probabilités de succès estimées par le modèle, $\widehat{p}$, en ordre décroissant.
- Sélectionner les $x$% d'observations ayant les plus grandes probabilités de succès. 
- Calculer le nombre total de succès parmi ces observations.


# Code pour produire la courbe lift 


La courbe lift donne le rapport nombre de succès du modèle versus ceux détectés au hasard. La référence est la ligne diagonale, qui correspond à une détection aléatoire.

```{r}
#| label: figcourbe-lift
#| eval: false
#| echo: true
tab_lift <- hecmulti::courbe_lift(
  prob = cv_prob,
  resp = classif, 
  plot = TRUE)
tab_lift
```


# Tableau du lift

```{r}
#| label: tbl-lift
#| echo: false
#| eval: true
#| tbl-cap: "Tableau du lift (déciles)."
tab_lift <- hecmulti::courbe_lift(
  prob = cv_prob,
  resp = classif, 
  plot = FALSE)
knitr::kable(tab_lift[,-1], 
             booktabs = TRUE,
             digits = 2,
             col.names = c("hasard", 
                           "modèle", 
                           "lift"),
             format = "latex") 
```

# Calcul du lift


Si on classifiait comme acheteurs les $`r tab_lift$pourcent[1]`$\% qui ont la plus forte probabilité estimée d'achat, on détecterait $`r tab_lift$modele[1]`$ des $210$ clients.


Si on prend $10$\% des clients au hasard et que $`r tab_lift$hasard[1]`$\% des observations correspondent à des achats, on détecterait en moyenne $`r tab_lift$hasard[1]`$ clients par tranche de $100$ personnes.


Le lift est le nombre de succès détecté par le modèle sur le nombre détecté au hasard.

Si on a un budget limité suffisant pour l'envoi du catalogue à $x$% des clients, on peut utiliser le lift pour comparer notre métrique.



# Courbe lift

```{r}
#| label: figcourbe-lift-pr
#| eval: true
#| echo: false
tab_lift <- hecmulti::courbe_lift(
  prob = cv_prob,
  resp = classif, 
  plot = TRUE)
```


# Calibration du modèle

Certains modèles sont trop confiants dans leurs prédictions (surajustement).


Une statistique simple proposée par [Spiegelhalter (1986)]( https://doi.org/10.1002/sim.4780050506) peut vérifier ce fait. 

Pour une variable binaire $Y \in \{0,1\}$, l'erreur quadratique moyenne s'écrit
\begin{align*}
\overline{B} &= \underset{\text{erreur quadratique moyenne}}{\frac{1}{n} \sum_{i=1}^n (Y_i-p_i)^2}
\\&=\underset{\text{manque de calibration}}{\frac{1}{n} \sum_{i=1}^n(Y_i-p_i)(1-2p_i)} + \underset{\text{variabilité}}{\frac{1}{n} \sum_{i=1}^n p_i(1-p_i)}.
\end{align*}

Si notre modèle était parfaitement calibré, $\mathsf{E}_0(Y_i)=p_i$ et $\mathsf{Va}_0(Y_i) = p_i(1-p_i)$.

# Test de Spiegelhalter

<!--  $$Z = \frac{\overline{B} -\mathsf{E}_0(\overline{B})}{\mathsf{sd}_0(\overline{B})}$$  -->

<!-- Sous l'hypothèse nulle, $Z \sim \mathsf{No}(0,1)$ en grand échantillon.  -->

On peut construire une statistique de test [(Spiegelhalter, 1986)]( https://doi.org/10.1002/sim.4780050506) pour l'hypothèse nulle  de calibration parfaite.

Sous l'hypothèse nulle $\mathcal{H}_0$, le modèle est adéquat (correctement calibré). 

Une petite valeur-$p$ mène au rejet de $\mathcal{H}_0$ et à conclure que le modèle est surajusté.

```{r}
#| label: classification
#| eval: true
#| echo: true
hecmulti::calibration(
   prob = cv_prob, 
   resp = classif)
```

Il n'y a pas de preuve ici que le modèle est mal calibré.

# Problèmes avec la régression logistique

Il arrive que, lors de l'ajustement d'une régression logistique, on obtienne un message d'avertissement: 

\footnotesize

```{r}
#| echo: true
#| eval: false
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
```

\normalsize

# Quasi-séparation de variables

Le deuxième message d'erreur survient quand une combinaison linéaire de variables explicatives permet de prédire exactement la réponse: nos probabilités prédites sont $0$ ou $1$.

- par exemple, si on ajuste un modèle pour `yachat` en fonction de `ymontant`.

\footnotesize

```{r}
#| eval: false
#| echo: true
quasisep <- appr |>  
  # remplacer les valeurs manquantes par des zéros et standardiser
  dplyr::mutate(ymontant = scale(ifelse(yachat == 0, 0, ymontant)))
# Régression logistique pour l'achat (0 ou 1) en fonction du montant
modele_qs <- glm(yachat ~ ymontant, 
              family = binomial, 
              data = quasisep)
```

\normalsize

# Illustration de la séparation de variables

```{r}
#| echo: false
#| eval: true
#| out-width: '60%'
#| fig-width: 8
#| fig-height: 5
#| fig-align: center
set.seed(1234)
x <- sort(rexp(20, rate = 0.2))
y <- as.integer(x>9)
m1 <- glm(y~ x, family=binomial)
fittedsepmod <- function(x){1/(1+exp(-coef(m1)[1] - x*coef(m1)[2]))}
library(ggplot2)
ggplot(data.frame(x, y), aes(x=x,y=y)) +
  geom_point() +
  geom_function(fun = fittedsepmod, n = 1001L, linewidth = 1.1) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.25), labels = c("0","0.25","0.5","0.75","1")) +
  labs(y = "", x = "x") +
  theme_classic()
```

\footnotesize 

Quasi-séparation de variable: les estimations des paramètres sont presques infinies pour permettre une transition abrupte de la probabilité de succès de $\widehat{p}=0$ à $\widehat{p}=1$ à $x=9$.

\normalsize

# Coefficients avec séparation de variable

Les valeurs élevées des coefficients et erreurs-type sont une autre indication de quasi-séparation de variables.

Avec des variables standardisées, un coefficient $|\beta_j| > 10$ est suspect.

```{r quasisepvar}
#| echo: false
#| eval: true
#| label: tbl-quasisep
#| tbl-cap: "Modèle logistique pour `yachat` en fonction de `ymontant` (variable standardisée)."
#| warning: false
#| message: false
quasisep <- appr |>  
  # remplacer les valeurs manquantes par des zéros et standardiser
  dplyr::mutate(ymontant = scale(ifelse(yachat == 0, 0, ymontant)))
# Régression logistique pour l'achat (0 ou 1) en fonction du montant
m1 <- glm(yachat ~ ymontant, 
              family = binomial, 
              data = quasisep)
tab1 <- data.frame(estimate = as.numeric(m1$coefficients), "std.error" = as.numeric(sqrt(diag(vcov(m1)))))
colnames(tab1) <- c("coef.", "erreur-type")
tab1 <- apply(tab1, 1:2, function(x){paste0("$", sprintf(x, fmt = "%.1f"), "$")})
rownames(tab1) <- c("cst","ymontant")
knitr::kable(x = tab1, align = "rrr", booktabs = TRUE, escape = FALSE)
```


# Causes de la quasi-séparation de variables

Typiquement, le problème survient parce que

- on a ajouté un dérivé de la variable réponse comme variable explicative
- on a un modèle surajusté, souvent une variable catégorielle pour laquelle toutes les observations d'un niveau donné ont la même réponse, soit $0$ ou $1$.

Ce n'est pas nécessairement un enjeu pour la prédiction, mais c'est souvent indicateur de problèmes plus importants. Les coefficients et interprétations ne sont plus valides...

# Solutions pour la quasi séparation de variables

1. Regarder quelles probabilités sont presque $0$ ou $1$ pour identifier les observations problématiques et s'assurer que l'échantillon que l'on emploie est adéquat.
   - Par exemple, les femmes ne peuvent pas avoir un cancer de la prostate, donc la probabilité prédite pour ces dernières est $0$. On pourrait simplement enlever les femmes et prédire zéro manuellement.
2. Déterminer si une variable explicative cause du surajustement en étudiant les coefficients dont la valeur absolue est très élevée.

# Récapitulatif

- La classification est une forme d'apprentissage supervisée.
- On peut assigner l'observation à la classe la plus plausible, ou déterminer un point de coupure.
- Si on a un objectif particulier (fonction de gain), on peut optimiser les profits en assignant une importance différente à chaque scénario.
- On peut catégoriser les observations dans une matrice de confusion.

# Récapitulatif

- On s'intéresse à
   - la spécificité (proportion d'échecs correctement classifiés) 
   - la sensibilité (proportion de succès correctement classifiés)
   - le taux de bonne classification
   - le taux de faux positifs ou faux négatifs
- L'aire sous la courbe de la fonction d'efficacité du récepteur (courbe ROC) et le lift donnent une mesure de la qualité des prédictions.



# Sélection de variables en régression logistique

- On applique les mêmes principes que précédemment.
- Notre mesure d'ajustement (gain, taux de bonne classification, log-vraisemblance) peut différer selon l'objectif.
- Les modèles de régression logistique sont plus coûteux à estimer.
- Pour la classification, le point de coupure est à déterminer.

# Fonctions **R** pour la sélection de modèles

- `glmbb::glmbb` permet une recherche exhaustive de tous les sous-modèles à au plus une certaine distance (`cutoff`) du modèle avec le plus petit critère d'information (`criterion`).
- `step` permet de faire une recherche séquentielle avec un critère d'information.
- `glmulti::glmulti` permet une recherche exhaustive (`method = "h"`) ou par le biais d'un algorithme génétique (`method = "g"`).
- `glmnet::glmnet` permet d'ajuster le modèle avec pénalité LASSO.

Voir le [code en ligne](https://lbelzile.github.io/math60602/05-reglogistique.html#s%C3%A9lection-de-variables-en-r%C3%A9gression-logistique) dans les notes de cours.

# Objectif du ciblage marketing

Déterminer si le revenu prévu justifie l'envoi du catalogue

$$\mathsf{E}(\textsf{ymontant}_i) = \mathsf{E}(\textsf{ymontant}_i \mid \textsf{yachat}_i = 1)\Pr(\textsf{yachat}_i = 1).$$

On peut combiner un modèle de régression logistique avec la régression linéaire et les ajuster simultanément --- voir les notes de cours pour plus de détail.

Plus simplement, on pourrait ignorer le montant d'achat et envoyer un catalogue si la probabilité d'achat excède notre point de coupure optimal.

# Stratégie de référence

- Parmi les $100$K clients, $23 179$ auraient acheté si on leur avait envoyé le catalogue
- Ces clients auraient généré des revenus de $1 601 212$\$.
- Si on enlève le coût des envois ($100 000 \times 10$\$), la stratégie de référence permet d'obtenir un revenu net de $601 212$\$.


# Stratégie d'ajustement


En résumé, la procédure numérique à réaliser est la suivante:

- Choisir les variables à essayer (termes quadratiques, interactions, etc.)
- Choisir l'algorithme ou la méthode de sélection du modèle.
- Construire un catalogue de modèles: pour chacun, calculer les prédictions par validation croisée.
- Calculer le point de coupure optimal pour chaque modèle selon la fonction de gain moyen.
- Sélectionner le modèle qui **maximise le gain**.

# Prédiction et envoi

- Prédire les $100 000$ observations de l'échantillon test.
- Envoyer un catalogue si la probabilité d'achat excède le point de coupure.
- Calculer le revenu résultant:
    - zéro si on n'envoie pas de catalogue
    - $-10$ si la personne n'achète pas
    - $-10$ plus le montant d'achat sinon.

**En pratique**, on ne pourrait pas *a priori* connaître le revenu résultant de cette stratégie.

# Conclusion


Si on avait fait une bête recherche séquentielle et qu'on avait pris le modèle avec le plus petit BIC (8 variables explicatives), on aurait dégagé des revenus nets de $978 226$\$.

C’est une énorme amélioration, de plus de $`r round(978226/601212*100,1)-100`$%, par rapport à la stratégie de référence (soit envoyer un catalogue à tout le monde).


# Récapitulatif

- Les principes de sélection de variable couverts précédemment s'appliquent toujours (recherche exhaustive, séquentielle et LASSO).
- On peut aussi calculer les critères d'information puisque le modèle est ajusté par maximum de vraisemblance.
- Attention au surajustement! Suspect si les probabilités estimées sont près de $0$ ou $1$ (vérifier la calibration).
- Deux étapes: sélectionner le modèle (variables) et le point de coupure.
- D'autres modèles que la régression logistique (arbres de classification, etc.) sont envisageables pour la classification.

