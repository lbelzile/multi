---
title: "Analyse de regroupements"
subtitle: "Analyse multidimensionnelle appliquée"
date: "automne 2022"
author: "Léo Belzile"
institute: "HEC Montréal"
format: beamer
navigation: empty
colortheme: Flip
innertheme: Flip
outertheme: Flip
themeoptions: "bullet=circle, topline=true, shadow=false"
beamerarticle: false
pdf-engine: lualatex
code-line-numbers: true
fig-align: 'center'
mainfont: "D-DIN"
mathfont: 'Latin Modern Math'
sansfont: 'Latin Modern Sans'
keep-tex: true
include-in-header: 
      text: |
        \usepackage{tabu}
        \usepackage{mathtools}
        \usepackage{mathrsfs}
---

# Algorithmes pour l'analyse de regroupements


L'analyse de regroupements cherche à créer une division de $n$ observations de $p$ variables en regroupements.

1. méthodes basées sur la connectivité (regroupements hiérarchiques agglomératifs et divisifs)
2. méthodes basées sur les centroïdes et les médoïdes ($k$-moyennes, $k$-médoides)
3. mélanges de modèles
4. méthodes basées sur la densité
5. méthodes spectrales


```{r}
#| label: setup
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
library(knitr)
library(kableExtra)
set.seed(1014)
library(hecmulti)
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  out.width = "80%",
  fig.align = 'center',
  fig.width = 8.5,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
options(knitr.table.format = function() {
  if (knitr::is_latex_output()) 
    "latex" else "html"
})

options(dplyr.print_min = 6, dplyr.print_max = 6)
options(knitr.graphics.auto_pdf = TRUE)
options(scipen = 1, digits = 3)
library(viridis)
library(ggplot2, warn.conflicts = FALSE, quietly = TRUE)
library(poorman, quietly = TRUE, warn.conflicts = FALSE)
library(patchwork)

safe_colorblind_palette <- MetBrewer::met.brewer("Hiroshige",10)

options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill = "viridis")
scale_colour_discrete <- scale_color_manual(MetBrewer::met.brewer("Hiroshige",10))
scale_fill_discrete <- scale_fill_manual(MetBrewer::met.brewer("Hiroshige",10))
theme_set(theme_classic())
```

```{r}
#| eval: true
#| echo: false
donsmult <- hecmulti::dons |>
  filter(ndons > 1L) |>
  mutate(mtdons = vdons/ndons,
         snrefus = nrefus/anciennete*mean(anciennete),
         mpromesse = case_when(
           npromesse > 0 ~ vpromesse/npromesse,
           TRUE ~ 0)) |>
  select(!c(
    vradiations, # valeurs manquantes
    nindecis, vdons, ddonsmax,
    ddonsmin, vdonsmin, npromesse,
    vpromesse, nrefus, nradiations)) |>
  relocate(mtdons)
donsmult_std <- scale(donsmult)
# Extraire moyenne et écart-type
dm_moy <- attr(donsmult_std, "scaled:center")
dm_std <- attr(donsmult_std, "scaled:scale")
```

# Algorithme de partition autour des médoïdes (PAM) 

1. Initialisation: sélectionner $K$ des $n$ observations comme médoïdes initiaux.
2. Assigner chaque observation au médoïde le plus près.
3. Calculer la dissimilarité totale entre chaque médoïde et les observations de son groupe.
4. Pour chaque médoïde $(k=1, \ldots, K$):  
   - considérer tous les $n-K$ observations à tour de rôle et permuter le médoïde avec l'observation.  
   - calculer la distance totale et sélectionner l'observation qui diminue le plus la distance totale.
4. Répéter les étapes 2 à 4 jusqu'à ce que les médoïdes ne changent plus.



# Algorithme CLARA (1/2)

L'algorithme CLARA, décrit dans Kaufman & Rousseeuw (1990), réduit le coût de calcul et de stockage.

- Diviser l'échantillon en $S$ sous-échantillons de taille approximativement égale de taille $n_S$ (typiquement $K \ll n_S < 1000$)
- Utiliser l'algorithme PAM sur chaque sous-échantillon. 

Une fois les médoïdes obtenus, le reste de toutes les observations de l'échantillon sont assignées au regroupement du médoïde le plus près.


# Algorithme CLARA (2/2)


La qualité de la segmentation pour chacune des $S$ segmentations est calculée en obtenant la distance moyenne entre les médoïdes et les observations.

On retourne la meilleure segmentation parmi les $S$ (celle qui a la plus petite distance moyenne).

# PAM et CLARA dans **R**

Disponible depuis le paquet `cluster`.

```{r}
#| eval: true
#| echo: true
set.seed(60602)
kmedoide5 <- cluster::clara(
   x = donsmult_std,
   k = 5L, # nombre de groupes
   sampsize = 500, #taille échantillon pour PAM
   metric = "euclidean", # distance l2
   #cluster.only = TRUE, # ne conserver que étiquettes
   rngR = TRUE, # germe aléatoire depuis R
   pamLike = TRUE, # même algorithme que PAM
   samples = 10) #nombre de répétitions aléatoires
```


# Valeurs initiales et paramètres

\footnotesize 

Même hyperparamètres que $K$-moyennes (dissemblance, nombre de regroupements, initialisation et séparation).

Comme les $K$-moyennes, on fera plusieurs essais pour trouver de bonnes valeurs de départ. On peut tracer le profil des silhouettes.

```{r}
#| label: fig-clarasilhouette
#| eval: true
#| echo: FALSE
#| fig-cap: "Silhouettes pour les données de dons multiples avec l'algorithme CLARA pour $K=5$ regroupements."
factoextra::fviz_silhouette(kmedoide5,
                                 print.summary = FALSE) + 
  labs(subtitle = "Largeur moyenne des silhouettes: 0.28",
       y = "",
       title = "") +
 scale_color_manual(values  =  MetBrewer::met.brewer("Hiroshige",5)[1:5]) + 
  theme(legend.position = "none")
```

# Prototypes

Puisque les prototypes (médoïdes) sont des observations, on peut simplement extraire leur identifiant.



```{r}
#| eval: false
#| echo: true
medoides_orig <- donsmult[kmedoide5$i.med,]
# Taille des regroupements
kmedoide5$clusinfo
```

# Avantages et inconvénients des $K$-médoïdes

- ($-$) solution approximative pour grand échantillons
- ($+$) les prototypes sont des observations de l'échantillon.
- ($+$) la fonction objective est moins impactée par les extrêmes.
- ($-$) le coût de calcul est prohibitif avec des mégadonnées (problème combinatoire) avec complexité $\mathrm{O}(n^2)$. PAM fonctionne avec maximum 1000 observations.


# Mélange de modèles

On suppose qu'on a $K$ groupes, chacun caractérisé par une densité de dimension $p$, soit $f_k(\boldsymbol{X}_i;\boldsymbol{\theta}_k)$ si $\boldsymbol{X}_i$ provient du groupe $k=1, \ldots, K$.

Généralement, on choisit une loi normale multidimensionnelle pour le $k$e groupe $G$,
\begin{align*}
\boldsymbol{X} \mid G=k \sim \mathsf{No}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{align*}

La probabilité qu'une observation soit tirée du groupe $G=k$ est $\pi_k$.

# Estimation du mélange de modèle


La vraisemblance est une fonction des paramètres $\boldsymbol{\mu}_k$, $\boldsymbol{\Sigma}_k$ et de la probabilité $\pi_k$ qu'une observation $\mathbf{X}_i$ tombe dans le groupe $k$,
\begin{align*}
 L_i(\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \boldsymbol{\pi}_k\}_{k-1}^K; \mathbf{X}_i)= \sum_{k=1}^K\pi_k
f_{k}(\boldsymbol{X}_i \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
\end{align*}


Le maximum de vraisemblance est obtenu à l'aide de l'algorithme d'espérance-maximisation en augmentant les observations avec un indicateur de groupe.

- Étape E: assignation aux groupes (multinomiale).
- Étape $M$: estimation des probabilités, des moyennes et variances.

Le mélange de modèle nous donne accès à la probabilité $\pi_k$ qu'une observation appartiennent au groupe $G_k$ (**assignation probabiliste**).

# Fléau de la dimension

Chacune des $K$ matrice de covariance contient $p(p+1)/2$ paramètres!

En paramétrisant ces dernière, on peut réduire le nombre de paramètres à estimer.

- compromis entre simplicité (d'estimation) et nombre de paramètres

# Paramétrisation des matrices de covariance

La matrice de covariance dans `mclust` est paramétrisée en fonction de 

- $\lambda$, qui contrôle le volume, 
- une matrice diagonale $\mathbf{A}$ qui contrôle les variances de chaque observation et 
- $\mathbf{D}$ une matrice orthogonale qui permet de créer de la corrélation entre observations.

Un index $k$ spécifie que cette composante varie d'un regroupement à l'autre.

# Paramétrisation des variables

Voir `mclust.options("emModelNames")` et la documentation dans le Tableau 3 de [l'article sur `mclust`](https://journal.r-project.org/archive/2016/RJ-2016-021/RJ-2016-021.pdf).

```{r}
#| label: fig-modeles
#| fig-cap: "Forme des ellipsoïdes pour le mélange de modèle selon la forme de la structure de covariance. Tirée de [`mclust5`]((https://journal.r-project.org/archive/2016/RJ-2016-021/RJ-2016-021.pdf)), licence [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)."
#| eval: true
#| echo: false
knitr::include_graphics("figures/mclust5-parametrization.png")
```

# Paquet `mclust`

```{r}
#| cache: true
#| eval: false
#| echo: true
## Mélanges de modèles gaussiens
set.seed(60602)
library(mclust)
mmg <- Mclust(data = donsmult_std,
       G = 1:10,
       # Ajouter composante uniforme
       #  pour bruit (aberrances)
       initialization = list(noise = TRUE))
# Résumé de la segmentation
summary(mmg)
```

On peut obtenir les étiquettes (avec `0` pour le bruit) avec `mmg$classification`.



# Hyperparamètres

- le nombre de regroupements $K$
- la forme des ellipsoïdes (structure de covariance)
- les valeurs pour l'initialisation. 

Les mêmes considérations pratiques qu'avec les $K$-moyennes s'appliquent.

En pratique, on ajuste le modèle avec différent nombre de regroupements et différentes structures de covariance et on prend le modèle avec le plus petit BIC.

# Sélection des hyperparamètres

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| error: false
#| cache: true
set.seed(60602)
suppressMessages(library(mclust))
mmg <- mclust::Mclust(data = donsmult_std,
       G = 1:10,
       # Ajouter composante uniforme
       #  pour bruit (aberrances)
       initialization = list(noise = TRUE))
```

```{r}
#| eval: false
#| echo: true
plot(mmg, what = "BIC")
```
\footnotesize 

```{r}
#| eval: true
#| cache: true
#| message: false
#| echo: false
#| label: fig-mclustbic
#| fig-cap: "Valeur du négatif du BIC pour les mélanges de modèles gaussiens selon le nombre de regroupements et la structure de covariance."
factoextra::fviz_mclust(mmg, what = "BIC") + 
  labs(x = "nombre de composantes",
       y = "",
       subtitle = "Négatif du BIC",
       title = "") +
  theme(legend.position = "right")
```

Le logiciel n'arrive pas à estimer certains modèles complexes quand le nombre de groupes est trop élevé.


# Représentation graphique des regroupements

```{r}
#| eval: true
#| echo: false
#| label: fig-classifreducmclust
#| out-width: '90%'
#| fig-width: 10
#| fig-height: 5
#| cache: true
#| error: true
#| warning: false
#| message: false
#| fig-cap: "Projection des observations, colorées par regroupement (gauche) et structure des regroupements avec ellipsoides de confiance (droite)."
# Matrice des nuage de points (paires de variables)
# plot(mmg, what = "classification")
# Réduction de la dimension
reduc_dim_mmg <- mclust::MclustDR(mmg)
par(mfrow = c(1,2)) # graphiques côte-à-côte
try(plot(reduc_dim_mmg, what = "contour"), silent = TRUE)
plot(reduc_dim_mmg, what = "scatterplot")
```


#  Avantages et inconvénients des mélanges de modèles

- ($+$) approche est plus flexible que les $K$-moyennes.
- ($+$) l'ajout d'une composante uniforme permet de gérer les aberrances (supporté par `mclust`).
- ($+$) l'algorithme EM garantie la convergence à un minimum local (comme pour les $K$-moyennes)
- ($+$) on obtient une assignation probabiliste plutôt que rigide
- ($-$) le coût de calcul est plus élevé que les $K$-moyennes 
- ($-$) le nombre de paramètre des matrices de covariance augmente rapidement avec la dimension $p$.


#  Regroupements hiérarchiques

Méthode déterministe de regroupement à partir d'une matrice de dissimilarité.

1. Initialisation: chaque observation est assignée à son propre groupe.
2. les deux groupes les plus rapprochés sont fusionnés; la distance entre le nouveau groupe et les autres regroupements est recalculée.
3. on répète l'étape 2 jusqu'à obtenir un seul regroupement.

# Fonction de liaison

Il y a plusieurs façons de calculer la distance entre deux groupes d'observations de plusieurs observations, notamment

- liaison simple (`method = single`): plus proches voisins
- liaison complète (`method = complete`): voisins les plus éloignés
- liaison moyenne (`method = average`): utilise la moyenne des distances entre toutes les paires de sujets (un pour chaque groupe) provenant des deux groupes.
- méthode de Ward (`method = ward.D2`): calcul de l'homogénéité globale

# Illustration des mesures de liaison

```{r}
#| eval: true
#| echo: false
#| label: fig-distances
#| fig-cap: "Distances entre regroupements selon la liaison (simple, complète, barycentre, homogenéité de Ward)."
#| out-width: '80%'
#| fig-width: 6
#| fig-height: 4
set.seed(1234)
df <- rbind(
  cbind(rnorm(5, -2, sd = 1.2), 
        rnorm(5, mean = -2, sd = 1)),
  cbind(2 + rt(n = 10, df = 5, ncp = 0.1),
        5 + rt(n = 10, df = 10)))
df <- data.frame(df)
df$groupe <- c(rep(1, 5), rep(2, 10))

cols = MetBrewer::met.brewer("Hiroshige", 3, type = "discrete")[c(1,3)]
par(mfrow = c(2,2), mar = c(0,0,0,0))
plot(df[,1:2], 
     bty = "n", 
     axes = FALSE, 
     xlab = "", 
     ylab = "", pch = 20, col = cols[df[,3]])
di <- as.matrix(dist(df[,1:2]))
dipair <- di[1:5,6:15]
segments(x0 = df[3,1], y0 = df[3,2], x1 = df[12,1], y1 = df[12,2])
text(x = -2, y = 6, "liaison simple")
plot(df[,1:2], 
     bty = "n", 
     axes = FALSE, 
     xlab = "", 
     ylab = "", pch = 20, col = cols[df[,3]])
segments(x0 = df[4,1], y0 = df[4,2], x1 = df[14,1], y1 = df[14,2])
text(x = -2, y = 6, "liaison complète")
plot(df[,1:2], 
     bty = "n", 
     axes = FALSE, 
     xlab = "", 
     ylab = "", pch = 20, col = cols[df[,3]])
centroid <- rbind(colMeans(df[1:5,1:2]),
                  colMeans(df[6:15,1:2]))
points(centroid, col = cols, pch = 4)
segments(x0 = centroid[2,1], y0 = centroid[2,2], 
         x1 = centroid[1,1], y1 = centroid[1,2])
text(x = -2, y = 6, "barycentre")

plot(df[,1:2], 
     bty = "n", 
     axes = FALSE, 
     xlab = "", 
     ylab = "", pch = 20, col = cols[df[,3]])
centroid <- rbind(colMeans(df[1:5,1:2]),
                  colMeans(df[6:15,1:2]))
text(x = -2, y = 2, "")
points(x = centroid[,1], y= centroid[,2], col = cols, pch = 4)
for(i in 1:5){
  segments(x0 = df[i,1], y0 = df[i,2], 
           x1 = centroid[1,1], y1 = centroid[1,2], 
           col = cols[1])
}
for(i in 6:15){
  segments(x0 = df[i,1], y0 = df[i,2], 
           x1 = centroid[2,1], y1 = centroid[2,2], 
           col = cols[2])
}
text(x = -2, y = 6, "Ward")
```

# Méthode de Ward

La méthode de Ward utilise l'homogénéité comme critère.

Pour chaque groupe, on calcule la somme des carrés des distances par rapport à la moyenne du groupe, disons $\mathsf{SCD}_k$ ($k=1, \ldots, M$).

On calcule ensuite l'homogénéité globale en faisant la somme,  $\mathrm{H}^{(M)} = \mathsf{SCD}_1 + \cdots + \mathsf{SCD}_M$. 

La méthode de Ward va fusionner les deux groupes qui feront augmenter le moins possible l'homogénéité.

# Méthodes hiérarchiques et coût de calcul

Les algorithmes de regroupement hiérarchiques stockent une matrice de dissemblance $n \times n$: coût de stockage quadratique $\mathrm{O}(n^2)$.

Généralement, le coût de calcul est au mieux $\Omega(n^2)$ et au pire $\mathrm{O}(n^3)$. 

Pour la méthode de liaison simple, un algorithme permet d'obtenir un coût de calcul quadratique de $\mathrm{O}(n^2)$ sans stocker la matrice de dissemblance, d'où un coût de stockage linéaire de $\mathrm{O}(n)$. 

`stat::hclust` permet de faire des regroupements agglomératifs, mais le paquet `fastcluster` propose une version avec une empreinte mémoire inférieure (*plus rapide!)

# Genie 

Alternative de Gagolewski (2016) qui modifie la fonction de liaison simple en retenant son efficacité de calcul. 

Plutôt que de simplement trouver la paire de regroupements à distance minimale, cette fusion n'est appliquée que si une mesure d'inéquité est inférieur à un seuil spécifié par l'utilisateur.

Si les regroupements sont fortement inéquitables, la fusion survient entre les regroupements dont un de la taille minimale courante. 

L'implémentation **R** dans le paquet `genieclust` est nettement plus rapide que les autres alternative.

# Performance relative

- **Liaison simple**: fonctionne bien si l’écart entre deux regroupements est suffisamment grand. S’il y a du bruit entre deux regroupements, la qualité des regroupements en sera affectée. Souvent quelques valeurs isolées et un seul grand regroupement

- **Liaison complète**: moins sensible au bruit et aux faibles écarts entre regroupements, mais a tendance à casser les regroupements globulaires.

- **Homogénéité de Ward**:  le critère ressemble à celui des $K$-moyennes.

Voir la page [scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html) pour une illustration.

# Hyperparamètres des méthodes hiérarchiques

1. choix de la fonction de liaison (et hyperparamètres associés)
2. mesure de dissemblance
3. nombre de regroupements


On peut représenter le modèle à l'aide d'un arbre, où les feuilles indiquent les regroupements à chaque étape jusqu'à la racine à la dernière étape (**dendrogramme**). 

La distance entre chaque embranchement est déterminée par notre critère: cela nous permet de sélectionner un nombre de regroupements $K$ après inspection visuelle du dendrogramme.

On élague l'arbre à la hauteur voulue.

# Dendrogramme

```{r}
#| eval: true
#| echo: false
#| label: fig-dendrogramme
#| fig-cap: "Dendrogramme pour l'exemple de regroupement hiérarchique avec la méthode de Ward et 20 observations."
library(ggdendro)
model <- hclust(dist(hecmulti::regroupements1[1:20,]),
                method = "ward.D2")
dhc <- as.dendrogram(model)
# Rectangular lines
ddata <- dendro_data(dhc, type = "rectangle")
p <- ggplot(segment(ddata)) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) + 
  coord_flip() + 
  scale_y_reverse(expand = c(0.2, 0)) +
  theme_minimal()
p

```

# Critères pour Ward

On peut choisir $K$ à partir du pourcentage de variance expliquée, $R^2$ en calculant $$R^2_{(M)} = 1-\mathrm{H}_{(M)}/\mathrm{H}_{(1)},$$ où $\mathrm{H}_{(1)}$ est l'homogénéité globale avec un seul groupe.

Le R-carré semi-partiel mesure la perte d'homogénéité d'une étape à l'autre, renormalisée par 
$$R^2_{\text{sp}(M)} =\frac{\mathrm{H}_{(M)} - \mathrm{H}_{(M-1)}}{\mathrm{H}_{(1)}},$$

mesure la perte d’homogénéité (relative) en combinant ces deux groupes. 


On cherche un point d'inflection (un coude).

# Critères d'homogénéité (Ward)

```{r}
#| eval: false
#| echo: false
hecmulti::homogeneite(rhier = hclust(dist(hecmulti::regroupements1),
                method = "ward.D2"), 
                      ngroupes = 10)
```

# Avantages et inconvénient, regroupements hiérarchiques

- ($+$) la solution du regroupement hiérarchique est toujours la même (déterministe)
- ($-$) l'assignation d'une observation à un regroupement est finale
- ($-$) les aberrances ne sont pas traitées et sont souvent assignées dans des regroupements à part
- ($+$) les méthodes d'arborescence sont faciles à expliquer
- ($-$) le nombre de groupes n'a pas à être spécifié apriori (une seule estimation)
- ($-$) le coût de calcul est prohibitif, avec une complexité quadratique de $\mathrm{O}(n^2)$ pour la méthode de liaison simple et autrement $\mathrm{O}(n^3)$ pour la plupart des autres fonctions de liaison.

# Comparaison de segmentations

On veut parfois comparer les regroupements de différentes méthodes.

Les étiquettes ne sont pas nécessairement identiques même si les regroupements le sont (permutation des étiquettes).

Une mesure de similarité, l'indice de Rand, permet de comparer deux vecteurs d'observations catégorielles (étiquettes des regroupements).

- même longueur (même nombre d'observations)
- mais pas nécessairement le même nombre de modalités (nombre de regroupements potentiellement différents d'une partition à l'autre).


Idée: comparer à tour de rôle chacune des $n(n-1)/2$ paires d'observation. 


# Indice de Rand

On indique si les paires d'observations sont dans le même groupe (1) ou un groupe différent (0).

Exemple: deux partitions de 4 observations

- Étiquettes $A$: (2, 2, 2, 1)
- Étiquettes $B$: (3, 1, 3, 2)



| \{1,2\} | \{1,3\} | \{1,4\} | \{2,3\} | \{2,4\} | \{3,4\} |
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| 10 | 11 | 00 | 10 | 00 | 00 |

Parmi les six paires, trois sont concordantes (00 ou 11).

L'indice de Rand est le taux de bonne classification du tableau de contingence résultant.


# Récapitulatif

Les étapes d'une analyse de regroupements

1. Choisir les variables pertinentes à l'analyse. Cette étape peut nécessiter de créer, transformer de nouvelles variables ou d'aggréger les données.
2. Décider quel méthode sera utilisée pour la segmentation.
3. Choisir les hyperparamètres de l'algorithme (nombre de regroupements, rayon, etc.) et la mesure de dissemblance.
4. Valider la qualité de la segmentation (interprétabilité, taille des groupes, homogénéité des regroupements).
5. Avec les étiquettes, calculer un prototype de groupe. 
6. Interpréter les regroupements obtenus à partir des prototypes.

# Récapitulatif

- L'analyste a une grande marge de manoeuvre.
- Il n'y a pas de vérité: la segmentation n'est utile que si elle a une valeur ajoutée.
- Aucun algorithme ne performe uniformément mieux, mais certains sont plus faciles à employer que d'autres.
   - avec des mégadonnées, la complexité est un facteur important pour choisir la méthode.
   - la plupart du temps, il faut faire plusieurs itérations pour le choix des hyperparamètres pour avoir une bonne segmentation.


# Récapitulatif

Le Diable est dans les détails:

- variables catégorielles et binaires vs standardisation
- valeurs manquantes
- inclusion de variables sociodémographiques
- aberrances

Le nombre de groupes peut être guidé par le contexte: les formules et indicateurs de qualité servent de balises.
